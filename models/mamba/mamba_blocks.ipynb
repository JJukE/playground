{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33081dca-37ba-4081-8d0c-02dea0311ac3",
   "metadata": {},
   "source": [
    "Implementation of Mamba blocks\n",
    "- ViM block from [ViM](https://github.com/hustvl/Vim)\n",
    "- VSS block from [VMamba](https://github.com/MzeroMiko/VMamba)\n",
    "- SiMBA block from [SiMBA](https://github.com/badripatro/simba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffde11cb-c71d-4122-8efd-3bfac1ffe08f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/dev/playground/models/mamba'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"/root/dev/playground/models/mamba\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36aa7671-5421-4012-ad46-7f34dbbf6154",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.set_device(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72712c94-8963-4e0a-aeb0-aeee872924bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37mdb80750feafb              \u001b[m  Wed Nov 20 17:54:48 2024  \u001b[1m\u001b[30m550.100\u001b[m\n",
      "\u001b[36m[0]\u001b[m \u001b[34mNVIDIA GeForce RTX 4090\u001b[m |\u001b[31m 35Â°C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m 3070\u001b[m / \u001b[33m24564\u001b[m MB |\n"
     ]
    }
   ],
   "source": [
    "!gpustat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e3c1b4-0ee9-4836-a304-820c12388462",
   "metadata": {},
   "source": [
    "# ViM block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf80506c-f332-4ee4-a4b9-6c4cc6651129",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, mixer_cls, norm_cls=nn.LayerNorm, fused_add_norm=False, residual_in_fp32=False, drop_path=0.):\n",
    "        \"\"\" Simple block wrapping a mixer class with LayerNorm/RMSNorm and residual connection\n",
    "        Standard block is \"LN -> MHA/MLP -> Add\", but here is \"Add -> LN -> Mixer\",\n",
    "        returning both hidden_states (output of the mixer) and the residual. (for performance reasons)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.residual_in_fp32 = residual_in_fp32\n",
    "        self.fused_add_norm = fused_add_norm\n",
    "        self.mixer = mixer_cls(dim)\n",
    "        self.norm = norm_cls(dim)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        \n",
    "        if self.fused_add_norm:\n",
    "            assert RMSNorm is not None, \"RMSNorm import fails\"\n",
    "            assert isinstance(self.norm, (nn.LayerNorm, RMSNorm)), \"Only LayerNorm and RMSNorm are supported for fused_add_norm\"\n",
    "    \n",
    "    def forward(self, hidden_states: Tensor, residual: Optional[Tensor] = None, inference_params=None):\n",
    "        r\"\"\" Pass the input through the encoder layer.\n",
    "\n",
    "        Args:\n",
    "            hidden_states: the sequence to the encoder layer (required)\n",
    "            residual: hidden_states = Mixer(LN(residual))\n",
    "        \"\"\"\n",
    "        \n",
    "        if not self.fused_add_norm:\n",
    "            if residual is None:\n",
    "                residual = hidden_states\n",
    "            else:\n",
    "                residual = residual + self.drop_path(hidden_states)\n",
    "        \n",
    "            hidden_states = self.norm(residual.to(dtype=self.norm.weight.dtype))\n",
    "            if self.residual_in_fp32:\n",
    "                residual = reisdual.to(torch.float32)\n",
    "        \n",
    "        else:\n",
    "            fused_add_norm_fn = rms_norm_fn if isinstance(self.norm, RMSNorm) else layer_norm_fn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mambadance",
   "language": "python",
   "name": "mambadance"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
