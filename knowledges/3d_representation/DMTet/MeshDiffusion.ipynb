{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d9b7680-8b66-4d97-ab96-08ef233dfb64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/dev/playground/knowledges/3d_representation/DMTet'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"/root/dev/playground/knowledges/3d_representation/DMTet/\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57aace66-1494-4e1a-8ffa-7f524c961b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m6f37a742720d              \u001b[m  Wed Apr 17 18:15:44 2024  \u001b[1m\u001b[30m525.89.02\u001b[m\n",
      "\u001b[36m[0]\u001b[m \u001b[34mNVIDIA GeForce RTX 4090\u001b[m |\u001b[31m 42°C\u001b[m, \u001b[32m  5 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m  944\u001b[m / \u001b[33m24564\u001b[m MB |\n",
      "\u001b[36m[1]\u001b[m \u001b[34mNVIDIA GeForce RTX 4090\u001b[m |\u001b[31m 38°C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m   10\u001b[m / \u001b[33m24564\u001b[m MB |\n"
     ]
    }
   ],
   "source": [
    "!gpustat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc2fa6dc-b53d-45e0-918f-d2a9bd5b5a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.set_device(1)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733ac6de-5a4a-4ad3-aca9-2d70cf96ac4c",
   "metadata": {},
   "source": [
    "# 1. Fit DMTets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dab32ca-ea4a-4241-9cf1-4d27d74541f4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Store a json file which includes list of gt mesh paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27c33759-db7d-4c6d-8c4c-a6a8fd8b2533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "texture0.png  texture1.png  texture2.png  texture3.png\n"
     ]
    }
   ],
   "source": [
    "!ls /root/hdd2/ShapeNetCoreV2/ShapeNetCore.v2/02958343/93ce8e230939dfc230714334794526d4/images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "637d5d3f-9acb-43d8-960d-11c1c51962a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapenet_root_dir = \"/root/hdd2/ShapeNetCoreV2/ShapeNetCore.v2\"\n",
    "output_dir = \"/root/hdd2/ShapeNetCoreV2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e6b06a5-4e4a-45b9-9f01-3dfc02da66ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6965cafce4649ab925ac03615df8ff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There's no `/root/hdd2/ShapeNetCoreV2/ShapeNetCore.v2/02958343/93ce8e230939dfc230714334794526d4/models/model_normalized.obj`.\n",
      "There's no `/root/hdd2/ShapeNetCoreV2/ShapeNetCore.v2/02958343/4ddef66f32e1902d3448fdcb67fe08ff/models/model_normalized.obj`.\n",
      "There's no `/root/hdd2/ShapeNetCoreV2/ShapeNetCore.v2/02958343/8070747805908ae62a9eb0f146e94477/models/model_normalized.obj`.\n",
      "There's no `/root/hdd2/ShapeNetCoreV2/ShapeNetCore.v2/02958343/7aa9619e89baaec6d9b8dfa78596b717/models/model_normalized.obj`.\n",
      "There's no `/root/hdd2/ShapeNetCoreV2/ShapeNetCore.v2/02958343/2307b51ca7e4a03d30714334794526d4/models/model_normalized.obj`.\n",
      "There's no `/root/hdd2/ShapeNetCoreV2/ShapeNetCore.v2/02958343/e6c22be1a39c9b62fb403c87929e1167/models/model_normalized.obj`.\n",
      "There's no `/root/hdd2/ShapeNetCoreV2/ShapeNetCore.v2/02958343/9fb1d03b22ecac5835da01f298003d56/models/model_normalized.obj`.\n",
      "There's no `/root/hdd2/ShapeNetCoreV2/ShapeNetCore.v2/02958343/3c33f9f8edc558ce77aa0b62eed1492/models/model_normalized.obj`.\n",
      "There's no `/root/hdd2/ShapeNetCoreV2/ShapeNetCore.v2/02958343/806d740ca8fad5c1473f10e6caaeca56/models/model_normalized.obj`.\n",
      "There's no `/root/hdd2/ShapeNetCoreV2/ShapeNetCore.v2/02958343/5973afc979049405f63ee8a34069b7c5/models/model_normalized.obj`.\n",
      "There's no `/root/hdd2/ShapeNetCoreV2/ShapeNetCore.v2/02958343/302612708e86efea62d2c237bfbc22ca/models/model_normalized.obj`.\n",
      "There's no `/root/hdd2/ShapeNetCoreV2/ShapeNetCore.v2/02958343/3ffeec4abd78c945c7c79bdb1d5fe365/models/model_normalized.obj`.\n",
      "There's no `/root/hdd2/ShapeNetCoreV2/ShapeNetCore.v2/02958343/d6ee8e0a0b392f98eb96598da750ef34/models/model_normalized.obj`.\n",
      "There's no `/root/hdd2/ShapeNetCoreV2/ShapeNetCore.v2/02958343/207e69af994efa9330714334794526d4/models/model_normalized.obj`.\n",
      "There's no `/root/hdd2/ShapeNetCoreV2/ShapeNetCore.v2/02958343/ea3f2971f1c125076c4384c3b17a86ea/models/model_normalized.obj`.\n",
      "There's no `/root/hdd2/ShapeNetCoreV2/ShapeNetCore.v2/02958343/5bf2d7c2167755a72a9eb0f146e94477/models/model_normalized.obj`.\n",
      "There's no `/root/hdd2/ShapeNetCoreV2/ShapeNetCore.v2/02958343/f5bac2b133a979c573397a92d1662ba5/models/model_normalized.obj`.\n",
      "There's no `/root/hdd2/ShapeNetCoreV2/ShapeNetCore.v2/02958343/407f2811c0fe4e9361c6c61410fc904b/models/model_normalized.obj`.\n",
      "There's no `/root/hdd2/ShapeNetCoreV2/ShapeNetCore.v2/02958343/986ed07c18a2e5592a9eb0f146e94477/models/model_normalized.obj`.\n",
      "Mesh paths have been svaed to /root/hdd2/ShapeNetCoreV2/shapenet_gt_mesh_paths.json.\n"
     ]
    }
   ],
   "source": [
    "# # create a list of paths of all gt meshes and store them as a json file\n",
    "# import json\n",
    "# from pathlib import Path\n",
    "# from tqdm.auto import tqdm\n",
    "\n",
    "# mesh_paths = []\n",
    "\n",
    "# for cat_dir in tqdm(Path(shapenet_root_dir).iterdir()):\n",
    "#     if cat_dir.is_dir():\n",
    "#         for model_dir in cat_dir.iterdir():\n",
    "#             file_path = model_dir / \"models\" / \"model_normalized.obj\"\n",
    "#             if file_path.exists():\n",
    "#                 mesh_paths.append(str(file_path))\n",
    "#             else:\n",
    "#                 print(f\"There's no `{file_path}`.\")\n",
    "\n",
    "# output_file = Path(output_dir) / \"shapenet_gt_mesh_paths.json\"\n",
    "# with open(output_file, \"w\") as f:\n",
    "#     json.dump(mesh_paths, f, indent=4)\n",
    "\n",
    "# print(f\"Mesh paths have been svaed to {output_file}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed165f7c-dbcf-4271-a4cd-57f272777a67",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99ec57a3-b14b-4237-aa79-ccfc5748b29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapeNetDataset(object):\n",
    "    def __init__(self, shapenet_json):\n",
    "        with open(shapenet_json, 'r') as f:\n",
    "            self.mesh_list = json.load(f)\n",
    "        \n",
    "        print(f\"len all data: {len(self.mesh_list)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mesh_list)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.mesh_list[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad7f85ca-1dc0-4221-87af-cc0fd939fc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Basic dataset interface\"\"\"\n",
    "    def __init__(self): \n",
    "        super().__init__()\n",
    "\n",
    "    def __len__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __getitem__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def collate(self, batch):\n",
    "        iter_res, iter_spp = batch[0]['resolution'], batch[0]['spp']\n",
    "        res_dict = {\n",
    "                'mv' : torch.cat(list([item['mv'] for item in batch]), dim=0),\n",
    "                'mvp' : torch.cat(list([item['mvp'] for item in batch]), dim=0),\n",
    "                'campos' : torch.cat(list([item['campos'] for item in batch]), dim=0),\n",
    "                'resolution' : iter_res,\n",
    "                'spp' : iter_spp,\n",
    "                'img' : torch.cat(list([item['img'] for item in batch]), dim=0)\n",
    "            }\n",
    "\n",
    "        if 'spts' in batch[0]:\n",
    "            res_dict['spts'] = batch[0]['spts']\n",
    "        if 'vpts' in batch[0]:\n",
    "            res_dict['vpts'] = batch[0]['vpts']\n",
    "        if 'faces' in batch[0]:\n",
    "            res_dict['faces'] = batch[0]['faces']\n",
    "        if 'rast_triangle_id' in batch[0]:\n",
    "            res_dict['rast_triangle_id'] = batch[0]['rast_triangle_id']\n",
    "        \n",
    "        if 'depth' in batch[0]:\n",
    "            res_dict['depth'] = torch.cat(list([item['depth'] for item in batch]), dim=0)\n",
    "        if 'normal' in batch[0]:\n",
    "            res_dict['normal'] = torch.cat(list([item['normal'] for item in batch]), dim=0)\n",
    "        if 'geo_normal' in batch[0]:\n",
    "            res_dict['geo_normal'] = torch.cat(list([item['geo_normal'] for item in batch]), dim=0)\n",
    "        if 'geo_viewdir' in batch[0]:\n",
    "            res_dict['geo_viewdir'] = torch.cat(list([item['geo_viewdir'] for item in batch]), dim=0)\n",
    "        if 'pos' in batch[0]:\n",
    "            res_dict['pos'] = torch.cat(list([item['pos'] for item in batch]), dim=0)\n",
    "        if 'mask' in batch[0]:\n",
    "            res_dict['mask'] = torch.cat(list([item['mask'] for item in batch]), dim=0)\n",
    "        if 'mask_cont' in batch[0]:\n",
    "            res_dict['mask_cont'] = torch.cat(list([item['mask_cont'] for item in batch]), dim=0)\n",
    "        if 'envlight_transform' in batch[0]:\n",
    "            if batch[0]['envlight_transform'] is not None:\n",
    "                res_dict['envlight_transform'] = torch.cat(list([item['envlight_transform'] for item in batch]), dim=0)\n",
    "            else:\n",
    "                res_dict['envlight_transform'] = None\n",
    "\n",
    "        try:\n",
    "            res_dict['depth_second'] = torch.cat(list([item['depth_second'] for item in batch]), dim=0)\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            res_dict['normal_second'] = torch.cat(list([item['normal_second'] for item in batch]), dim=0)\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            res_dict['img_second'] = torch.cat(list([item['img_second'] for item in batch]), dim=0)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "        return res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "add3538e-5e49-4fa0-9e93-e4ed0550c765",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from https://github.com/NVIDIAGameWorks/kaolin/blob/master/kaolin/ops/mesh/trianglemesh.py'''\n",
    "# kaolin.ops.mesh.sample_points(self.ref_mesh.v_pos.unsqueeze(0), self.ref_mesh.t_pos_idx, 50000)[0][0]\n",
    "def _base_face_areas(face_vertices_0, face_vertices_1, face_vertices_2):\n",
    "    \"\"\"Base function to compute the face areas.\"\"\"\n",
    "    x1, x2, x3 = torch.split(face_vertices_0 - face_vertices_1, 1, dim=-1)\n",
    "    y1, y2, y3 = torch.split(face_vertices_1 - face_vertices_2, 1, dim=-1)\n",
    "\n",
    "    a = (x2 * y3 - x3 * y2) ** 2\n",
    "    b = (x3 * y1 - x1 * y3) ** 2\n",
    "    c = (x1 * y2 - x2 * y1) ** 2\n",
    "    areas = torch.sqrt(a + b + c) * 0.5\n",
    "\n",
    "    return areas\n",
    "\n",
    "def _base_sample_points_selected_faces(face_vertices, face_features=None):\n",
    "    \"\"\"Base function to sample points over selected faces.\n",
    "       The coordinates of the face vertices are interpolated to generate new samples.\n",
    "\n",
    "    Args:\n",
    "        face_vertices (tuple of torch.Tensor):\n",
    "            Coordinates of vertices, corresponding to selected faces to sample from.\n",
    "            A tuple of 3 entries corresponding to each of the face vertices.\n",
    "            Each entry is a torch.Tensor of shape :math:`(\\\\text{batch_size}, \\\\text{num_samples}, 3)`.\n",
    "        face_features (tuple of torch.Tensor, Optional):\n",
    "            Features of face vertices, corresponding to selected faces to sample from.\n",
    "            A tuple of 3 entries corresponding to each of the face vertices.\n",
    "            Each entry is a torch.Tensor of shape\n",
    "            :math:`(\\\\text{batch_size}, \\\\text{num_samples}, \\\\text{feature_dim})`.\n",
    "\n",
    "    Returns:\n",
    "        (torch.Tensor, torch.Tensor):\n",
    "            Sampled point coordinates of shape :math:`(\\\\text{batch_size}, \\\\text{num_samples}, 3)`.\n",
    "            Sampled points interpolated features of shape\n",
    "            :math:`(\\\\text{batch_size}, \\\\text{num_samples}, \\\\text{feature_dim})`.\n",
    "            If `face_vertices_features` arg is not specified, the returned interpolated features are None.\n",
    "    \"\"\"\n",
    "\n",
    "    face_vertices0, face_vertices1, face_vertices2 = face_vertices\n",
    "\n",
    "    sampling_shape = tuple(int(d) for d in face_vertices0.shape[:-1]) + (1,)\n",
    "    # u is proximity to middle point between v1 and v2 against v0.\n",
    "    # v is proximity to v2 against v1.\n",
    "    #\n",
    "    # The probability density for u should be f_U(u) = 2u.\n",
    "    # However, torch.rand use a uniform (f_X(x) = x) distribution,\n",
    "    # so using torch.sqrt we make a change of variable to have the desired density\n",
    "    # f_Y(y) = f_X(y ^ 2) * |d(y ^ 2) / dy| = 2y\n",
    "    u = torch.sqrt(torch.rand(sampling_shape,\n",
    "                              device=face_vertices0.device,\n",
    "                              dtype=face_vertices0.dtype))\n",
    "\n",
    "    v = torch.rand(sampling_shape,\n",
    "                   device=face_vertices0.device,\n",
    "                   dtype=face_vertices0.dtype)\n",
    "    w0 = 1 - u\n",
    "    w1 = u * (1 - v)\n",
    "    w2 = u * v\n",
    "\n",
    "    points = w0 * face_vertices0 + w1 * face_vertices1 + w2 * face_vertices2\n",
    "\n",
    "    features = None\n",
    "    if face_features is not None:\n",
    "        face_features0, face_features1, face_features2 = face_features\n",
    "        features = w0 * face_features0 + w1 * face_features1 + \\\n",
    "            w2 * face_features2\n",
    "\n",
    "    return points, features\n",
    "\n",
    "def sample_points(vertices, faces, num_samples, areas=None, face_features=None):\n",
    "    r\"\"\"Uniformly sample points over the surface of triangle meshes.\n",
    "\n",
    "    First face on which the point is sampled is randomly selected,\n",
    "    with the probability of selection being proportional to the area of the face.\n",
    "    then the coordinate on the face is uniformly sampled.\n",
    "\n",
    "    If ``face_features`` is defined for the mesh faces,\n",
    "    the sampled points will be returned with interpolated features as well,\n",
    "    otherwise, no feature interpolation will occur.\n",
    "\n",
    "    Args:\n",
    "        vertices (torch.Tensor):\n",
    "            The vertices of the meshes, of shape\n",
    "            :math:`(\\text{batch_size}, \\text{num_vertices}, 3)`.\n",
    "        faces (torch.LongTensor):\n",
    "            The faces of the mesh, of shape :math:`(\\text{num_faces}, 3)`.\n",
    "        num_samples (int):\n",
    "            The number of point sampled per mesh.\n",
    "        areas (torch.Tensor, optional):\n",
    "            The areas of each face, of shape :math:`(\\text{batch_size}, \\text{num_faces})`,\n",
    "            can be preprocessed, for fast on-the-fly sampling,\n",
    "            will be computed if None (default).\n",
    "        face_features (torch.Tensor, optional):\n",
    "            Per-vertex-per-face features, matching ``faces`` order,\n",
    "            of shape :math:`(\\text{batch_size}, \\text{num_faces}, 3, \\text{feature_dim})`.\n",
    "            For example:\n",
    "\n",
    "                1. Texture uv coordinates would be of shape\n",
    "                   :math:`(\\text{batch_size}, \\text{num_faces}, 3, 2)`.\n",
    "                2. RGB color values would be of shape\n",
    "                   :math:`(\\text{batch_size}, \\text{num_faces}, 3, 3)`.\n",
    "\n",
    "            When specified, it is used to interpolate the features for new sampled points.\n",
    "\n",
    "    See also:\n",
    "        :func:`~kaolin.ops.mesh.index_vertices_by_faces` for conversion of features defined per vertex\n",
    "        and need to be converted to per-vertex-per-face shape of :math:`(\\text{num_faces}, 3)`.\n",
    "\n",
    "    Returns:\n",
    "        (torch.Tensor, torch.LongTensor, (optional) torch.Tensor):\n",
    "            the pointclouds of shape :math:`(\\text{batch_size}, \\text{num_samples}, 3)`,\n",
    "            and the indexes of the faces selected,\n",
    "            of shape :math:`(\\text{batch_size}, \\text{num_samples})`.\n",
    "\n",
    "            If ``face_features`` arg is specified, then the interpolated features of sampled points of shape\n",
    "            :math:`(\\text{batch_size}, \\text{num_samples}, \\text{feature_dim})` are also returned.\n",
    "    \"\"\"\n",
    "    if faces.shape[-1] != 3:\n",
    "        raise NotImplementedError(\"sample_points is only implemented for triangle meshes\")\n",
    "    faces_0, faces_1, faces_2 = torch.split(faces, 1, dim=1)        # (num_faces, 3) -> tuple of (num_faces,)\n",
    "    face_v_0 = torch.index_select(vertices, 1, faces_0.reshape(-1))  # (batch_size, num_faces, 3)\n",
    "    face_v_1 = torch.index_select(vertices, 1, faces_1.reshape(-1))  # (batch_size, num_faces, 3)\n",
    "    face_v_2 = torch.index_select(vertices, 1, faces_2.reshape(-1))  # (batch_size, num_faces, 3)\n",
    "\n",
    "    if areas is None:\n",
    "        areas = _base_face_areas(face_v_0, face_v_1, face_v_2).squeeze(-1)\n",
    "    face_dist = torch.distributions.Categorical(areas)\n",
    "    face_choices = face_dist.sample([num_samples]).transpose(0, 1)\n",
    "    _face_choices = face_choices.unsqueeze(-1).repeat(1, 1, 3)\n",
    "    v0 = torch.gather(face_v_0, 1, _face_choices)  # (batch_size, num_samples, 3)\n",
    "    v1 = torch.gather(face_v_1, 1, _face_choices)  # (batch_size, num_samples, 3)\n",
    "    v2 = torch.gather(face_v_2, 1, _face_choices)  # (batch_size, num_samples, 3)\n",
    "    face_vertices_choices = (v0, v1, v2)\n",
    "\n",
    "    # UV coordinates are available, make sure to calculate them for sampled points as well\n",
    "    face_features_choices = None\n",
    "    if face_features is not None:\n",
    "        feat_dim = face_features.shape[-1]\n",
    "        # (num_faces, 3) -> tuple of (num_faces,)\n",
    "        _face_choices = face_choices[..., None, None].repeat(1, 1, 3, feat_dim)\n",
    "        face_features_choices = torch.gather(face_features, 1, _face_choices)\n",
    "        face_features_choices = tuple(\n",
    "            tmp_feat.squeeze(2) for tmp_feat in torch.split(face_features_choices, 1, dim=2))\n",
    "\n",
    "    points, point_features = _base_sample_points_selected_faces(\n",
    "        face_vertices_choices, face_features_choices)\n",
    "\n",
    "    if point_features is not None:\n",
    "        return points, face_choices, point_features\n",
    "    else:\n",
    "        return points, face_choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "331725df-7518-4be6-a6fd-496791ed0407",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "from render import util\n",
    "from render import mesh\n",
    "from render import render\n",
    "from render import light\n",
    "\n",
    "###############################################################################\n",
    "# Reference dataset using mesh & rendering\n",
    "###############################################################################\n",
    "\n",
    "class DatasetMesh(Dataset):\n",
    "\n",
    "    def __init__(self, ref_mesh, glctx, cam_radius, FLAGS, validate=False):\n",
    "        # Init \n",
    "        self.glctx              = glctx\n",
    "        self.cam_radius         = cam_radius\n",
    "        self.FLAGS              = FLAGS\n",
    "        self.validate           = validate\n",
    "        self.fovy               = np.deg2rad(45)\n",
    "        self.aspect             = FLAGS.train_res[1] / FLAGS.train_res[0]\n",
    "        self.random_lgt         = FLAGS.random_lgt\n",
    "        self.camera_lgt         = False\n",
    "        self.flat_shading       = FLAGS.dataset_flat_shading\n",
    "        \n",
    "\n",
    "        if self.FLAGS.local_rank == 0:\n",
    "            print(f\"use flag shading {FLAGS.dataset_flat_shading}\")\n",
    "            print(\"DatasetMesh: ref mesh has %d triangles and %d vertices\" % (ref_mesh.t_pos_idx.shape[0], ref_mesh.v_pos.shape[0]))\n",
    "\n",
    "        # Sanity test training texture resolution\n",
    "        ref_texture_res = np.maximum(ref_mesh.material['kd'].getRes(), ref_mesh.material['ks'].getRes())\n",
    "        if 'normal' in ref_mesh.material:\n",
    "            ref_texture_res = np.maximum(ref_texture_res, ref_mesh.material['normal'].getRes())\n",
    "        if self.FLAGS.local_rank == 0 and FLAGS.texture_res[0] < ref_texture_res[0] or FLAGS.texture_res[1] < ref_texture_res[1]:\n",
    "            print(\"---> WARNING: Picked a texture resolution lower than the reference mesh [%d, %d] < [%d, %d]\" % (FLAGS.texture_res[0], FLAGS.texture_res[1], ref_texture_res[0], ref_texture_res[1]))\n",
    "\n",
    "        print(\"Loading env map\")\n",
    "        sys.stdout.flush()\n",
    "        # Load environment map texture\n",
    "        self.envlight = light.load_env(FLAGS.envmap, scale=FLAGS.env_scale)\n",
    "        \n",
    "        print(\"Computing tangents\")\n",
    "        sys.stdout.flush()\n",
    "        try:\n",
    "            self.ref_mesh = mesh.compute_tangents(ref_mesh)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Continue without tangents...\")\n",
    "            self.ref_mesh = ref_mesh\n",
    "\n",
    "    def _rotate_scene(self, itr):\n",
    "        proj_mtx = util.perspective(self.fovy, self.FLAGS.display_res[1] / self.FLAGS.display_res[0], self.FLAGS.cam_near_far[0], self.FLAGS.cam_near_far[1])\n",
    "\n",
    "        # Smooth rotation for display.\n",
    "        ang    = (itr / 50) * np.pi * 2\n",
    "        mv     = util.translate(0, 0, -self.cam_radius) @ (util.rotate_x(-0.4) @ util.rotate_y(ang))\n",
    "        mvp    = proj_mtx @ mv\n",
    "        campos = torch.linalg.inv(mv)[:3, 3]\n",
    "\n",
    "        return mv[None, ...].cuda(), mvp[None, ...].cuda(), campos[None, ...].cuda(), self.FLAGS.display_res, self.FLAGS.spp\n",
    "\n",
    "    def _random_scene(self):\n",
    "        # ==============================================================================================\n",
    "        #  Setup projection matrix\n",
    "        # ==============================================================================================\n",
    "        iter_res = self.FLAGS.train_res\n",
    "        proj_mtx = util.perspective(self.fovy, iter_res[1] / iter_res[0], self.FLAGS.cam_near_far[0], self.FLAGS.cam_near_far[1])\n",
    "\n",
    "        # ==============================================================================================\n",
    "        #  Random camera & light position\n",
    "        # ==============================================================================================\n",
    "\n",
    "        # Random rotation/translation matrix for optimization.\n",
    "        mv     = util.translate(0, 0, -self.cam_radius) @ util.random_rotation_translation(0.2)\n",
    "        mvp    = proj_mtx @ mv\n",
    "        campos = torch.linalg.inv(mv)[:3, 3]\n",
    "\n",
    "        return mv[None, ...].cuda(), mvp[None, ...].cuda(), campos[None, ...].cuda(), iter_res, self.FLAGS.spp # Add batch dimension\n",
    "\n",
    "    def __len__(self):\n",
    "        return 50 if self.validate else (self.FLAGS.iter + 1) * self.FLAGS.batch\n",
    "\n",
    "    def __getitem__(self, itr):\n",
    "        # ==============================================================================================\n",
    "        #  Randomize scene parameters\n",
    "        # ==============================================================================================\n",
    "\n",
    "        if self.validate:\n",
    "            mv, mvp, campos, iter_res, iter_spp = self._rotate_scene(itr)\n",
    "            camera_mv = None\n",
    "        else:\n",
    "            mv, mvp, campos, iter_res, iter_spp = self._random_scene()\n",
    "            if self.random_lgt:\n",
    "                rnd_rot = util.random_rotation()\n",
    "                camera_mv = rnd_rot.unsqueeze(0).clone()\n",
    "            elif self.camera_lgt:\n",
    "                camera_mv = mv.clone()\n",
    "            else:\n",
    "                camera_mv = None\n",
    "\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            render_out = render.render_mesh(self.glctx, self.ref_mesh, mvp, campos, self.envlight, iter_res, spp=iter_spp, \n",
    "                                num_layers=self.FLAGS.layers, msaa=True, background=None, xfm_lgt=camera_mv, flat_shading=self.flat_shading)\n",
    "            img = render_out['shaded']\n",
    "            img_second = render_out['shaded_second']\n",
    "            normal = render_out['normal']\n",
    "            depth = render_out['depth']\n",
    "            geo_normal = render_out['geo_normal']\n",
    "            pos = render_out['pos']\n",
    "\n",
    "            sample_points = torch.tensor(kaolin.ops.mesh.sample_points(self.ref_mesh.v_pos.unsqueeze(0), self.ref_mesh.t_pos_idx, 50000)[0][0])\n",
    "            vertex_points = self.ref_mesh.v_pos\n",
    "        \n",
    "        return_dict = {\n",
    "            'mv' : mv,\n",
    "            'mvp' : mvp,\n",
    "            'campos' : campos,\n",
    "            'resolution' : iter_res,\n",
    "            'spp' : iter_spp,\n",
    "            'img' : img,\n",
    "            'img_second' : img_second,\n",
    "            'spts': sample_points,\n",
    "            'vpts': vertex_points,\n",
    "            'faces': self.ref_mesh.t_pos_idx,\n",
    "            'depth': depth,\n",
    "            'normal': normal,\n",
    "            'geo_normal': geo_normal,\n",
    "            'geo_viewdir': render_out['geo_viewdir'],\n",
    "            'pos': pos,\n",
    "            'envlight_transform': camera_mv,\n",
    "            'mask': render_out['mask'],\n",
    "            'mask_cont': render_out['mask_cont'],\n",
    "            'rast_triangle_id': render_out['rast_triangle_id']\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            return_dict['depth_second'] = render_out['depth_second']\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            return_dict['normal_second'] = render_out['normal_second']\n",
    "        except:\n",
    "            pass\n",
    "        return return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0635cc7-8612-41ed-bb01-5e6e0020c8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from render import texture\n",
    "\n",
    "mtl_override = None\n",
    "mtl_default = {\n",
    "    'name' : '_default_mat',\n",
    "    'bsdf': 'diffuse',\n",
    "    'uniform': True,\n",
    "    'kd'   : texture.Texture2D(torch.tensor([0.75, 0.3, 0.6], dtype=torch.float32, device='cuda'), trainable=False),\n",
    "    'ks'   : texture.Texture2D(torch.tensor([0.0, 0.0, 0.0], dtype=torch.float32, device='cuda'), trainable=False)\n",
    "}\n",
    "normal_only = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "740e9f69-83cf-47ad-820e-d1f58fcffc64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShapeNetCore.v2\t\t     visualize\n",
      "ShapeNetCore.v2.PC15k\t     volume_10M\n",
      "no_mtl_objs\t\t     volume_200K\n",
      "preprocessed_captions.csv    volume_500K\n",
      "render_for_fid.tar.gz\t     volume_6M\n",
      "render_for_fid_20_views      volume_occupancies_airplane\n",
      "render_for_fid_no_light      volume_occupancies_car\n",
      "render_for_gen_desc\t     volume_occupancies_chair\n",
      "shapenet_gt_mesh_paths.json  volume_occupancies_table\n",
      "split\t\t\t     watertight\n",
      "split_pc\n"
     ]
    }
   ],
   "source": [
    "!ls /root/hdd2/ShapeNetCoreV2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ab67031-74a2-4683-9789-2ad792701635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len all data: 52472\n",
      "Load use-defined default mtl\n",
      "Skip loading non-default materials\n",
      "[{'name': '_default_mat', 'bsdf': 'diffuse', 'uniform': True, 'kd': Texture2D(), 'ks': Texture2D()}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from render import mesh\n",
    "\n",
    "shapenet_dataset = ShapeNetDataset(\"/root/hdd2/ShapeNetCoreV2/shapenet_gt_mesh_paths.json\")\n",
    "mesh_fname = shapenet_dataset[0]\n",
    "mtl_override = None\n",
    "ref_mesh = mesh.load_mesh(mesh_fname, mtl_override, mtl_default, use_default=normal_only, no_additional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce90b81b-186b-42dc-8f7b-bf187a42ced4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/hdd2/ShapeNetCoreV2/ShapeNetCore.v2/03211117/e6bedde4571506491fde762a4c6848dd/models/model_normalized.obj'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mesh_fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90a863a4-8b1f-4274-8bd0-03769d93c03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalization: tensor([-0.0713,  0.1578, -0.0310], device='cuda:1') 1.238707634872645\n"
     ]
    }
   ],
   "source": [
    "ref_mmesh = mesh.center_by_reference(ref_mesh, mesh.aabb_clean(ref_mesh), 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70225fc9-4c62-4660-ba04-64fb09d8ceea",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ref_mesh.v_nrm.clone()\n",
    "ref_mesh = mesh.auto_normals(ref_mesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17d1abfd-f46e-443b-8874-398597c02dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "RADIUS = 2.0\n",
    "\n",
    "# configs only related to the dataset\n",
    "configs = \"\"\"\n",
    "    tarin_res: (512, 512)\n",
    "    texture_res: (1024, 1024)\n",
    "    random_lgt: True\n",
    "    dataset_flat_shading: False\n",
    "    local_rank: 0 # TODO: delete\n",
    "    env_map: Null\n",
    "    env_scale: 1.\n",
    "    display_res: Null\n",
    "    cam_near_far: (0.1, 1000.0)\n",
    "    spp: 1\n",
    "    iter: 5000\n",
    "    batch: 1\n",
    "    layers: 1\n",
    "\"\"\"\n",
    "configs = yaml.safe_load(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8643d114-ab96-4e50-9790-a240ab59dbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nvdiffrast\n",
    "\n",
    "glctx = nvdiffrast.torch.RasterizeGLContext()\n",
    "dataset_train = DatasetMesh(ref_mesh, glctx, FLAGS=configs, validate=False)\n",
    "dataset_val = DAtasetMesh(ref_mesh, glctx, FLAGS-configs, validate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92419930-1f95-416b-a662-e7715e46a2c9",
   "metadata": {},
   "source": [
    "# Fitting DMTets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b059bfee-92e4-4102-bacb-f7a5d6cbc2e6",
   "metadata": {},
   "source": [
    "### Get geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18872a82-42a5-4687-b40a-3c92d25b7df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DMTetGeometry in MeshDiffusion/nvdiffrec/lib/geometry/dmtet.py\n",
    "geometry = DMTetGeometry(\n",
    "    model_config.dmtet_grid, model_config.mesh_scale, model_config, deform_scale=model_config.first_stage_deform\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96adf1ea-9664-4747-b405-e7829cd65b8e",
   "metadata": {},
   "source": [
    "### Get textures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afe8863-5216-4b26-8c79-184e682e5351",
   "metadata": {},
   "outputs": [],
   "source": [
    "from render import texture, mlptexture, material\n",
    "\n",
    "###############################################################################\n",
    "# Utility functions for material\n",
    "###############################################################################\n",
    "\n",
    "def initial_guess_material(geometry, mlp, FLAGS, init_mat=None):\n",
    "    kd_min, kd_max = torch.tensor(FLAGS.kd_min, dtype=torch.float32, device='cuda'), torch.tensor(FLAGS.kd_max, dtype=torch.float32, device='cuda')\n",
    "    ks_min, ks_max = torch.tensor(FLAGS.ks_min, dtype=torch.float32, device='cuda'), torch.tensor(FLAGS.ks_max, dtype=torch.float32, device='cuda')\n",
    "    nrm_min, nrm_max = torch.tensor(FLAGS.nrm_min, dtype=torch.float32, device='cuda'), torch.tensor(FLAGS.nrm_max, dtype=torch.float32, device='cuda')\n",
    "    if mlp:\n",
    "        mlp_min = torch.cat((kd_min[0:3], ks_min, nrm_min), dim=0)\n",
    "        mlp_max = torch.cat((kd_max[0:3], ks_max, nrm_max), dim=0)\n",
    "        mlp_map_opt = mlptexture.MLPTexture3D(geometry.getAABB(), channels=9, min_max=[mlp_min, mlp_max])\n",
    "        mat =  material.Material({'kd_ks_normal' : mlp_map_opt})\n",
    "    else:\n",
    "        # Setup Kd (albedo) and Ks (x, roughness, metalness) textures\n",
    "        if FLAGS.random_textures or init_mat is None:\n",
    "            num_channels = 4 if FLAGS.layers > 1 else 3\n",
    "            kd_init = torch.rand(size=FLAGS.texture_res + [num_channels], device='cuda') * (kd_max - kd_min)[None, None, 0:num_channels] + kd_min[None, None, 0:num_channels]\n",
    "            kd_map_opt = texture.create_trainable(kd_init , FLAGS.texture_res, not FLAGS.custom_mip, [kd_min, kd_max])\n",
    "\n",
    "            ksR = np.random.uniform(size=FLAGS.texture_res + [1], low=0.0, high=0.01)\n",
    "            ksG = np.random.uniform(size=FLAGS.texture_res + [1], low=ks_min[1].cpu(), high=ks_max[1].cpu())\n",
    "            ksB = np.random.uniform(size=FLAGS.texture_res + [1], low=ks_min[2].cpu(), high=ks_max[2].cpu())\n",
    "\n",
    "            ks_map_opt = texture.create_trainable(np.concatenate((ksR, ksG, ksB), axis=2), FLAGS.texture_res, not FLAGS.custom_mip, [ks_min, ks_max])\n",
    "        else:\n",
    "            kd_map_opt = texture.create_trainable(init_mat['kd'], FLAGS.texture_res, not FLAGS.custom_mip, [kd_min, kd_max])\n",
    "            ks_map_opt = texture.create_trainable(init_mat['ks'], FLAGS.texture_res, not FLAGS.custom_mip, [ks_min, ks_max])\n",
    "\n",
    "        # Setup normal map\n",
    "        if FLAGS.random_textures or init_mat is None or 'normal' not in init_mat:\n",
    "            normal_map_opt = texture.create_trainable(np.array([0, 0, 1]), FLAGS.texture_res, not FLAGS.custom_mip, [nrm_min, nrm_max])\n",
    "        else:\n",
    "            normal_map_opt = texture.create_trainable(init_mat['normal'], FLAGS.texture_res, not FLAGS.custom_mip, [nrm_min, nrm_max])\n",
    "\n",
    "        mat = material.Material({\n",
    "            'kd'     : kd_map_opt,\n",
    "            'ks'     : ks_map_opt,\n",
    "            'normal' : normal_map_opt\n",
    "        })\n",
    "\n",
    "    if init_mat is not None:\n",
    "        mat['bsdf'] = init_mat['bsdf']\n",
    "    else:\n",
    "        mat['bsdf'] = 'pbr'\n",
    "\n",
    "    return mat\n",
    "\n",
    "def initial_guess_material_knownkskd(geometry, mlp, FLAGS, init_mat=None):\n",
    "    nrm_min, nrm_max = torch.tensor(FLAGS.nrm_min, dtype=torch.float32, device='cuda'), torch.tensor(FLAGS.nrm_max, dtype=torch.float32, device='cuda')\n",
    "\n",
    "    if mlp:\n",
    "        mlp_min = nrm_min\n",
    "        mlp_max = nrm_max\n",
    "        mlp_map_opt = mlptexture.MLPTexture3D(geometry.getAABB(), channels=3, min_max=[mlp_min, mlp_max])\n",
    "        # mlp_map_opt = mlptexture.MLPTexture3D(geometry.getAABB(), channels=3, min_max=None)\n",
    "        mat =  material.Material({\n",
    "            'kd'     : init_mat['kd'],\n",
    "            'ks'     : init_mat['ks'],\n",
    "            'normal' : mlp_map_opt,\n",
    "        })\n",
    "    else:\n",
    "        # Setup normal map\n",
    "        if FLAGS.random_textures or init_mat is None or 'normal' not in init_mat:\n",
    "            normal_map_opt = texture.create_trainable(np.array([0, 0, 1]), FLAGS.texture_res, not FLAGS.custom_mip, [nrm_min, nrm_max])\n",
    "        else:\n",
    "            normal_map_opt = texture.create_trainable(init_mat['normal'], FLAGS.texture_res, not FLAGS.custom_mip, [nrm_min, nrm_max])\n",
    "\n",
    "        mat = material.Material({\n",
    "            'kd'     : init_mat['kd'],\n",
    "            'ks'     : init_mat['ks'],\n",
    "            'normal' : normal_map_opt\n",
    "        })\n",
    "\n",
    "    if init_mat is not None:\n",
    "        mat['bsdf'] = init_mat['bsdf']\n",
    "    else:\n",
    "        mat['bsdf'] = 'pbr'\n",
    "\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a0450e-b01e-4781-a008-1634848bed00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set textures, make initial guess from reference if possible\n",
    "mat = initial_guess_material_knownkskd(geometry, True, mat_config, mtl_default)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49aed55-a17a-49ff-ac60-afd4f0bc8236",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd6dc22-1960-4b90-a8d5-7baff420f35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass 1\n",
    "geometry, mat = optimize_mesh(glctx, geometry, mat, lgt, dataset_train, dataset_val, config,\n",
    "                             pass_idx=0, pass_name=\"dmtet_pass1\", optimize_light=config.learn_light)\n",
    "base_mesh = geometry.getMesh(mat)\n",
    "vert_mask = torch.zeros_like(geoemtry.sdf).long().cuda().view(-1, 1)\n",
    "vert_mask[geometry.getValidVertsIdx()] = 1\n",
    "\n",
    "torch.cuda.empty_cache() # may slow down training\n",
    "\n",
    "torch.save({\n",
    "    \"sdf\": geometry.sdf.cpu().detach(),\n",
    "    \"sdf_ema\": geometry.sdf_ema.cpu().detach(),\n",
    "    \"deform\": (geometry.deform * vert_mask).cpu().detach(),\n",
    "    \"deform_unmasked\": geometry.deform.cpu().detach(),\n",
    "}, os.path.join(config.out_dir, \"tets_pre/dmt_dict_{:05d}.pt\".format(global_index)))\n",
    "old_geometry = geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5872b163-39c7-48ec-9bb2-da00f2e56968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass 2\n",
    "geometry = DMTetGeometryFixedTopo(\n",
    "    geometry, base_mesh, config.dmtet_grid, config.mesh_scale, config, deform_scale=config.second_stage_deform\n",
    ")\n",
    "\n",
    "geometry.sdf_sign.requires_grad = False\n",
    "geometry.sdf_abs.requires_grad = False\n",
    "geometry.deform.requires_grad = True\n",
    "\n",
    "geometry.deform.data[:] = geometry.deform * config.first_stage_deform / config.second_stage_deform\n",
    "\n",
    "if config.use_ema:\n",
    "    geometry.sdf_sign.data[:] = torch.sign(old_geoemtry.sdf_ema)\n",
    "else:\n",
    "    geometry.sdf_sign.data[:] = torch.sign(old_geometry.sdf)\n",
    "\n",
    "geometry.set_init_v_pos()\n",
    "\n",
    "geometry, mat = optimize_mesh(glctx, geometry, mat, lgt, dataset_train, dataset_val, config,\n",
    "                             pass_idx=1, pass_name=\"mesh_pass\", warmup_iter=100,\n",
    "                             optimize_light=config.learn_light and not config.lock_light,\n",
    "                             optimize_geometry=not config.lock_pos)\n",
    "vert_mask = torch.zeros_like(geometry.sdf_sign).long().cuda().view(-1, 1)\n",
    "vert_mask[geometry.getValidVertsIdx()] = 1\n",
    "\n",
    "torch.save({\n",
    "    \"sdf\": geometry.sdf_sign.cpu().detach(),\n",
    "    \"deform\": (geometry.deform * vert_mask).cpu().detach(),\n",
    "    \"deform_unmasked\": geometry.deform.cpu().detach()\n",
    "}, os.path.join(config.out_dir, \"tets/dmt_dict_{:05d}.pt\".format(global_index)))\n",
    "\n",
    "# ... validation ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7bad05-c794-432e-9702-e458e18d8994",
   "metadata": {},
   "source": [
    "# Training Diffusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e462c899-66c1-4504-b2fa-b43595de9e7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MeshDiffusion",
   "language": "python",
   "name": "meshdiffusion"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
