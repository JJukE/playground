{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa071e71-623a-40f7-beba-450f1acfba48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/dev/playground/knowledges/peft'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"/root/dev/playground/knowledges/peft\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eeb09e45-4494-4617-b4f1-241410c658b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37maa4173e0a5f2              \u001b[m  Thu May 30 21:08:10 2024  \u001b[1m\u001b[30m535.129.03\u001b[m\n",
      "\u001b[36m[0]\u001b[m \u001b[34mNVIDIA GeForce RTX 4090\u001b[m |\u001b[31m 40°C\u001b[m, \u001b[1m\u001b[32m 55 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m16701\u001b[m / \u001b[33m24564\u001b[m MB |\n",
      "\u001b[36m[1]\u001b[m \u001b[34mNVIDIA GeForce RTX 4090\u001b[m |\u001b[31m 48°C\u001b[m, \u001b[1m\u001b[32m 53 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m17013\u001b[m / \u001b[33m24564\u001b[m MB |\n",
      "\u001b[36m[2]\u001b[m \u001b[34mNVIDIA GeForce RTX 4090\u001b[m |\u001b[31m 46°C\u001b[m, \u001b[1m\u001b[32m 91 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m18281\u001b[m / \u001b[33m24564\u001b[m MB |\n",
      "\u001b[36m[3]\u001b[m \u001b[34mNVIDIA GeForce RTX 4090\u001b[m |\u001b[31m 49°C\u001b[m, \u001b[1m\u001b[32m 66 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m18305\u001b[m / \u001b[33m24564\u001b[m MB |\n",
      "\u001b[36m[4]\u001b[m \u001b[34mNVIDIA GeForce RTX 4090\u001b[m |\u001b[1m\u001b[31m 50°C\u001b[m, \u001b[1m\u001b[32m 45 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m17927\u001b[m / \u001b[33m24564\u001b[m MB |\n",
      "\u001b[36m[5]\u001b[m \u001b[34mNVIDIA GeForce RTX 4090\u001b[m |\u001b[1m\u001b[31m 51°C\u001b[m, \u001b[1m\u001b[32m 60 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m18163\u001b[m / \u001b[33m24564\u001b[m MB |\n"
     ]
    }
   ],
   "source": [
    "!gpustat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8f8a1154-37de-475f-8576-b1ab7bad1cec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.set_device(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da50865-1080-4846-8276-05bb1bde6c63",
   "metadata": {},
   "source": [
    "# Huggingface PEFT tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d370393-40ad-4580-958d-aa4e2e710250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peft version:  0.10.0\n",
      "accelerate version:  0.29.1\n",
      "datasets version:  2.18.0\n",
      "transformers version:  4.39.3\n",
      "diffusers version:  0.27.2\n"
     ]
    }
   ],
   "source": [
    "import peft\n",
    "import accelerate\n",
    "import datasets\n",
    "import transformers\n",
    "import diffusers\n",
    "\n",
    "print(\"peft version: \", peft.__version__)\n",
    "print(\"accelerate version: \", accelerate.__version__)\n",
    "print(\"datasets version: \", datasets.__version__)\n",
    "print(\"transformers version: \", transformers.__version__)\n",
    "print(\"diffusers version: \", diffusers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32bfcf2d-284e-4e31-93ee-8e9c3f4640e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import UNet2DConditionModel\n",
    "\n",
    "# unet = UNet2DConditionModel.from_pretrained(\n",
    "#     \"stabilityai/stable-diffusion-xl-base-1.0\", subfolder=\"unet\"\n",
    "# )\n",
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-2-base\", subfolder=\"unet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "117bff86-5e8f-4605-b06d-2d6352fb5377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unet.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00ce4a5e-afa6-44ec-b175-2503e1a66b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unet.to(device)\n",
    "# x = torch.rand((8, 4, 64, 64)).to(device)\n",
    "# time_step = torch.randn((8,)).to(device)\n",
    "# enc_h = torch.rand((8, 77, 1024)).to(device)\n",
    "# output = unet(x, timestep=time_step, encoder_hidden_states=enc_h).sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a4a7e66-302b-4699-8b2d-40f710e1efd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params:  865910724\n"
     ]
    }
   ],
   "source": [
    "model_size = 0\n",
    "for param in unet.parameters():\n",
    "    model_size += param.data.nelement()\n",
    "print(\"trainable params: \", model_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9262e31-7422-4846-a653-e740b9205a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find modules\n",
    "def find_modules(module, module_types):\n",
    "    matching_modules = []\n",
    "    for name, mod in module.named_modules():\n",
    "        if isinstance(mod, module_types):\n",
    "            module_name = name.split(\".\")[-1]\n",
    "            if len(module_name) == 1:\n",
    "                # print(name)\n",
    "                module_name = name\n",
    "            matching_modules.append(module_name)\n",
    "    return matching_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b47f908a-3fd2-4c85-8988-16338f08b415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0', 'conv_out', 'up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2', 'down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0', 'up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0', 'up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0', 'conv_shortcut', 'proj_in', 'up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0', 'up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0', 'up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0', 'down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0', 'down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2', 'to_v', 'down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0', 'down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2', 'time_emb_proj', 'conv1', 'up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2', 'linear_2', 'up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2', 'mid_block.attentions.0.transformer_blocks.0.ff.net.2', 'up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0', 'down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0', 'up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2', 'up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0', 'up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2', 'linear_1', 'to_q', 'up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2', 'mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0', 'up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2', 'up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2', 'up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0', 'down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2', 'to_k', 'down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2', 'down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0', 'up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0', 'up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0', 'down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2', 'down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0', 'up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0', 'conv2', 'proj', 'up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0', 'conv', 'up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0', 'down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0', 'proj_out', 'up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0', 'down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0', 'down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0', 'down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2', 'conv_in', 'up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0', 'down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0', 'up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0', 'down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0', 'mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0', 'down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0', 'up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0', 'up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2']\n"
     ]
    }
   ],
   "source": [
    "module_types = (torch.nn.Linear, torch.nn.Embedding, torch.nn.Conv2d)\n",
    "target_modules = find_modules(unet, module_types)\n",
    "target_modules = list(set(target_modules))\n",
    "print(target_modules)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fb3529-b565-424b-8f22-fc486446e3b0",
   "metadata": {},
   "source": [
    "## LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de5ba05f-c288-4fc8-b1b5-54bc2b0d493a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 829,952 || all params: 866,740,676 || trainable%: 0.09575551522864031\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "unet_lora = unet.to(device)\n",
    "\n",
    "# freeze params of models to save more memory\n",
    "unet_lora.requires_grad_(False)\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=4,\n",
    "    lora_alpha=4,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"to_q\", \"to_k\", \"to_v\", \"to_out.0\"],\n",
    "    init_lora_weights= \"gaussian\",\n",
    "    bias=\"none\"\n",
    ") # scale = alpha / r\n",
    "\n",
    "unet_lora.add_adapter(config)\n",
    "lora_layers = filter(lambda p: p.requires_grad, unet.parameters())\n",
    "\n",
    "trainable_params = 0\n",
    "all_params = 0\n",
    "for _, param in unet.named_parameters():\n",
    "    num_params = param.numel()\n",
    "    all_params += num_params\n",
    "    if param.requires_grad:\n",
    "        trainable_params += num_params\n",
    "print(f\"trainable params: {trainable_params:,d} || all params: {all_params:,d} || trainable%: {100 * trainable_params / all_params}\")\n",
    "unet_lora.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a434d80-df53-4511-b9d2-b29e5cb116f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand((8, 4, 64, 64)).to(device)\n",
    "time_step = torch.randn((8,)).to(device)\n",
    "enc_h = torch.rand((8, 77, 1024)).to(device)\n",
    "output = unet_lora(x, timestep=time_step, encoder_hidden_states=enc_h).sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80eae1fe-5595-4fcc-b6f7-6354b5758e6d",
   "metadata": {},
   "source": [
    "## LoKr (slightly different to KAdaptation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "369b6d2d-6f83-4e3c-8b0c-d835306fb582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 112,448 || all params: 866,023,172 || trainable%: 0.012984410075346113\n"
     ]
    }
   ],
   "source": [
    "from peft import LoKrConfig, get_peft_model\n",
    "\n",
    "unet_lokr = unet.to(device)\n",
    "\n",
    "# freeze params of models to save more memory\n",
    "unet_lokr.requires_grad_(False)\n",
    "\n",
    "config = LoKrConfig(\n",
    "    r=4,\n",
    "    alpha=4,\n",
    "    # rank_dropout=0.1,\n",
    "    module_dropout=0.1,\n",
    "    use_effective_conv2d=True,\n",
    "    target_modules=[\"to_q\", \"to_k\", \"to_v\", \"to_out.0\"]\n",
    ")\n",
    "\n",
    "unet_lokr.add_adapter(config)\n",
    "lokr_layers = filter(lambda p: p.requires_grad, unet.parameters())\n",
    "\n",
    "trainable_params = 0\n",
    "all_params = 0\n",
    "for _, param in unet.named_parameters():\n",
    "    num_params = param.numel()\n",
    "    all_params += num_params\n",
    "    if param.requires_grad:\n",
    "        trainable_params += num_params\n",
    "print(f\"trainable params: {trainable_params:,d} || all params: {all_params:,d} || trainable%: {100 * trainable_params / all_params}\")\n",
    "unet_lokr.train();\n",
    "# unet = get_peft_model(unet, config).to(device)\n",
    "# unet.print_trainable_parameters()\n",
    "# unet.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62877435-3c80-43aa-85fb-dd73216c4608",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand((8, 4, 64, 64)).to(device)\n",
    "time_step = torch.randn((8,)).to(device)\n",
    "enc_h = torch.rand((8, 77, 1024)).to(device)\n",
    "output = unet(x, timestep=time_step, encoder_hidden_states=enc_h).sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0163b3cc-0336-4acd-a43a-ac0503b08499",
   "metadata": {},
   "source": [
    "# Layer replication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd7589fe-1994-44f4-ab7a-f36f99cee7a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([0,1,0,2,0,3,1,2,1,3,2,3], dtype=torch.long, device='cuda').shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3598ec6-d22f-4ac2-ace2-d4f38e310126",
   "metadata": {},
   "source": [
    "# Adapt LoRA for Conv2D and Linear layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8fe831a3-f415-4286-bb56-a4ab67a13414",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Optional, Union, Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class LoRALinearLayer(nn.Module):\n",
    "    r\"\"\"\n",
    "    A linear layer that is used with LoRA.\n",
    "\n",
    "    Parameters:\n",
    "        in_features (`int`):\n",
    "            Number of input features.\n",
    "        out_features (`int`):\n",
    "            Number of output features.\n",
    "        rank (`int`, `optional`, defaults to 4):\n",
    "            The rank of the LoRA layer.\n",
    "        network_alpha (`float`, `optional`, defaults to `None`):\n",
    "            The value of the network alpha used for stable learning and preventing underflow. This value has the same\n",
    "            meaning as the `--network_alpha` option in the kohya-ss trainer script. See\n",
    "            https://github.com/darkstorm2150/sd-scripts/blob/main/docs/train_network_README-en.md#execute-learning\n",
    "        device (`torch.device`, `optional`, defaults to `None`):\n",
    "            The device to use for the layer's weights.\n",
    "        dtype (`torch.dtype`, `optional`, defaults to `None`):\n",
    "            The dtype to use for the layer's weights.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        rank: int = 4,\n",
    "        network_alpha: Optional[float] = None,\n",
    "        device: Optional[Union[torch.device, str]] = None,\n",
    "        dtype: Optional[torch.dtype] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.down = nn.Linear(in_features, rank, bias=False, device=device, dtype=dtype)\n",
    "        self.up = nn.Linear(rank, out_features, bias=False, device=device, dtype=dtype)\n",
    "        # This value has the same meaning as the `--network_alpha` option in the kohya-ss trainer script.\n",
    "        # See https://github.com/darkstorm2150/sd-scripts/blob/main/docs/train_network_README-en.md#execute-learning\n",
    "        self.network_alpha = network_alpha\n",
    "        self.rank = rank\n",
    "        self.out_features = out_features\n",
    "        self.in_features = in_features\n",
    "\n",
    "        nn.init.normal_(self.down.weight, std=1 / rank)\n",
    "        nn.init.zeros_(self.up.weight)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        orig_dtype = hidden_states.dtype\n",
    "        dtype = self.down.weight.dtype\n",
    "\n",
    "        down_hidden_states = self.down(hidden_states.to(dtype))\n",
    "        up_hidden_states = self.up(down_hidden_states)\n",
    "\n",
    "        if self.network_alpha is not None:\n",
    "            up_hidden_states *= self.network_alpha / self.rank\n",
    "\n",
    "        return up_hidden_states.to(orig_dtype)\n",
    "\n",
    "\n",
    "class LoRAConv2dLayer(nn.Module):\n",
    "    r\"\"\"\n",
    "    A convolutional layer that is used with LoRA.\n",
    "\n",
    "    Parameters:\n",
    "        in_features (`int`):\n",
    "            Number of input features.\n",
    "        out_features (`int`):\n",
    "            Number of output features.\n",
    "        rank (`int`, `optional`, defaults to 4):\n",
    "            The rank of the LoRA layer.\n",
    "        kernel_size (`int` or `tuple` of two `int`, `optional`, defaults to 1):\n",
    "            The kernel size of the convolution.\n",
    "        stride (`int` or `tuple` of two `int`, `optional`, defaults to 1):\n",
    "            The stride of the convolution.\n",
    "        padding (`int` or `tuple` of two `int` or `str`, `optional`, defaults to 0):\n",
    "            The padding of the convolution.\n",
    "        network_alpha (`float`, `optional`, defaults to `None`):\n",
    "            The value of the network alpha used for stable learning and preventing underflow. This value has the same\n",
    "            meaning as the `--network_alpha` option in the kohya-ss trainer script. See\n",
    "            https://github.com/darkstorm2150/sd-scripts/blob/main/docs/train_network_README-en.md#execute-learning\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        rank: int = 4,\n",
    "        kernel_size: Union[int, Tuple[int, int]] = (1, 1),\n",
    "        stride: Union[int, Tuple[int, int]] = (1, 1),\n",
    "        padding: Union[int, Tuple[int, int], str] = 0,\n",
    "        network_alpha: Optional[float] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.down = nn.Conv2d(in_features, rank, kernel_size=kernel_size, stride=stride, padding=padding, bias=False)\n",
    "        # according to the official kohya_ss trainer kernel_size are always fixed for the up layer\n",
    "        # # see: https://github.com/bmaltais/kohya_ss/blob/2accb1305979ba62f5077a23aabac23b4c37e935/networks/lora_diffusers.py#L129\n",
    "        self.up = nn.Conv2d(rank, out_features, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "\n",
    "        # This value has the same meaning as the `--network_alpha` option in the kohya-ss trainer script.\n",
    "        # See https://github.com/darkstorm2150/sd-scripts/blob/main/docs/train_network_README-en.md#execute-learning\n",
    "        self.network_alpha = network_alpha\n",
    "        self.rank = rank\n",
    "\n",
    "        nn.init.normal_(self.down.weight, std=1 / rank)\n",
    "        nn.init.zeros_(self.up.weight)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        orig_dtype = hidden_states.dtype\n",
    "        dtype = self.down.weight.dtype\n",
    "\n",
    "        down_hidden_states = self.down(hidden_states.to(dtype))\n",
    "        up_hidden_states = self.up(down_hidden_states)\n",
    "\n",
    "        if self.network_alpha is not None:\n",
    "            up_hidden_states *= self.network_alpha / self.rank\n",
    "\n",
    "        return up_hidden_states.to(orig_dtype)\n",
    "\n",
    "\n",
    "class LoRACompatibleConv(nn.Conv2d):\n",
    "    \"\"\"\n",
    "    A convolutional layer that can be used with LoRA.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, lora_layer: Optional[LoRAConv2dLayer] = None, **kwargs):\n",
    "\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.lora_layer = lora_layer\n",
    "\n",
    "    def set_lora_layer(self, lora_layer: Optional[LoRAConv2dLayer]):\n",
    "        self.lora_layer = lora_layer\n",
    "\n",
    "    def _fuse_lora(self, lora_scale: float = 1.0, safe_fusing: bool = False):\n",
    "        if self.lora_layer is None:\n",
    "            return\n",
    "\n",
    "        dtype, device = self.weight.data.dtype, self.weight.data.device\n",
    "\n",
    "        w_orig = self.weight.data.float()\n",
    "        w_up = self.lora_layer.up.weight.data.float()\n",
    "        w_down = self.lora_layer.down.weight.data.float()\n",
    "\n",
    "        if self.lora_layer.network_alpha is not None:\n",
    "            w_up = w_up * self.lora_layer.network_alpha / self.lora_layer.rank\n",
    "\n",
    "        fusion = torch.mm(w_up.flatten(start_dim=1), w_down.flatten(start_dim=1))\n",
    "        fusion = fusion.reshape((w_orig.shape))\n",
    "        fused_weight = w_orig + (lora_scale * fusion)\n",
    "\n",
    "        if safe_fusing and torch.isnan(fused_weight).any().item():\n",
    "            raise ValueError(\n",
    "                \"This LoRA weight seems to be broken. \"\n",
    "                f\"Encountered NaN values when trying to fuse LoRA weights for {self}.\"\n",
    "                \"LoRA weights will not be fused.\"\n",
    "            )\n",
    "\n",
    "        self.weight.data = fused_weight.to(device=device, dtype=dtype)\n",
    "\n",
    "        # we can drop the lora layer now\n",
    "        self.lora_layer = None\n",
    "\n",
    "        # offload the up and down matrices to CPU to not blow the memory\n",
    "        self.w_up = w_up.cpu()\n",
    "        self.w_down = w_down.cpu()\n",
    "        self._lora_scale = lora_scale\n",
    "\n",
    "    def _unfuse_lora(self):\n",
    "        if not (getattr(self, \"w_up\", None) is not None and getattr(self, \"w_down\", None) is not None):\n",
    "            return\n",
    "\n",
    "        fused_weight = self.weight.data\n",
    "        dtype, device = fused_weight.data.dtype, fused_weight.data.device\n",
    "\n",
    "        self.w_up = self.w_up.to(device=device).float()\n",
    "        self.w_down = self.w_down.to(device).float()\n",
    "\n",
    "        fusion = torch.mm(self.w_up.flatten(start_dim=1), self.w_down.flatten(start_dim=1))\n",
    "        fusion = fusion.reshape((fused_weight.shape))\n",
    "        unfused_weight = fused_weight.float() - (self._lora_scale * fusion)\n",
    "        self.weight.data = unfused_weight.to(device=device, dtype=dtype)\n",
    "\n",
    "        self.w_up = None\n",
    "        self.w_down = None\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor, scale: float = 1.0) -> torch.Tensor:\n",
    "        if self.padding_mode != \"zeros\":\n",
    "            hidden_states = F.pad(hidden_states, self._reversed_padding_repeated_twice, mode=self.padding_mode)\n",
    "            padding = (0, 0)\n",
    "        else:\n",
    "            padding = self.padding\n",
    "\n",
    "        original_outputs = F.conv2d(\n",
    "            hidden_states, self.weight, self.bias, self.stride, padding, self.dilation, self.groups\n",
    "        )\n",
    "\n",
    "        if self.lora_layer is None:\n",
    "            return original_outputs\n",
    "        else:\n",
    "            return original_outputs + (scale * self.lora_layer(hidden_states))\n",
    "\n",
    "\n",
    "class LoRACompatibleLinear(nn.Linear):\n",
    "    \"\"\"\n",
    "    A Linear layer that can be used with LoRA.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, lora_layer: Optional[LoRALinearLayer] = None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.lora_layer = lora_layer\n",
    "\n",
    "    def set_lora_layer(self, lora_layer: Optional[LoRALinearLayer]):\n",
    "        self.lora_layer = lora_layer\n",
    "\n",
    "    def _fuse_lora(self, lora_scale: float = 1.0, safe_fusing: bool = False):\n",
    "        if self.lora_layer is None:\n",
    "            return\n",
    "\n",
    "        dtype, device = self.weight.data.dtype, self.weight.data.device\n",
    "\n",
    "        w_orig = self.weight.data.float()\n",
    "        w_up = self.lora_layer.up.weight.data.float()\n",
    "        w_down = self.lora_layer.down.weight.data.float()\n",
    "\n",
    "        if self.lora_layer.network_alpha is not None:\n",
    "            w_up = w_up * self.lora_layer.network_alpha / self.lora_layer.rank\n",
    "\n",
    "        fused_weight = w_orig + (lora_scale * torch.bmm(w_up[None, :], w_down[None, :])[0])\n",
    "\n",
    "        if safe_fusing and torch.isnan(fused_weight).any().item():\n",
    "            raise ValueError(\n",
    "                \"This LoRA weight seems to be broken. \"\n",
    "                f\"Encountered NaN values when trying to fuse LoRA weights for {self}.\"\n",
    "                \"LoRA weights will not be fused.\"\n",
    "            )\n",
    "\n",
    "        self.weight.data = fused_weight.to(device=device, dtype=dtype)\n",
    "\n",
    "        # we can drop the lora layer now\n",
    "        self.lora_layer = None\n",
    "\n",
    "        # offload the up and down matrices to CPU to not blow the memory\n",
    "        self.w_up = w_up.cpu()\n",
    "        self.w_down = w_down.cpu()\n",
    "        self._lora_scale = lora_scale\n",
    "\n",
    "    def _unfuse_lora(self):\n",
    "        if not (getattr(self, \"w_up\", None) is not None and getattr(self, \"w_down\", None) is not None):\n",
    "            return\n",
    "\n",
    "        fused_weight = self.weight.data\n",
    "        dtype, device = fused_weight.dtype, fused_weight.device\n",
    "\n",
    "        w_up = self.w_up.to(device=device).float()\n",
    "        w_down = self.w_down.to(device).float()\n",
    "\n",
    "        unfused_weight = fused_weight.float() - (self._lora_scale * torch.bmm(w_up[None, :], w_down[None, :])[0])\n",
    "        self.weight.data = unfused_weight.to(device=device, dtype=dtype)\n",
    "\n",
    "        self.w_up = None\n",
    "        self.w_down = None\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor, scale: float = 1.0) -> torch.Tensor:\n",
    "        if self.lora_layer is None:\n",
    "            out = super().forward(hidden_states)\n",
    "            return out\n",
    "        else:\n",
    "            out = super().forward(hidden_states) + (scale * self.lora_layer(hidden_states))\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de2a8d9e-7805-422f-b3e6-98341cb76da6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# these two functions are used in the forward of the model\n",
    "# use case (UNet): https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/unets/unet_2d_condition.py\n",
    "def scale_lora_layers(model, weight):\n",
    "    \"\"\"\n",
    "    Adjust the weightage given to the LoRA layers of the model.\n",
    "\n",
    "    Args:\n",
    "        model (`torch.nn.Module`):\n",
    "            The model to scale.\n",
    "        weight (`float`):\n",
    "            The weight to be given to the LoRA layers.\n",
    "    \"\"\"\n",
    "    from peft.tuners.tuners_utils import BaseTunerLayer\n",
    "\n",
    "    if weight == 1.0:\n",
    "        return\n",
    "\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, BaseTunerLayer):\n",
    "            module.scale_layer(weight)\n",
    "\n",
    "\n",
    "def unscale_lora_layers(model, weight: Optional[float] = None):\n",
    "    \"\"\"\n",
    "    Removes the previously passed weight given to the LoRA layers of the model.\n",
    "\n",
    "    Args:\n",
    "        model (`torch.nn.Module`):\n",
    "            The model to scale.\n",
    "        weight (`float`, *optional*):\n",
    "            The weight to be given to the LoRA layers. If no scale is passed the scale of the lora layer will be\n",
    "            re-initialized to the correct value. If 0.0 is passed, we will re-initialize the scale with the correct\n",
    "            value.\n",
    "    \"\"\"\n",
    "    from peft.tuners.tuners_utils import BaseTunerLayer\n",
    "\n",
    "    if weight is None or weight == 1.0:\n",
    "        return\n",
    "\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, BaseTunerLayer):\n",
    "            if weight != 0:\n",
    "                module.unscale_layer(weight)\n",
    "            else:\n",
    "                for adapter_name in module.active_adapters:\n",
    "                    # if weight == 0 unscale should re-set the scale to the original value.\n",
    "                    module.set_scale(adapter_name, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "988c4877-860a-44fe-a8f4-0cc3ed4eac9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# example: Convolution Network (1 block)\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels*2, kernel_size=3, stride=stride, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(in_channels*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels*2, out_channels, kernel_size=3, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.residual_function(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class LoRAConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.residual_function = nn.Sequential(\n",
    "            LoRAConv2dLayer(in_channels, in_channels*2, kernel_size=3, stride=stride, padding=0),\n",
    "            nn.BatchNorm2d(in_channels*2),\n",
    "            nn.ReLU(),\n",
    "            LoRAConv2dLayer(in_channels*2, out_channels, kernel_size=3, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        scale_lora_layers(self, 1.)\n",
    "        x = self.residual_function(x)\n",
    "        x = self.relu(x)\n",
    "        unscale_lora_layers(self, 1.)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7d96b1c5-4fc3-4fc1-9aba-bbe21ed0be7e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['up', 'down']\n"
     ]
    }
   ],
   "source": [
    "def find_modules(module, module_types):\n",
    "    matching_modules = []\n",
    "    for name, mod in module.named_modules():\n",
    "        if isinstance(mod, module_types):\n",
    "            module_name = name.split(\".\")[-1]\n",
    "            if len(module_name) == 1:\n",
    "                # print(name)\n",
    "                module_name = name\n",
    "            matching_modules.append(module_name)\n",
    "    return matching_modules\n",
    "\n",
    "module_types = (torch.nn.Linear, torch.nn.Conv2d)\n",
    "target_modules = find_modules(example_cnn_lora, module_types)\n",
    "target_modules = list(set(target_modules))\n",
    "print(target_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aeac5ac9-7059-443c-8da3-07075baac5fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 622 || all params: 622 || trainable%: 100.0\n"
     ]
    }
   ],
   "source": [
    "example_cnn = ConvBlock(3, 8).to(device)\n",
    "example_cnn_lora = LoRAConvBlock(3, 8).to(device)\n",
    "\n",
    "trainable_params = 0\n",
    "all_params = 0\n",
    "for _, param in example_cnn.named_parameters():\n",
    "    num_params = param.numel()\n",
    "    all_params += num_params\n",
    "    if param.requires_grad:\n",
    "        trainable_params += num_params\n",
    "print(f\"trainable params: {trainable_params:,d} || all params: {all_params:,d} || trainable%: {100 * trainable_params / all_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "02635205-14da-4e8a-af9d-cc9ed178f6ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 444 || all params: 852 || trainable%: 52.1127\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "exmaple_cnn_lora = example_cnn_lora.to(device)\n",
    "\n",
    "# freeze params of models to save more memory\n",
    "example_cnn_lora.requires_grad_(False)\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=4,\n",
    "    lora_alpha=4,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"up\", \"down\"],\n",
    "    init_lora_weights= \"gaussian\",\n",
    "    bias=\"none\"\n",
    ") # scale = alpha / r\n",
    "\n",
    "example_cnn_lora = get_peft_model(example_cnn_lora, config).to(device)\n",
    "example_cnn_lora.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ec1017a4-b620-497f-b6ab-377737fa8f3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img = torch.rand((8, 3, 224, 224)).to(device) # B, C, H, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "35543eb7-9d7c-47aa-846b-c915bde983d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8, 220, 220])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_cnn(img).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "aa108c5c-0ca3-44d7-9be0-7bcecb163c6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8, 220, 220])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_cnn_lora(img).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mambadance",
   "language": "python",
   "name": "mambadance"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
