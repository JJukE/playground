{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17b93fe4-d49a-4600-90a5-b8ce2a5e625b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/dev/playground/modules/mamba'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"/root/dev/playground/modules/mamba\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b349812-98fd-47dc-a0af-addfa0f804a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.set_device(5)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36f4f0cc-11ad-4cdf-a853-3dd27bec7945",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m794894ff7784              \u001b[m  Tue Oct 22 00:24:39 2024  \u001b[1m\u001b[30m535.154.05\u001b[m\n",
      "\u001b[36m[0]\u001b[m \u001b[34mNVIDIA GeForce RTX 4090\u001b[m |\u001b[31m 29°C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m 2536\u001b[m / \u001b[33m24564\u001b[m MB |\n",
      "\u001b[36m[1]\u001b[m \u001b[34mNVIDIA GeForce RTX 4090\u001b[m |\u001b[31m 32°C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m    3\u001b[m / \u001b[33m24564\u001b[m MB |\n",
      "\u001b[36m[2]\u001b[m \u001b[34mNVIDIA GeForce RTX 4090\u001b[m |\u001b[31m 28°C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m 2234\u001b[m / \u001b[33m24564\u001b[m MB |\n",
      "\u001b[36m[3]\u001b[m \u001b[34mNVIDIA GeForce RTX 4090\u001b[m |\u001b[31m 31°C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m    3\u001b[m / \u001b[33m24564\u001b[m MB |\n",
      "\u001b[36m[4]\u001b[m \u001b[34mNVIDIA GeForce RTX 4090\u001b[m |\u001b[31m 33°C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m    3\u001b[m / \u001b[33m24564\u001b[m MB |\n",
      "\u001b[36m[5]\u001b[m \u001b[34mNVIDIA GeForce RTX 4090\u001b[m |\u001b[31m 35°C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m  390\u001b[m / \u001b[33m24564\u001b[m MB |\n"
     ]
    }
   ],
   "source": [
    "!gpustat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224ea33a-456c-4aad-b1ee-8ba467c6d211",
   "metadata": {},
   "source": [
    "# Selective scan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecbc366-35f5-4de5-b473-a4b60a3fbd8b",
   "metadata": {},
   "source": [
    "- https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/triton/selective_state_update.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abb0f9e3-5b49-4053-bed8-fc046632e95b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import triton\n",
    "import triton.language as tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf2193f0-49ca-456d-9a6d-3d6af3ee6278",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/triton/softplus.py\n",
    "@triton.jit\n",
    "def softplus(dt):\n",
    "    return tl.math.log1p(tl.exp(dt)) # log(1 + exp(dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652a5d7c-3fdf-4b73-9f0d-56976b5ea8b1",
   "metadata": {},
   "source": [
    "- The derivative of softplus is sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6177e0b-7ee0-4762-976d-4aa80abfedb8",
   "metadata": {},
   "source": [
    "### _selective_scan_update_kernel function\n",
    "\n",
    "Terminologies\n",
    "- Stride: The step size (in memory) between consecutive elements along a specific dimension of a multi-dimensional tensor.\n",
    "    - stride_x_batch: Tells how far to move in memory to get to the **next batch** of the `x` tensor\n",
    "    - stride_x_head: Tells how far to move in memory to get to the **next head** of the `x` tensor\n",
    "    - stride_x_dim: Tells how far to move in memory to get to the **next dim** of the `x` tensor\n",
    "- Representations of dimensions\n",
    "    $$ \\tilde{A} \\in \\mathbb{R}^{(D, N)}, \\; \\tilde{B}, \\tilde{C} \\in \\mathbb{R}^{(B, L, N)}, \\; \\Delta \\in \\mathbb{R}^{(B, L, D)}, \\; A, B \\in \\mathbb{R}^{(B, L, D, N)}  $$\n",
    "    - **B**: batch\n",
    "    - **L**: head\n",
    "    - **D**: dim\n",
    "    - **N**: dstate\n",
    "\n",
    "Triton-based GPU kernel designed for efficient updates of a recurrent state using selective state-space models (SSMs).\n",
    "\n",
    "- Inputs: pointers to different matrices representing states, inputs, weights (A, B, C, D), and other meta-parameters, which correspond to various components of SSMs.\n",
    "    - state_ptr: points to the recurrent state that is updated\n",
    "    - x_ptr: Input matrix for the current time step\n",
    "    - dt_ptr: Delta time for the update (can vary across steps)\n",
    "    - A_ptr, B_ptr, C_ptr: SSM parameter matrices\n",
    "    - (Optional) dt_bias_ptr, D_ptr, z_ptr: bias, weights for direct input-to-output connection, and auxiliary gating term\n",
    "    - Meta-parameters\n",
    "        - TIE-HDIM: whether it simplifies calculations by assuming head dimensions are tied, reproducing complexity\n",
    "        - BLOCK_SIZE_M: determines the size of blocks processed in parallel (based on GPU memory constraints)\n",
    "        - HAS_DT_BIAS, HAS_D, HAS_Z: Flags indicating whether certain inputs are present\n",
    "\n",
    "So, the corresponding equation is:\n",
    "$$ \\begin{align*} h_t &= A_t h_{t-1} + B_t x_t \\\\ y_t &= C_t^\\top h_t \\end{align*} $$\n",
    "\n",
    "- $\\Delta t = \\tau(\\operatorname{Linear}(x_t)) = \\operatorname{softplus}(\\operatorname{Linear}(x_t))$ (by the proof of Theorem 1 in the paper)\n",
    "- $A = \\operatorname{exp}(\\tilde{A} \\cdot \\Delta t)$\n",
    "- $B = \\sigma(\\operatorname{Linear}(x_t))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0474ba94-71f1-430f-8541-6a9b2d375b1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# perform the discretization and recurrence in SRAM\n",
    "@triton.heuristics({\"HAS_DT_BIAS\": lambda args: args[\"dt_bias_ptr\"] is not None})\n",
    "@triton.heuristics({\"HAS_D\": lambda args: args[\"D_ptr\"] is not None})\n",
    "@triton.heuristics({\"HAS_Z\": lambda args: args[\"z_ptr\"] is not None})\n",
    "@triton.heuristics({\"BLOCK_SIZE_DSTATE\": lambda args: triton.next_power_of_2(args[\"dstate\"])})\n",
    "@triton.jit\n",
    "def _selective_scan_update_kernel(\n",
    "    # Pointers to matrices\n",
    "    state_ptr, x_ptr, dt_ptr, dt_bias_ptr, A_ptr, B_ptr, C_ptr, D_ptr, z_ptr, out_ptr,\n",
    "    # Matrix dimensions\n",
    "    batch, nheads, dim, dstate, nheads_ngroups_ratio,\n",
    "    # Strides\n",
    "    stride_state_batch, stride_state_head, stride_state_dim, stride_state_dstate,\n",
    "    stride_x_batch, stride_x_head, stride_x_dim,\n",
    "    stride_dt_batch, stride_dt_head, stride_dt_dim,\n",
    "    stride_dt_bias_head, stride_dt_bias_dim,\n",
    "    stride_A_head, stride_A_dim, stride_A_dstate,\n",
    "    stride_B_batch, stride_B_group, stride_B_dstate,\n",
    "    stride_C_batch, stride_C_group, stride_C_dstate,\n",
    "    stride_D_head, stride_D_dim,\n",
    "    stride_z_batch, stride_z_head, stride_z_dim,\n",
    "    stride_out_batch, stride_out_head, stride_out_dim,\n",
    "    # Meta-parameters\n",
    "    DT_SOFTPLUS: tl.constexpr,\n",
    "    TIE_HDIM: tl.constexpr,\n",
    "    BLOCK_SIZE_M: tl.constexpr,\n",
    "    HAS_DT_BIAS: tl.constexpr, HAS_D: tl.constexpr, HAS_Z: tl.constexpr,\n",
    "    BLOCK_SIZE_DSTATE: tl.constexpr,\n",
    "    ):\n",
    "    # program id and pointer adjustments\n",
    "    pid_m, pid_b, pid_h = tl.program_id(axis=0), tl.program_id(axis=1), tl.program_id(axis=2)\n",
    "    state_ptr += pid_b * stride_state_batch + pid_h * stride_state_head\n",
    "    x_ptr += pid_b * stride_x_batch + pid_h * stride_x_head\n",
    "    dt_ptr += pid_b * stride_dt_batch + pid_h * stride_dt_head\n",
    "    if HAS_DT_BIAS:\n",
    "        dt_bias_ptr += pid_h * stride_dt_bias_head\n",
    "    A_ptr += pid_h * stride_A_head\n",
    "    B_ptr += pid_b * stride_B_batch + (pid_h // nheads_ngroups_ratio) * stride_B_group\n",
    "    C_ptr += pid_b * stride_C_batch + (pid_h // nheads_ngroups_ratio) * stride_C_group\n",
    "    if HAS_Z:\n",
    "        z_ptr += pid_b * stride_z_batch + pid_h * stride_z_head\n",
    "    out_ptr += pid_b * stride_out_batch + pid_h * stride_out_head\n",
    "    \n",
    "    # pointers for loading blocks of data\n",
    "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M) # offset along the first dimension (dim)\n",
    "    offs_n = tl.arange(0, BLOCK_SIZE_DSTATE) # offset along the second dimension (dstate)\n",
    "    state_ptrs = state_ptr + (offs_m[:, None] * stride_state_dim + offs_n[None, :] * stride_state_dstate) # (stride_state_dim, stride_state_dstate)\n",
    "    x_ptrs, dt_ptrs = x_ptr + offs_m * stride_x_dim, dt_ptr + offs_m * stride_dt_dim\n",
    "    if HAS_DT_BIAS:\n",
    "        dt_bias_ptrs = dt_bias_ptr + offs_m * stride_dt_bias_dim\n",
    "    if HAS_D:\n",
    "        D_ptr += pid_h * stride_D_head\n",
    "    A_ptrs = A_ptr + (offs_m[:, None] * stride_A_dim + offs_n[None, :] * stride_A_dstate)\n",
    "    B_ptrs = B_ptr + offs_n * stride_B_dstate\n",
    "    C_ptrs = C_ptr + offs_n * stride_C_dstate\n",
    "    if HAS_D:\n",
    "        D_ptrs = D_ptr + offs_m * stride_D_dim\n",
    "    if HAS_Z:\n",
    "        z_ptrs = z_ptr + offs_m * stride_z_dim\n",
    "    out_ptrs = out_ptr + offs_m * stride_out_dim\n",
    "    \n",
    "    # loading and discretization\n",
    "    state = tl.load(state_ptrs, mask=(offs_m[:, None] < dim) & (offs_n[None, :] < dstate), other=0.)\n",
    "    x = tl.load(x_ptrs, mask=offs_m < dim, other=0.).to(tl.float32)\n",
    "    if not TIE_HDIM:\n",
    "        dt = tl.load(dt_ptrs, mask=offs_m < dim, other=0.).to(tl.float32)\n",
    "        if HAS_DT_BIAS:\n",
    "            dt += tl.load(dt_bias_ptrs, mask=offs_m < dim, other=0.).to(tl.float32)\n",
    "        if DT_SOFTPLUS:\n",
    "            dt = tl.where(dt <= 20., softplus(dt), dt) # dt = softplus(dt) = log(1 + exp(dt))\n",
    "        A = tl.load(A_ptrs, mask=(offs_m[:, None] < dim) & (offs_n[None, :] < dstate), other=0.).to(tl.float32)\n",
    "        dA = tl.exp(A * dt[:, None]) # A_t = exp(\\Delta A) = exp(A \\Delta t)\n",
    "    else:\n",
    "        dt = tl.load(dt_ptr).to(tl.float32)\n",
    "        if HAS_DT_BIAS:\n",
    "            dt += tl.load(dt_bias_ptr).to(tl.float32)\n",
    "        if DT_SOFTPLUS:\n",
    "            dt = tl.where(dt <= 20., softplus(dt), dt) # dt = softplus(dt) = log(1 + exp(dt))\n",
    "        A = tl.load(A_ptr).to(tl.float32)\n",
    "        dA = tl.exp(A * dt) # scalar, not a matrix\n",
    "    \n",
    "    B = tl.load(B_ptrs, mask=offs_n < dstate, other=0.).to(tl.float32)\n",
    "    C = tl.load(C_ptrs, mask=offs_n < dstate, other=0.).to(tl.float32)\n",
    "    if HAS_D:\n",
    "        D = tl.load(D_ptrs, mask=offs_m < dim, other=0.).to(tl.float32)\n",
    "    if HAS_Z:\n",
    "        z = tl.load(z_ptrs, mask=offs_m < dim, other=0.).to(tl.float32)\n",
    "    \n",
    "    if not TIE_HDIM:\n",
    "        dB = B[None, :] * dt[:, None] # B_t = sigmoid(Linear(x_t)) → not explicitly reprsented here\n",
    "    else:\n",
    "        dB = B * dt # vector of size (dstate,)\n",
    "    state = state * dA + dB * x[:, None] # Linear(x_t)\n",
    "    tl.store(state_ptrs, state, mask=(offs_m[:, None] < dim) & (offs_n[None, :] < dstate))\n",
    "    \n",
    "    # output calculation\n",
    "    out = tl.sum(state * C[None, :], axis=1)\n",
    "    if HAS_D:\n",
    "        out += x * D\n",
    "    if HAS_Z:\n",
    "        out *= z * tl.sigmoid(z)\n",
    "    tl.store(out_ptrs, out, mask=offs_m < dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afce6c2b-4fa6-4566-9065-82cfef27cc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selective_state_update(state, x, dt, A, B, C, D=None, z=None, dt_bias=None, dt_softplus=False):\n",
    "    has_heads = state.dim() > 3\n",
    "    if state.dim() == 3:\n",
    "        state = state.unsqueeze(1) # (B, L, D, N)\n",
    "    if x.dim() == 2:\n",
    "        x = x.unsqueeze(1) # (B, L, D)\n",
    "    if dt.dim() == 2:\n",
    "        dt = dt.unsqueeze(1) # (B, L, D)\n",
    "    if A.dim() == 2: # (D, N)\n",
    "        A = A.unsqueeze(0) # (L, D, N)\n",
    "    if B.dim() == 2:\n",
    "        B = B.unsqueeze(1) # (B, L, N)\n",
    "    if C.dim() == 2:\n",
    "        C = C.unsqueeze(1) # (B, L, N)\n",
    "    if D is not None and D.dim() == 1:\n",
    "        D = D.unsqueeze(0) # (L, D)\n",
    "    if z is not None and z.dim() == 2:\n",
    "        z = z.unsqueeze(1) # (B, L, D)\n",
    "    if dt_bias is not None and dt_bias.dim() == 1:\n",
    "        dt_bias = dt_bias.unsqueeze(0) # (L, D)\n",
    "    \n",
    "    batch, nheads, dim, dstate = state.shape # B L D N\n",
    "    assert x.shape == (batch, nheads, dim)\n",
    "    assert dt.shape == x.shape\n",
    "    assert A.shape == (nheads, dim, dstate)\n",
    "    ngroups = B.shape[1]\n",
    "    assert nheads % ngroups == 0, \"nheads must be divisible by ngroups\"\n",
    "    assert B.shape == (batch, ngroups, dstate)\n",
    "    assert C.shape == B.shape\n",
    "    if D is not None:\n",
    "        assert D.shape == (nheads, dim)\n",
    "    if z is not None:\n",
    "        assert z.shape == x.shape\n",
    "    if dt_bias is not None:\n",
    "        assert dt_bias.shape == (nheads, dim)\n",
    "    \n",
    "    out = torch.empty_like(x)\n",
    "    grid = lambda META: (triton.cdiv(dim, META['BLOCK_SIZE_M']), batch, nheads)\n",
    "    z_strides = ((z.stride(0), z.stride(1), z.stride(2)) if z is not None else (0, 0, 0))\n",
    "    \n",
    "    # We don't want to autotune since it will overwrite the state\n",
    "    # Instead, we tune by hand.\n",
    "    BLOCK_SIZE_M = num_warps = ((32, 4) if dstate <= 16\n",
    "                                else ((16, 4) if dstate <= 32 else\n",
    "                                      ((8, 4) if dstate <= 64 else\n",
    "                                      ((4, 4) if dstate <= 128 else\n",
    "                                      ((4, 8))))))\n",
    "    tie_hdim = A.stride(-1) == 0 and A.stride(-2) == 0 and dt.stride(-1) == 0 and dt_bias.stride(-1) == 0\n",
    "    with torch.cuda.device(x.device.index):\n",
    "        _selective_scan_update_kernel[grid](\n",
    "            state, x, dt, dt_bias, A, B, C, D, z, out,\n",
    "            batch, nheads, dim, dstate, nheads // ngroups,\n",
    "            state.stride(0), state.stride(1), state.stride(2), state.stride(3),\n",
    "            x.stride(0), x.stride(1), x.stride(2),\n",
    "            dt.stride(0), dt.stride(1), dt.stride(2),\n",
    "            *(dt_bias.stride(0), dt_bias.stride(1)) if dt_bias is not None else 0,\n",
    "            A.stride(0), A.stride(1), A.stride(2),\n",
    "            B.stride(0), B.stride(1), B.stride(2),\n",
    "            C.stride(0), C.stride(1), C.stride(2),\n",
    "            *(D.stride(0), D.stride(1)) if D is not None else 0,\n",
    "            z_strides[0], z_strides[1], z_strides[2],\n",
    "            out.stride(0), out.stride(1), out.stride(2),\n",
    "            dt_softplus, tie_hdim, BLOCK_SIZE_M, num_warps=num_warps)\n",
    "        if not has_heads:\n",
    "            out = out.squeeze(1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770e558b-e522-4ee1-82b0-add65c3ef823",
   "metadata": {},
   "source": [
    "## Mamba Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35528f0e-7574-4946-a139-2b4efcd7f9b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CausalConv1dFn(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(\n",
    "        ctx,\n",
    "        x,\n",
    "        weight,\n",
    "        bias=None,\n",
    "        seq_idx=None,\n",
    "        initial_states=None,\n",
    "        return_final_states=False,\n",
    "        final_states_out=None,\n",
    "        activation=None,\n",
    "    ):\n",
    "        if activation not in [None, \"silu\", \"swish\"]:\n",
    "            raise NotImplementedError(\"activation must be None, silu, or swish\")\n",
    "        if x.stride(2) != 1 and x.stride(1) != 1:\n",
    "            x = x.contiguous()\n",
    "        bias = bias.contiguous() if bias is not None else None\n",
    "        if seq_idx is not None:\n",
    "            assert (\n",
    "                initial_states is None\n",
    "            ), \"initial_states must be None if seq_idx is not None\"\n",
    "            assert (\n",
    "                not return_final_states\n",
    "            ), \"If seq_idx is not None, we don't return final_states_out\"\n",
    "        seq_idx = seq_idx.contiguous() if seq_idx is not None else None\n",
    "        if initial_states is not None and (\n",
    "            initial_states.stride(2) != 1 and initial_states.stride(1) != 1\n",
    "        ):\n",
    "            initial_states = initial_states.contiguous()\n",
    "        if return_final_states:\n",
    "            assert (\n",
    "                x.stride(1) == 1\n",
    "            ), \"Only channel-last layout support returning final_states_out\"\n",
    "            if final_states_out is not None:\n",
    "                assert (\n",
    "                    final_states_out.stride(2) == 1 or final_states_out.stride(1) == 1\n",
    "                )\n",
    "            else:\n",
    "                batch, dim, seqlen = x.shape\n",
    "                width = weight.shape[1]\n",
    "                final_states_out = torch.empty(\n",
    "                    batch, width - 1, dim, device=x.device, dtype=x.dtype\n",
    "                ).transpose(1, 2)\n",
    "        else:\n",
    "            final_states_out = None\n",
    "        ctx.activation = activation in [\"silu\", \"swish\"]\n",
    "        out = causal_conv1d_cuda.causal_conv1d_fwd(\n",
    "            x, weight, bias, seq_idx, initial_states, final_states_out, ctx.activation\n",
    "        )\n",
    "        ctx.save_for_backward(x, weight, bias, seq_idx, initial_states)\n",
    "        ctx.return_final_states = return_final_states\n",
    "        ctx.return_dinitial_states = (\n",
    "            initial_states is not None and initial_states.requires_grad\n",
    "        )\n",
    "        return out if not return_final_states else (out, final_states_out)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, dout, *args):\n",
    "        x, weight, bias, seq_idx, initial_states = ctx.saved_tensors\n",
    "        dfinal_states = args[0] if ctx.return_final_states else None\n",
    "        if dout.stride(2) != 1 and dout.stride(1) != 1:\n",
    "            dout = dout.contiguous()\n",
    "        # The kernel supports passing in a pre-allocated dx (e.g., in case we want to fuse the\n",
    "        # backward of conv1d with the backward of chunk).\n",
    "        # Here we just pass in None and dx will be allocated in the C++ code.\n",
    "        dx, dweight, dbias, dinitial_states = causal_conv1d_cuda.causal_conv1d_bwd(\n",
    "            x,\n",
    "            weight,\n",
    "            bias,\n",
    "            dout,\n",
    "            seq_idx,\n",
    "            initial_states,\n",
    "            dfinal_states,\n",
    "            None,\n",
    "            ctx.return_dinitial_states,\n",
    "            ctx.activation,\n",
    "        )\n",
    "        return (\n",
    "            dx,\n",
    "            dweight,\n",
    "            dbias if bias is not None else None,\n",
    "            None,\n",
    "            dinitial_states if initial_states is not None else None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "        )\n",
    "\n",
    "\n",
    "def causal_conv1d_fn(\n",
    "    x,\n",
    "    weight,\n",
    "    bias=None,\n",
    "    seq_idx=None,\n",
    "    initial_states=None,\n",
    "    return_final_states=False,\n",
    "    final_states_out=None,\n",
    "    activation=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    x: (batch, dim, seqlen)\n",
    "    weight: (dim, width)\n",
    "    bias: (dim,)\n",
    "    seq_idx: (batch, seqlen)\n",
    "    initial_states: (batch, dim, width - 1)\n",
    "    final_states_out: (batch, dim, width - 1), to be written to\n",
    "    activation: either None or \"silu\" or \"swish\"\n",
    "\n",
    "    out: (batch, dim, seqlen)\n",
    "    \"\"\"\n",
    "    return CausalConv1dFn.apply(\n",
    "        x,\n",
    "        weight,\n",
    "        bias,\n",
    "        seq_idx,\n",
    "        initial_states,\n",
    "        return_final_states,\n",
    "        final_states_out,\n",
    "        activation,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9cdaa6-0673-4e86-898d-5a6f9183b632",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import custom_bwd, custom_fwd\n",
    "\n",
    "class MambaInnerFn(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    @custom_fwd\n",
    "    def forward(ctx, xz, conv1d_weight, conv1d_bias, x_proj_weight, delta_proj_weight,\n",
    "                out_proj_weight, out_proj_bias,\n",
    "                A, B=None, C=None, D=None, delta_bias=None, B_proj_bias=None,\n",
    "                C_proj_bias=None, delta_softplus=True, checkpoint_lvl=1):\n",
    "        \"\"\" xz: (batch, dim, seqlen) \"\"\"\n",
    "        assert checkpoint_lvl in [0, 1]\n",
    "        L = xz.shape[-1]\n",
    "        delta_rank = delta_proj_weight.shape[1]\n",
    "        d_state = A.shape[-1] * (1 if not A.is_complex() else 2)\n",
    "        if xz.stride(-1) != 1:\n",
    "            xz = xz.contiguous()\n",
    "        conv1d_weight = rearrange(conv1d_weight, \"d 1 w -> d w\")\n",
    "        x, z = xz.chunk(2, dim=1)\n",
    "        conv1d_bias = conv1d_bias.contiguous() if conv1d_bias is not None else None\n",
    "        conv1d_out = causal_conv1d_cuda.causal_conv1d_fwd(x, conv1d_weight, conv1d_bias, None, None, None, True)\n",
    "        \n",
    "        # We're being very careful here about the layout, to avoid extra transposes.\n",
    "        # We want delta to have d as the slowest moving dimension\n",
    "        # and L as the fastest moving dimension, since those are what the ssm_scan kernel expects.\n",
    "        x_dbl = F.linear(rearrange(conv1d_out, 'b d l -> (b l) d'), x_proj_weight)  # (bl d)\n",
    "        delta = rearrange(delta_proj_weight @ x_dbl[:, :delta_rank].t(), \"d (b l) -> b d l\", l = L)\n",
    "        ctx.is_variable_B = B is None\n",
    "        ctx.is_variable_C = C is None\n",
    "        ctx.B_proj_bias_is_None = B_proj_bias is None\n",
    "        ctx.C_proj_bias_is_None = C_proj_bias is None\n",
    "        if B is None:  # variable B\n",
    "            B = x_dbl[:, delta_rank:delta_rank + d_state]  # (bl dstate)\n",
    "            if B_proj_bias is not None:\n",
    "                B = B + B_proj_bias.to(dtype=B.dtype)\n",
    "            if not A.is_complex():\n",
    "                # B = rearrange(B, \"(b l) dstate -> b dstate l\", l=L).contiguous()\n",
    "                B = rearrange(B, \"(b l) dstate -> b 1 dstate l\", l=L).contiguous()\n",
    "            else:\n",
    "                B = rearrange(B, \"(b l) (dstate two) -> b 1 dstate (l two)\", l=L, two=2).contiguous()\n",
    "        else:\n",
    "            if B.stride(-1) != 1:\n",
    "                B = B.contiguous()\n",
    "        if C is None:  # variable C\n",
    "            C = x_dbl[:, -d_state:]  # (bl dstate)\n",
    "            if C_proj_bias is not None:\n",
    "                C = C + C_proj_bias.to(dtype=C.dtype)\n",
    "            if not A.is_complex():\n",
    "                # C = rearrange(C, \"(b l) dstate -> b dstate l\", l=L).contiguous()\n",
    "                C = rearrange(C, \"(b l) dstate -> b 1 dstate l\", l=L).contiguous()\n",
    "            else:\n",
    "                C = rearrange(C, \"(b l) (dstate two) -> b 1 dstate (l two)\", l=L, two=2).contiguous()\n",
    "        else:\n",
    "            if C.stride(-1) != 1:\n",
    "                C = C.contiguous()\n",
    "        if D is not None:\n",
    "            D = D.contiguous()\n",
    "        out, scan_intermediates, out_z = selective_scan_cuda.fwd(\n",
    "            conv1d_out, delta, A, B, C, D, z, delta_bias, delta_softplus\n",
    "        )\n",
    "        ctx.delta_softplus = delta_softplus\n",
    "        ctx.out_proj_bias_is_None = out_proj_bias is None\n",
    "        ctx.checkpoint_lvl = checkpoint_lvl\n",
    "        if checkpoint_lvl >= 1:  # Will recompute conv1d_out and delta in the backward pass\n",
    "            conv1d_out, delta = None, None\n",
    "        ctx.save_for_backward(xz, conv1d_weight, conv1d_bias, x_dbl, x_proj_weight,\n",
    "                              delta_proj_weight, out_proj_weight, conv1d_out, delta,\n",
    "                              A, B, C, D, delta_bias, scan_intermediates, out)\n",
    "        return F.linear(rearrange(out_z, \"b d l -> b l d\"), out_proj_weight, out_proj_bias)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    @custom_bwd\n",
    "    def backward(ctx, dout):\n",
    "        # dout: (batch, seqlen, dim)\n",
    "        assert causal_conv1d_cuda is not None, \"causal_conv1d_cuda is not available. Please install causal-conv1d.\"\n",
    "        (xz, conv1d_weight, conv1d_bias, x_dbl, x_proj_weight, delta_proj_weight, out_proj_weight,\n",
    "         conv1d_out, delta, A, B, C, D, delta_bias, scan_intermediates, out) = ctx.saved_tensors\n",
    "        L = xz.shape[-1]\n",
    "        delta_rank = delta_proj_weight.shape[1]\n",
    "        d_state = A.shape[-1] * (1 if not A.is_complex() else 2)\n",
    "        x, z = xz.chunk(2, dim=1)\n",
    "        if dout.stride(-1) != 1:\n",
    "            dout = dout.contiguous()\n",
    "        if ctx.checkpoint_lvl == 1:\n",
    "            conv1d_out = causal_conv1d_cuda.causal_conv1d_fwd(\n",
    "                x, conv1d_weight, conv1d_bias, None, None, None, True\n",
    "            )\n",
    "            delta = rearrange(delta_proj_weight @ x_dbl[:, :delta_rank].t(),\n",
    "                              \"d (b l) -> b d l\", l = L)\n",
    "        # The kernel supports passing in a pre-allocated dz (e.g., in case we want to fuse the\n",
    "        # backward of selective_scan_cuda with the backward of chunk).\n",
    "        dxz = torch.empty_like(xz)  # (batch, dim, seqlen)\n",
    "        dx, dz = dxz.chunk(2, dim=1)\n",
    "        dout = rearrange(dout, \"b l e -> e (b l)\")\n",
    "        dout_y = rearrange(out_proj_weight.t() @ dout, \"d (b l) -> b d l\", l=L)\n",
    "        dconv1d_out, ddelta, dA, dB, dC, dD, ddelta_bias, dz, out_z = selective_scan_cuda.bwd(\n",
    "            conv1d_out, delta, A, B, C, D, z, delta_bias, dout_y, scan_intermediates, out, dz,\n",
    "            ctx.delta_softplus,\n",
    "            True  # option to recompute out_z\n",
    "        )\n",
    "        dout_proj_weight = torch.einsum(\"eB,dB->ed\", dout, rearrange(out_z, \"b d l -> d (b l)\"))\n",
    "        dout_proj_bias = dout.sum(dim=(0, 1)) if not ctx.out_proj_bias_is_None else None\n",
    "        dD = dD if D is not None else None\n",
    "        dx_dbl = torch.empty_like(x_dbl)\n",
    "        dB_proj_bias = None\n",
    "        if ctx.is_variable_B:\n",
    "            if not A.is_complex():\n",
    "                dB = rearrange(dB, \"b 1 dstate l -> (b l) dstate\").contiguous()\n",
    "            else:\n",
    "                dB = rearrange(dB, \"b 1 dstate (l two) -> (b l) (dstate two)\", two=2).contiguous()\n",
    "            dB_proj_bias = dB.sum(0) if not ctx.B_proj_bias_is_None else None\n",
    "            dx_dbl[:, delta_rank:delta_rank + d_state] = dB  # (bl d)\n",
    "            dB = None\n",
    "        dC_proj_bias = None\n",
    "        if ctx.is_variable_C:\n",
    "            if not A.is_complex():\n",
    "                dC = rearrange(dC, \"b 1 dstate l -> (b l) dstate\").contiguous()\n",
    "            else:\n",
    "                dC = rearrange(dC, \"b 1 dstate (l two) -> (b l) (dstate two)\", two=2).contiguous()\n",
    "            dC_proj_bias = dC.sum(0) if not ctx.C_proj_bias_is_None else None\n",
    "            dx_dbl[:, -d_state:] = dC  # (bl d)\n",
    "            dC = None\n",
    "        ddelta = rearrange(ddelta, \"b d l -> d (b l)\")\n",
    "        ddelta_proj_weight = torch.einsum(\"dB,Br->dr\", ddelta, x_dbl[:, :delta_rank])\n",
    "        dx_dbl[:, :delta_rank] = torch.einsum(\"dB,dr->Br\", ddelta, delta_proj_weight)\n",
    "        dconv1d_out = rearrange(dconv1d_out, \"b d l -> d (b l)\")\n",
    "        dx_proj_weight = torch.einsum(\"Br,Bd->rd\", dx_dbl, rearrange(conv1d_out, \"b d l -> (b l) d\"))\n",
    "        dconv1d_out = torch.addmm(dconv1d_out, x_proj_weight.t(), dx_dbl.t(), out=dconv1d_out)\n",
    "        dconv1d_out = rearrange(dconv1d_out, \"d (b l) -> b d l\", b=x.shape[0], l=x.shape[-1])\n",
    "        # The kernel supports passing in a pre-allocated dx (e.g., in case we want to fuse the\n",
    "        # backward of conv1d with the backward of chunk).\n",
    "        dx, dconv1d_weight, dconv1d_bias, *_ = causal_conv1d_cuda.causal_conv1d_bwd(\n",
    "            x, conv1d_weight, conv1d_bias, dconv1d_out, None, None, None, dx, False, True\n",
    "        )\n",
    "        dconv1d_bias = dconv1d_bias if conv1d_bias is not None else None\n",
    "        dconv1d_weight = rearrange(dconv1d_weight, \"d w -> d 1 w\")\n",
    "        return (dxz, dconv1d_weight, dconv1d_bias, dx_proj_weight, ddelta_proj_weight,\n",
    "                dout_proj_weight, dout_proj_bias,\n",
    "                dA, dB, dC, dD,\n",
    "                ddelta_bias if delta_bias is not None else None,\n",
    "                dB_proj_bias, dC_proj_bias, None)\n",
    "\n",
    "\n",
    "def mamba_inner_fn(\n",
    "    xz, conv1d_weight, conv1d_bias, x_proj_weight, delta_proj_weight,\n",
    "    out_proj_weight, out_proj_bias,\n",
    "    A, B=None, C=None, D=None, delta_bias=None, B_proj_bias=None,\n",
    "    C_proj_bias=None, delta_softplus=True\n",
    "):\n",
    "    return MambaInnerFn.apply(xz, conv1d_weight, conv1d_bias, x_proj_weight, delta_proj_weight,\n",
    "                              out_proj_weight, out_proj_bias,\n",
    "                              A, B, C, D, delta_bias, B_proj_bias, C_proj_bias, delta_softplus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b654ff-157c-4cb8-bb07-ea1552693a17",
   "metadata": {},
   "source": [
    "### Mamba\n",
    "\n",
    "- Mamba in Transformers (https://github.com/huggingface/transformers/blob/main/src/transformers/models/mamba/modeling_mamba.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ddce85e-a9f7-41ac-b362-a5f9f91f5e15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Mamba(nn.Module):\n",
    "    \"\"\" Compute \\Delta, A, B, C, and D the state space parameters and compute the `contextualized_states`.\n",
    "        A, D are input independent, and \\Delta, B, C are input-dependent. This is why Mamba is called\n",
    "        selective state spaces\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, layer_idx: int):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.ssm_state_size = config.state_size\n",
    "        self.conv_kernel_size = config.conv_kernel\n",
    "        self.intermediate_size = config.intermediate_size\n",
    "        self.time_step_rank = int(config.time_step_rank)\n",
    "        self.layer_idx = layer_idx\n",
    "        self.use_conv_bias = config.use_conv_bias\n",
    "        self.conv1d = nn.Conv1d(\n",
    "            in_channels=self.intermediate_size,\n",
    "            out_channels=self.intermediate_size,\n",
    "            bias=config.use_conv_bias,\n",
    "            kernel_size=config.conv_kernel,\n",
    "            groups=self.intermediate_size,\n",
    "            padding=config.conv_kernel - 1,\n",
    "        )\n",
    "        \n",
    "        self.activation = confgi.hidden_act\n",
    "        self.act = ACT2FN[config.hidden_act]\n",
    "        \n",
    "        self.use_mambapy = config.use_mambapy\n",
    "        \n",
    "        # projection of the input hidden states\n",
    "        self.in_proj = nn.Linear(self.hidden_size, self.intermediate_size * 2, bias=config.use_bias)\n",
    "        # selective projection used to make dt, B and C input dependant\n",
    "        self.x_proj = nn.Linear(self.intermediate_size, self.time_step_rank + self.ssm_state_size * 2, bias=False)\n",
    "        # time step projection (discretization)\n",
    "        self.dt_proj = nn.Linear(self.time_step_rank, self.intermediate_size, bias=True)\n",
    "        \n",
    "        # S4D real initialization. There are not discretized!\n",
    "        # The core is to load them, compute the discrete states, then write the update state. Keeps the memory bounded\n",
    "        A = torch.arange(1, self.ssm_state_size + 1, dtype=torch.float32)[None, :]\n",
    "        A = A.expand(self.intermediate_size, -1).contiguous()\n",
    "        \n",
    "        self.A_log = nn.Parameter(torch.log(A))\n",
    "        self.D = nn.Parameter(torch.ones(self.intermediate_size))\n",
    "        self.out_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config.use_bias)\n",
    "        self.use_bias = config.use_bias\n",
    "\n",
    "    def cuda_kernels_forward(self, hidden_states: torch.Tensor):\n",
    "        # 1. Gated MLP's linear projection\n",
    "        projected_states = self.in_proj(hidden_states).transpose(1, 2)\n",
    "        \n",
    "        contextualized_states = mamba_inner_fn(\n",
    "            projected_states,\n",
    "            self.conv1d.weight,\n",
    "            self.conv1d.bias if self.use_conv_bias else None,\n",
    "            self.x_proj.weight,\n",
    "            self.dt_proj.weight,\n",
    "            self.out_proj.weight,\n",
    "            self.out_proj.bias.float() if self.use_bias else None,\n",
    "            -torch.exp(self.a_log.float()),\n",
    "            None, # input-dependent B\n",
    "            None, # input-dependent C\n",
    "            self.D.float(),\n",
    "            delta_bias=self.dt_proj.bias.float(),\n",
    "            delta_softplus=True,\n",
    "        )\n",
    "        \n",
    "        return contextualized_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8e6b57-a654-44dd-bb01-fac815c359c3",
   "metadata": {},
   "source": [
    "## Mamba2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "778868e4-0616-4d9e-bbed-74966412c694",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MambaSplitConv1dScanCombinedFn(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    @custom_fwd\n",
    "    def forward(ctx, zxbcdt, conv1d_weight, conv1d_bias, dt_bias, A, D, chunk_size, initial_states=None, seq_idx=None, dt_limit=(0.0, float(\"inf\")), return_final_states=False, activation=\"silu\",\n",
    "                rmsnorm_weight=None, rmsnorm_eps=1e-6, outproj_weight=None, outproj_bias=None, headdim=None,\n",
    "                ngroups=1, norm_before_gate=True):\n",
    "        assert activation in [None, \"silu\", \"swish\"]\n",
    "        if D.dim() == 1:\n",
    "            assert headdim is not None\n",
    "            nheads, = D.shape\n",
    "        else:\n",
    "            nheads, headdim = D.shape\n",
    "        batch, seqlen, _ = zxbcdt.shape\n",
    "        dim = nheads * headdim\n",
    "        assert nheads % ngroups == 0\n",
    "        dstate = (conv1d_weight.shape[0] - dim) // ngroups // 2\n",
    "        d_nonssm = (zxbcdt.shape[-1] - 2 * dim - 2 * ngroups * dstate - nheads) // 2\n",
    "        assert d_nonssm >= 0\n",
    "        assert zxbcdt.shape == (batch, seqlen, 2 * d_nonssm + 2 * dim + 2 * ngroups * dstate + nheads)\n",
    "        assert dt_bias.shape == (nheads,)\n",
    "        assert A.shape == (nheads,)\n",
    "        zx0, z, xBC, dt = torch.split(zxbcdt, [2 * d_nonssm, dim, dim + ngroups * dstate * 2, nheads], dim=-1)\n",
    "        seq_idx = seq_idx.contiguous() if seq_idx is not None else None\n",
    "        xBC_conv = rearrange(\n",
    "            causal_conv1d_cuda.causal_conv1d_fwd(rearrange(xBC, \"b s d -> b d s\"),\n",
    "                                                 conv1d_weight, conv1d_bias, seq_idx, None, None, activation in [\"silu\", \"swish\"]),\n",
    "            \"b d s -> b s d\"\n",
    "        )\n",
    "        x, B, C = torch.split(xBC_conv, [dim, ngroups * dstate, ngroups * dstate], dim=-1)\n",
    "        x = rearrange(x, \"b l (h p) -> b l h p\", h=nheads)\n",
    "        B = rearrange(B, \"b l (g n) -> b l g n\", g=ngroups)\n",
    "        C = rearrange(C, \"b l (g n) -> b l g n\", g=ngroups)\n",
    "        z = rearrange(z, \"b l (h p) -> b l h p\", h=nheads) if z is not None else None\n",
    "        if rmsnorm_weight is None:\n",
    "            out, out_x, dt_out, dA_cumsum, states, final_states = _mamba_chunk_scan_combined_fwd(x, dt, A, B, C, chunk_size=chunk_size, D=D, z=z, dt_bias=dt_bias, initial_states=initial_states, seq_idx=seq_idx, dt_softplus=True, dt_limit=dt_limit)\n",
    "            out = rearrange(out, \"b s h p -> b s (h p)\")\n",
    "            rstd = None\n",
    "            if d_nonssm > 0:\n",
    "                out = torch.cat([_swiglu_fwd(zx0), out], dim=-1)\n",
    "        else:\n",
    "            out_x, _, dt_out, dA_cumsum, states, final_states = _mamba_chunk_scan_combined_fwd(x, dt, A, B, C, chunk_size=chunk_size, D=D, z=None, dt_bias=dt_bias, initial_states=initial_states, seq_idx=seq_idx, dt_softplus=True, dt_limit=dt_limit)\n",
    "            # reshape input data into 2D tensor\n",
    "            x_rms = rearrange(out_x, \"b s h p -> (b s) (h p)\")\n",
    "            z_rms = rearrange(z, \"b s h p -> (b s) (h p)\")\n",
    "            rmsnorm_weight = rmsnorm_weight.contiguous()\n",
    "            if d_nonssm == 0:\n",
    "                out = None\n",
    "            else:\n",
    "                out01 = torch.empty((batch, seqlen, d_nonssm + dim), dtype=x_rms.dtype, device=x_rms.device)\n",
    "                out = rearrange(out01[..., d_nonssm:], \"b s d -> (b s) d\")\n",
    "                _swiglu_fwd(zx0, out=out01[..., :d_nonssm])\n",
    "            out, _, rstd = _layer_norm_fwd(x_rms, rmsnorm_weight, None, rmsnorm_eps, z_rms, out=out,\n",
    "                                           group_size=dim // ngroups,\n",
    "                                           norm_before_gate=norm_before_gate, is_rms_norm=True)\n",
    "            if d_nonssm == 0:\n",
    "                out = rearrange(out, \"(b s) d -> b s d\", b=batch)\n",
    "            else:\n",
    "                out = out01\n",
    "        ctx.outproj_weight_dtype = outproj_weight.dtype if outproj_weight is not None else None\n",
    "        if outproj_weight is not None:\n",
    "            if torch.is_autocast_enabled():\n",
    "                dtype = torch.get_autocast_gpu_dtype()\n",
    "                out, outproj_weight = out.to(dtype), outproj_weight.to(dtype)\n",
    "                outproj_bias = outproj_bias.to(dtype) if outproj_bias is not None else None\n",
    "            out = F.linear(out, outproj_weight, outproj_bias)\n",
    "        else:\n",
    "            assert outproj_bias is None\n",
    "        ctx.save_for_backward(zxbcdt, conv1d_weight, conv1d_bias,\n",
    "                              out_x, A, D, dt_bias, initial_states, seq_idx, rmsnorm_weight, rstd, outproj_weight, outproj_bias)\n",
    "        ctx.dt_limit = dt_limit\n",
    "        ctx.return_final_states = return_final_states\n",
    "        ctx.activation = activation\n",
    "        ctx.rmsnorm_eps = rmsnorm_eps\n",
    "        ctx.norm_before_gate = norm_before_gate\n",
    "        ctx.chunk_size = chunk_size\n",
    "        ctx.headdim = headdim\n",
    "        ctx.ngroups = ngroups\n",
    "        return out if not return_final_states else (out, final_states)\n",
    "\n",
    "    @staticmethod\n",
    "    @custom_bwd\n",
    "    def backward(ctx, dout, *args):\n",
    "        zxbcdt, conv1d_weight, conv1d_bias, out, A, D, dt_bias, initial_states, seq_idx, rmsnorm_weight, rstd, outproj_weight, outproj_bias = ctx.saved_tensors\n",
    "        dfinal_states = args[0] if ctx.return_final_states else None\n",
    "        headdim = ctx.headdim\n",
    "        nheads = D.shape[0]\n",
    "        dim = nheads * headdim\n",
    "        assert nheads % ctx.ngroups == 0\n",
    "        dstate = (conv1d_weight.shape[0] - dim) // ctx.ngroups // 2\n",
    "        d_nonssm = (zxbcdt.shape[-1] - 2 * dim - 2 * ctx.ngroups * dstate - nheads) // 2\n",
    "        assert d_nonssm >= 0\n",
    "        recompute_output = outproj_weight is not None\n",
    "        if recompute_output:\n",
    "            out_recompute = torch.empty(*out.shape[:2], d_nonssm + dim, device=out.device, dtype=out.dtype)\n",
    "            out0_recompute, out1_recompute = out_recompute.split([d_nonssm, dim], dim=-1)\n",
    "        zx0, z, xBC, dt = torch.split(zxbcdt, [2 * d_nonssm, dim, dim + 2 * ctx.ngroups * dstate, nheads], dim=-1)\n",
    "        # Recompute x, B, C\n",
    "        xBC_conv = rearrange(\n",
    "            causal_conv1d_cuda.causal_conv1d_fwd(rearrange(xBC, \"b s d -> b d s\"),\n",
    "                                                 conv1d_weight, conv1d_bias, seq_idx, None, None, ctx.activation in [\"silu\", \"swish\"]),\n",
    "            \"b d s -> b s d\"\n",
    "        )\n",
    "        x, B, C = torch.split(xBC_conv, [dim, ctx.ngroups * dstate, ctx.ngroups * dstate], dim=-1)\n",
    "        x = rearrange(x, \"b l (h p) -> b l h p\", h=nheads)\n",
    "        B = rearrange(B, \"b l (g n) -> b l g n\", g=ctx.ngroups)\n",
    "        C = rearrange(C, \"b l (g n) -> b l g n\", g=ctx.ngroups)\n",
    "        dzxbcdt = torch.empty_like(zxbcdt)\n",
    "        dzx0, dz, dxBC_given, ddt_given = torch.split(dzxbcdt, [2 * d_nonssm, dim, dim + 2 * ctx.ngroups * dstate, nheads], dim=-1)\n",
    "        dxBC = torch.empty_like(xBC)\n",
    "        dx, dB, dC = torch.split(dxBC, [dim, ctx.ngroups * dstate, ctx.ngroups * dstate], dim=-1)\n",
    "        z = rearrange(z, \"b l (h p) -> b l h p\", h=nheads)\n",
    "        dx = rearrange(dx, \"b l (h p) -> b l h p\", h=nheads)\n",
    "        dB = rearrange(dB, \"b l (g n) -> b l g n\", g=ctx.ngroups)\n",
    "        dC = rearrange(dC, \"b l (g n) -> b l g n\", g=ctx.ngroups)\n",
    "        if outproj_weight is not None:\n",
    "            dout_og = dout\n",
    "            dout = F.linear(dout, outproj_weight.t())\n",
    "        if d_nonssm > 0:\n",
    "            dout0, dout = dout.split([d_nonssm, dim], dim=-1)\n",
    "            _swiglu_bwd(zx0, dout0, dxy=dzx0, recompute_output=True, out=out0_recompute)\n",
    "        dout = rearrange(dout, \"b s (h p) -> b s h p\", p=headdim)\n",
    "        if rmsnorm_weight is None:\n",
    "            dz = rearrange(dz, \"b l (h p) -> b l h p\", h=nheads)\n",
    "            dx, ddt, dA, dB, dC, dD, dz, ddt_bias, dinitial_states, *rest = _mamba_chunk_scan_combined_bwd(\n",
    "                dout, x, dt, A, B, C, out, ctx.chunk_size, D=D, z=z, dt_bias=dt_bias, initial_states=initial_states, dfinal_states=dfinal_states, seq_idx=seq_idx, dt_softplus=True, dt_limit=ctx.dt_limit, dx=dx, ddt=ddt_given, dB=dB, dC=dC, dz=dz, recompute_output=recompute_output\n",
    "            )\n",
    "            out_for_linear = rearrange(rest[0], \"b s h p -> b s (h p)\") if recompute_output else None\n",
    "            drmsnorm_weight = None\n",
    "        else:\n",
    "            batch = dout.shape[0]\n",
    "            dy_rms = rearrange(dout, \"b s h p -> (b s) (h p)\")\n",
    "            dz = rearrange(dz, \"b l d -> (b l) d\")\n",
    "            x_rms = rearrange(out, \"b s h p -> (b s) (h p)\")\n",
    "            z_rms = rearrange(z, \"b s h p -> (b s) (h p)\")\n",
    "            out1_recompute = rearrange(out1_recompute, \"b s d -> (b s) d\") if recompute_output else None\n",
    "            dout, drmsnorm_weight, _, dz, *rest = _layer_norm_bwd(dy_rms, x_rms, rmsnorm_weight, None, ctx.rmsnorm_eps, None, rstd, z_rms, group_size=dim//ctx.ngroups, norm_before_gate=ctx.norm_before_gate, is_rms_norm=True, recompute_output=recompute_output, dz=dz, out=out1_recompute if recompute_output else None)\n",
    "            out_for_linear = out_recompute if recompute_output else None\n",
    "            dout = rearrange(dout, \"(b s) (h p) -> b s h p\", b=batch, p=headdim)\n",
    "            dx, ddt, dA, dB, dC, dD, _, ddt_bias, dinitial_states = _mamba_chunk_scan_combined_bwd(\n",
    "                dout, x, dt, A, B, C, out, ctx.chunk_size, D=D, z=None, dt_bias=dt_bias, initial_states=initial_states, dfinal_states=dfinal_states, seq_idx=seq_idx, dt_softplus=True, dt_limit=ctx.dt_limit, dx=dx, ddt=ddt_given, dB=dB, dC=dC\n",
    "            )\n",
    "\n",
    "        if outproj_weight is not None:\n",
    "            doutproj_weight = torch.einsum(\"bso,bsd->od\", dout_og, out_for_linear)\n",
    "            doutproj_bias = dout_og.sum(dim=(0, 1)) if outproj_bias is not None else None\n",
    "        else:\n",
    "            doutproj_weight, doutproj_bias = None, None\n",
    "        dxBC_given = rearrange(dxBC_given, \"b s d -> b d s\")\n",
    "        dxBC_given, dweight, dbias, *_ = causal_conv1d_cuda.causal_conv1d_bwd(\n",
    "            rearrange(xBC, \"b s d -> b d s\"), conv1d_weight, conv1d_bias,\n",
    "            rearrange(dxBC, \"b s d -> b d s\"), seq_idx, None, None, dxBC_given, False, ctx.activation in [\"silu\", \"swish\"]\n",
    "        )\n",
    "        dxBC_given = rearrange(dxBC_given, \"b d s -> b s d\")\n",
    "        return dzxbcdt, dweight, dbias, ddt_bias, dA, dD, None, dinitial_states, None, None, None, None, drmsnorm_weight, None, doutproj_weight, doutproj_bias, None, None, None\n",
    "\n",
    "\n",
    "def mamba_split_conv1d_scan_combined(zxbcdt, conv1d_weight, conv1d_bias, dt_bias, A, D, chunk_size, initial_states=None, seq_idx=None, dt_limit=(0.0, float(\"inf\")), return_final_states=False, activation=\"silu\", rmsnorm_weight=None, rmsnorm_eps=1e-6, outproj_weight=None, outproj_bias=None, headdim=None, ngroups=1, norm_before_gate=True):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "        zxbcdt: (batch, seqlen, 2 * dim + 2 * ngroups * dstate + nheads) where dim == nheads * headdim\n",
    "        conv1d_weight: (dim + 2 * ngroups * dstate, width)\n",
    "        conv1d_bias: (dim + 2 * ngroups * dstate,)\n",
    "        dt_bias: (nheads,)\n",
    "        A: (nheads)\n",
    "        D: (nheads, headdim) or (nheads,)\n",
    "        initial_states: (batch, nheads, headdim, dstate)\n",
    "        seq_idx: (batch, seqlen), int32\n",
    "        rmsnorm_weight: (dim,)\n",
    "        outproj_weight: (out_dim, dim)\n",
    "        outproj_bias: (out_dim,)\n",
    "        headdim: if D is 1D, headdim must be passed in\n",
    "        norm_before_gate: if True, we do RMSNorm(x) * F.silu(z). If False, we do RMSNorm(x * F.silu(z))\n",
    "    Return:\n",
    "        out: (batch, seqlen, dim)\n",
    "    \"\"\"\n",
    "    return MambaSplitConv1dScanCombinedFn.apply(zxbcdt, conv1d_weight, conv1d_bias, dt_bias, A, D, chunk_size, initial_states, seq_idx, dt_limit, return_final_states, activation, rmsnorm_weight, rmsnorm_eps, outproj_weight, outproj_bias, headdim, ngroups, norm_before_gate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fe621ae8-125d-4519-9b4c-f9b7e10f71a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Mamba2(nn.Module):\n",
    "    \"\"\" Compute \\Delta, A, B, C, and D the state space parameters and compute the `contextualized_states`.\n",
    "        A, D are input independent, and \\Delta, B, C are input-dependent.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, layer_idx: int):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.ssm_state_size = config.state_size\n",
    "        self.conv_kernel_size = config.conv_kernel\n",
    "        self.intermediate_size = config.intermediate_size\n",
    "        self.time_step_rank = int(config.time_step_rank)\n",
    "        self.layer_idx = layer_idx\n",
    "        self.use_conv_bias = config.use_conv_bias\n",
    "        \n",
    "        self.activation = config.hidden_act\n",
    "        self.act = ACT2FN[config.hidden_act]\n",
    "        \n",
    "        self.layer_norm_epsilon = config.layer_norm_epsilon\n",
    "        self.rms_norm = config.rms_norm\n",
    "        \n",
    "        self.n_groups = config.n_groups\n",
    "        self.head_dim = config.head_dim\n",
    "        self.chunk_size = config.chunk_size\n",
    "        \n",
    "        self.time_step_limit = config.time_step_limit\n",
    "        self.time_step_min = config.time_step_min\n",
    "        self.time_step_max = config.time_step_max\n",
    "        \n",
    "        self.conv_dim = self.intermediate_size + 2 * self.n_groups * self.ssm_state_size\n",
    "        self.conv1d = nn.Conv1d(\n",
    "            in_channels=self.conv_dim, # intermediate_size in Mamba1\n",
    "            out_channels=self.conv_dim, # intermediate_size in Mamba1\n",
    "            bias=config.use_conv_bias,\n",
    "            kernel_size=config.conv_kernel,\n",
    "            groups=self.conv_dim, # intermediate_size in Mamba1\n",
    "            padding=config.conv_kernel - 1,\n",
    "        )\n",
    "        \n",
    "        # projection of the input hidden states\n",
    "        projection_size = self.intermediate_size + self.conv_dim + self.num_heads\n",
    "        self.in_proj = nn.Linear(self.hidden_size, projection_size, bias=config.use_bias) # selective projection\n",
    "        \n",
    "        # time step projection (discretization) - instantiate once and copy inv_dt in init_weights of PretrainedModel\n",
    "        self.dt_bias = nn.Parameter(torch.ones(self.num_heads)) # no x_proj and dt_proj, compared to Mamba1\n",
    "        \n",
    "        # S4D real initialization. These are not discretized!\n",
    "        # The core is to load them, compute the discrete states, then write the updated state. Keeps the memory bounded\n",
    "        A = torch.arange(1, self.num_heads + 1)\n",
    "        self.A_log = nn.Parameter(torch.log(A))\n",
    "        self.A_log._no_weight_decay = True\n",
    "        self.norm = MambaRMSNormGated(self.intermediate_size, eps=self.layer_norm_epsilon) # added, compared to Mamba1\n",
    "        self.D = nn.Parameter(torch.ones(self.num_heads))\n",
    "        self.D._no_weight_decay = True\n",
    "        \n",
    "        self.out_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config.use_bias)\n",
    "        self.use_bias = config.use_bias\n",
    "        \n",
    "    def cuda_kernels_forward(self, hidden_states: torch.Tensor):\n",
    "        batch_size, seq_len, _ = hidden_states.shape\n",
    "        groups_time_state_size = self.n_groups * self.ssm_state_size\n",
    "        d_to_remove = 2 * self.intermediate_size + 2 * self.n_groups * self.ssm_state_size + self.num_heads\n",
    "        \n",
    "        projected_states = self.in_proj(hidden_states)\n",
    "        A = -torch.exp(self.A_log.float()) # (num_heads) or (intermediate_size, state_size)\n",
    "        dt_limit_kwargs = {} if self.time_step_limit == (0., float(\"inf\")) else {\"dt_limit\": self.time_step_limit}\n",
    "        out, ssm_state = mamba_split_conv1d_scan_combined(\n",
    "            projected_states,\n",
    "            self.conv1d.weight.squeeze(1),\n",
    "            self.conv1d.bias,\n",
    "            self.dt_bias,\n",
    "            A,\n",
    "            D=self.D,\n",
    "            chunk_size=self.chunk_size,\n",
    "            seq_idx=None, # was seq_idx\n",
    "            activation=self.activation,\n",
    "            rmsnorm_weight=self.norm.weight,\n",
    "            rmsnorm_eps=self.norm.variance_epsilon,\n",
    "            outproj_weight=self.out_proj.weight,\n",
    "            outproj_bias=self.out_proj.bias,\n",
    "            headdim=self.head_dim,\n",
    "            ngroups=self.n_groups,\n",
    "            norm_before_gate=False,\n",
    "            return_final_states=True,\n",
    "            **dt_limit_kwargs,\n",
    "        )\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c510762-59c0-4c0e-9020-aa7f0de4bd5a",
   "metadata": {},
   "source": [
    "# Mamba vs Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73917bd2-791e-434d-9976-70ee3b8eb105",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for Mamba1\n",
    "from mamba_ssm.ops.selective_scan_interface import mamba_inner_fn, selective_scan_fn\n",
    "from mamba_ssm.ops.triton.selective_state_update import selective_state_update\n",
    "\n",
    "# for Mamba2\n",
    "from mamba_ssm.ops.triton.selective_state_update import selective_state_update\n",
    "from mamba_ssm.ops.triton.ssd_combined import mamba_chunk_scan_combined, mamba_split_conv1d_scan_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c7031fb-0b70-4d97-8f6e-ea74a9169f4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accelerate   models--stabilityai--stable-diffusion-2-base  token\n",
      "hub\t     models--state-spaces--mamba-130m-hf\n",
      "huggingface  stable-diffusion-2-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!ls /root/dataset_sj/hf_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fba0fb8-ee68-4b75-acf0-734d06b8dcde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cache_dir = \"/root/dataset_sj/hf_cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a1d96c5-3835-4db8-bcb0-221a41cdb83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Hey, how are you doing?\\n\\nI'm so glad you're here.\"]\n"
     ]
    }
   ],
   "source": [
    "from transformers import MambaConfig, MambaForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"state-spaces/mamba-130m-hf\", cache_dir=cache_dir)\n",
    "model = MambaForCausalLM.from_pretrained(\"state-spaces/mamba-130m-hf\", cache_dir=cache_dir)\n",
    "input_ids = tokenizer(\"Hey, how are you doing?\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "out = model.generate(input_ids, max_new_tokens=10)\n",
    "print(tokenizer.batch_decode(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9988618-aabc-4db4-8d2c-dab48018bf5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers.models.mamba import MambaConfig, MambaModel\n",
    "from transformers.models.mamba2 import Mamba2Config, Mamba2Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d602f73a-3dd0-4b27-a8ba-fc7c95cf4ba5",
   "metadata": {},
   "source": [
    "Configuration of Mamba1\n",
    "- vocab_size (int, optional, defaults to 50280) — Vocabulary size of the MAMBA model. Defines the number of different tokens that can be represented by the inputs_ids passed when calling MambaModel.\n",
    "- hidden_size (int, optional, defaults to 768) — Dimensionality of the embeddings and hidden states.\n",
    "- state_size (int, optional, defaults to 16) — shape of the state space latents.\n",
    "- num_hidden_layers (int, optional, defaults to 32) — Number of hidden layers in the model.\n",
    "- layer_norm_epsilon (float, optional, defaults to 1e-05) — The epsilon to use in the layer normalization layers.\n",
    "- expand (int, optional, defaults to 2) — Expanding factor used to determine the intermediate size.\n",
    "- conv_kernel (int, optional, defaults to 4) — Size of the convolution kernel.\n",
    "- use_bias (bool, optional, defaults to False) — Whether or not to use bias in [“in_proj”, “out_proj”] of the mixer block\n",
    "- use_conv_bias (bool, optional, defaults to True) — Whether or not to use bias in the convolution layer of the mixer block.\n",
    "- hidden_act (str, optional, defaults to \"silu\") — The non-linear activation function (function or string) in the decoder.\n",
    "- initializer_range (float, optional, defaults to 0.1) — The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n",
    "- residual_in_fp32 (bool, optional, defaults to True) — Whether or not residuals should be in float32. If set to False residuals will keep the same dtype as the rest of the model\n",
    "- time_step_rank (Union[int,str], optional, defaults to \"auto\") — Rank of the discretization projection matrix. \"auto\" means that it will default to math.ceil(self.hidden_size / 16)\n",
    "- time_step_scale (float, optional, defaults to 1.0) — Scale used used to scale dt_proj.bias.\n",
    "- time_step_min (float, optional, defaults to 0.001) — Minimum time_step used to bound dt_proj.bias.\n",
    "- time_step_max (float, optional, defaults to 0.1) — Maximum time_step used to bound dt_proj.bias.\n",
    "- time_step_init_scheme (float, optional, defaults to \"random\") — Init scheme used for dt_proj.weight. Should be one of [\"random\",\"uniform\"]\n",
    "- time_step_floor (float, optional, defaults to 0.0001) — Minimum clamping value of the dt_proj.bias layer initialization.\n",
    "- rescale_prenorm_residual (bool, optional, defaults to False) — Whether or not to rescale out_proj weights when initializing.\n",
    "- use_mambapy (bool, optional, defaults to False) — Determines the fallback strategy during training if the CUDA-based official implementation of Mamba is not avaiable. If True, the mamba.py implementation is used. If False, the naive and slower implementation is used. Consider switching to the naive version if memory is limited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8961b0ca-f2fb-42dc-a47b-697c0314b508",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mamba1 = MambaModel(MambaConfig())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f232eed5-c2b1-4838-9ad5-c2b70475082c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159308544\n"
     ]
    }
   ],
   "source": [
    "model_size = 0\n",
    "for param in mamba1.parameters():\n",
    "    model_size += param.data.nelement()\n",
    "print(model_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c206fa-e539-4f2a-9c04-87002fef7b56",
   "metadata": {},
   "source": [
    "Configuration of Mamba2\n",
    "- num_heads (int, optional, defaults to 128) — Number of heads for the evolution matrices of mamba 2.\n",
    "- head_dim (int, optional, defaults to 64) — Dimension of each head.\n",
    "- vocab_size (int, optional, defaults to 32768) — Vocabulary size of the MAMBA2 model. Defines the number of different tokens that can be represented by the inputs_ids passed when calling Mamba2Model.\n",
    "- hidden_size (int, optional, defaults to 4096) — Dimensionality of the embeddings and hidden states.\n",
    "- state_size (int, optional, defaults to 128) — shape of the state space latents.\n",
    "- num_hidden_layers (int, optional, defaults to 64) — Number of hidden layers in the model.\n",
    "- layer_norm_epsilon (float, optional, defaults to 1e-05) — The epsilon to use in the layer normalization layers.\n",
    "- pad_token_id (int, optional, defaults to 1) — Padding token id.\n",
    "- bos_token_id (int, optional, defaults to 0) — The id of the beginning of sentence token in the vocabulary.\n",
    "- eos_token_id (int, optional, defaults to 2) — The id of the end of sentence token in the vocabulary.\n",
    "- expand (int, optional, defaults to 2) — Expanding factor used to determine the intermediate size.\n",
    "- conv_kernel (int, optional, defaults to 4) — Size of the convolution kernel.\n",
    "- n_groups (int, optional, defaults to 8) — Number of groups for the evolution matrices of mamba 2.\n",
    "- use_bias (bool, optional, defaults to False) — Whether or not to use bias in [“in_proj”, “out_proj”] of the mixer block\n",
    "- use_conv_bias (bool, optional, defaults to True) — Whether or not to use bias in the convolution layer of the mixer block.\n",
    "- hidden_act (str, optional, defaults to \"silu\") — The non-linear activation function (function or string) in the decoder.\n",
    "- initializer_range (float, optional, defaults to 0.1) — The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n",
    "- residual_in_fp32 (bool, optional, defaults to True) — Whether or not residuals should be in float32. If set to False residuals will keep the same dtype as the rest of the model\n",
    "- time_step_rank (Union[int,str], optional, defaults to \"auto\") — Rank of the discretization projection matrix. \"auto\" means that it will default to math.ceil(self.hidden_size / 16)\n",
    "- time_step_min (float, optional, defaults to 0.001) — Minimum time_step used to bound dt_proj.bias.\n",
    "- time_step_max (float, optional, defaults to 0.1) — Maximum time_step used to bound dt_proj.bias.\n",
    "- time_step_floor (float, optional, defaults to 0.0001) — Minimum clamping value of the dt_proj.bias layer initialization.\n",
    "- time_step_limit (tuple, optional, defaults to (0.0, inf)) — Accepted range of time step values.\n",
    "- rescale_prenorm_residual (bool, optional, defaults to False) — Whether or not to rescale out_proj weights when initializing.\n",
    "- use_cache (bool, optional, defaults to True) — Whether or not the cache should be used.\n",
    "- rms_norm (bool, optional, defaults to True) — Whether to use RMS norm or not.\n",
    "- chunk_size (int, optional, defaults to 256) — Size of the chunks that will comprise the sequence.\n",
    "- tie_word_embeddings (bool, optional, defaults to False) — Whether to tie word embeddings or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "44fb4ee3-7bea-4755-a7fb-0e52872d3d5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mamba2 = Mamba2Model(Mamba2Config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "574a669f-0858-4677-97a2-9c3273b5db61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7151185920\n"
     ]
    }
   ],
   "source": [
    "model_size = 0\n",
    "for param in mamba2.parameters():\n",
    "    model_size += param.data.nelement()\n",
    "print(model_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123e2a9e-3550-4297-a010-de0488aef75e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jjuke_39_231",
   "language": "python",
   "name": "jjuke_39_231"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
