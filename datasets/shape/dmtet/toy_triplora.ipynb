{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dde209cb-b429-4a2d-8b7f-4db8b5f70562",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/dev/triplora'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"/root/dev/triplora/\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "821051b7-64fd-46c4-aa37-5975c33082bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.set_device(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afcf8f2f-5aee-46c6-9ae3-93b9ef703100",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37maa4173e0a5f2              \u001b[m  Mon May 20 22:32:13 2024  \u001b[1m\u001b[30m535.129.03\u001b[m\n",
      "\u001b[36m[0]\u001b[m \u001b[34mNVIDIA GeForce RTX 4090\u001b[m |\u001b[31m 27°C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m    5\u001b[m / \u001b[33m24564\u001b[m MB |\n",
      "\u001b[36m[1]\u001b[m \u001b[34mNVIDIA GeForce RTX 4090\u001b[m |\u001b[31m 32°C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m    5\u001b[m / \u001b[33m24564\u001b[m MB |\n",
      "\u001b[36m[2]\u001b[m \u001b[34mNVIDIA GeForce RTX 4090\u001b[m |\u001b[31m 32°C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m    5\u001b[m / \u001b[33m24564\u001b[m MB |\n",
      "\u001b[36m[3]\u001b[m \u001b[34mNVIDIA GeForce RTX 4090\u001b[m |\u001b[31m 31°C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m    5\u001b[m / \u001b[33m24564\u001b[m MB |\n",
      "\u001b[36m[4]\u001b[m \u001b[34mNVIDIA GeForce RTX 4090\u001b[m |\u001b[31m 32°C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m    5\u001b[m / \u001b[33m24564\u001b[m MB |\n",
      "\u001b[36m[5]\u001b[m \u001b[34mNVIDIA GeForce RTX 4090\u001b[m |\u001b[31m 32°C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m    5\u001b[m / \u001b[33m24564\u001b[m MB |\n"
     ]
    }
   ],
   "source": [
    "!gpustat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0469ea7f-fcf8-4a24-824e-57fab7aa74eb",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0db9143c-9c94-4c56-93ad-400ddd9e9567",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0f401b90-25e5-4d8e-bdb9-043f82e3cdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_res = 128\n",
    "mesh_scale = 1.1\n",
    "fitted_tet_path = \"/root/dataset_sj/DMTet/res_128/chair/tets_pre/dmt_dict_00000.pt\"\n",
    "init_tet_path = \"/root/dev/DMTet_Models/MeshDiffusion/nvdiffrec/data_/tets/128_tets_cropped.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dd7f71d9-11ad-4cf4-82b5-bc90cad393eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tet_init = np.load(init_tet_path)\n",
    "init_verts = torch.tensor(tet_init[\"vertices\"], dtype=torch.float32, device=device) * mesh_scale\n",
    "tet_indices = torch.tensor(tet_init[\"indices\"], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1c1b610d-3a89-4ae3-9b2c-d3383c678031",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_deformed(verts, deform, grid_res=128, deform_scale=0.45, no_grad=False):\n",
    "    \"\"\" deform_scale is 0.45 for resolution 128 and 2.0 for resolution 64 \"\"\"\n",
    "    deform = deform.detach() if no_grad else deform\n",
    "    return verts + 2 / (grid_res * 2) * deform * deform_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e450ee92-c0a9-436c-a39f-28ecc019d564",
   "metadata": {},
   "source": [
    "## DatasetMesh from MeshDiffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "04d9231e-4788-47ef-9226-a43cb3fe1547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.mtl  model.obj  models\n"
     ]
    }
   ],
   "source": [
    "!ls /root/dev/DMTet_Models/MeshDiffusion/data/03001627/1006be65e7bc937e9141f9b58470d646"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dfa16760-8407-4bff-9021-10f8e5a3d460",
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_path = \"/root/dev/DMTet_Models/MeshDiffusion/data/03001627/1006be65e7bc937e9141f9b58470d646/model.obj\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7e671b94-e1c6-480f-8018-e9e5ff5d3f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['v_pos', 'v_nrm', 'v_tex', 'v_tng', 't_pos_idx', 't_nrm_idx', 't_tex_idx', 't_tng_idx', 'material', 'f_nrm'])\n"
     ]
    }
   ],
   "source": [
    "from utils.render.mesh import load_mesh\n",
    "from utils.render import texture\n",
    "\n",
    "mtl_default = {\n",
    "    'name' : '_default_mat',\n",
    "    'bsdf': 'diffuse',\n",
    "    'uniform': True,\n",
    "    'kd'   : texture.Texture2D(torch.tensor([0.75, 0.3, 0.6], dtype=torch.float32, device='cuda'), trainable=False),\n",
    "    'ks'   : texture.Texture2D(torch.tensor([0.0, 0.0, 0.0], dtype=torch.float32, device='cuda'), trainable=False)\n",
    "}\n",
    "\n",
    "ref_mesh = load_mesh(mesh_path, mtl_override=None, mtl_default=mtl_default, use_default=True, no_additional=True)\n",
    "print(vars(ref_mesh).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c784c749-6dba-4fe5-ba00-1f5308373a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kaolin\n",
    "\n",
    "sampled_pts, face_indices = kaolin.ops.mesh.sample_points(ref_mesh.v_pos.unsqueeze(0), ref_mesh.t_pos_idx, 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4f3b7934-6cf9-4189-8471-79f277495ad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50000, 3])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_pts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "101e3f15-d54d-45de-8907-13c8ac7f6c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import trimesh\n",
    "\n",
    "# scene = trimesh.Scene()\n",
    "# sampled_pc = trimesh.PointCloud(sampled_pts[0].detach().cpu().numpy())\n",
    "# sampled_pc.colors = [255, 0, 0, 0]\n",
    "# scene.add_geometry(sampled_pc)\n",
    "# scene.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6f982d99-de20-48f9-a8ae-6ace300a584f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max:  tensor(0.3420, device='cuda:0')\n",
      "min:  tensor(-0.3423, device='cuda:0')\n",
      "mean:  tensor(0.0139, device='cuda:0')\n",
      "mean:  tensor(0.1493, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(\"max: \", sampled_pts.max())\n",
    "print(\"min: \", sampled_pts.min())\n",
    "print(\"mean: \", sampled_pts.mean())\n",
    "print(\"mean: \", sampled_pts.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eee1a195-0a12-4f5c-ae99-edee5a0cab41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_point_clouds(pcs: torch.Tensor, return_shift_scale=False, padding=0.0):\n",
    "    # refactored version for batched processing\n",
    "    pcs = pcs.clone()\n",
    "    pc_max = pcs[..., :3].amax(dim=1, keepdim=True)  # (B, 1, 3)\n",
    "    pc_min = pcs[..., :3].amin(dim=1, keepdim=True)\n",
    "    shift = (pc_min + pc_max) / 2\n",
    "    scale = 2 / (pc_max - pc_min).amax(dim=-1, keepdim=True) * (1 - padding)\n",
    "    pcs[..., :3] = (pcs[..., :3] - shift) * scale\n",
    "\n",
    "    if return_shift_scale:\n",
    "        return pcs, shift, scale\n",
    "    else:\n",
    "        return pcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "77b0645e-7d06-4314-acb5-135ba4c8b0bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 3])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(sampled_pts, dim=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2e3842f4-1f8d-4691-9813-fceccb1e9613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max:  tensor(0.9000, device='cuda:0')\n",
      "min:  tensor(-0.9000, device='cuda:0')\n",
      "mean:  tensor(0.0369, device='cuda:0')\n",
      "std:  tensor(0.3927, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "sampled_pts_norm, shift, scale = normalize_point_clouds(sampled_pts, return_shift_scale=True, padding=0.1)\n",
    "print(\"max: \", sampled_pts_norm.max())\n",
    "print(\"min: \", sampled_pts_norm.min())\n",
    "print(\"mean: \", sampled_pts_norm.mean())\n",
    "print(\"std: \", sampled_pts_norm.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "be2053f5-26db-434d-89ff-205c404b188d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 3])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shift.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "64728470-86e0-4f7e-a957-5b029754c513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize_point_clouds(pcs: torch.Tensor, shift: torch.Tensor, scale: torch.Tensor):\n",
    "    pcs = pcs.clone()\n",
    "    pcs[..., :3] = pcs[..., :3] / scale + shift\n",
    "    return pcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a4bade76-514c-40b7-8c74-b8d58ea4770b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max:  tensor(0.3420, device='cuda:0')\n",
      "min:  tensor(-0.3423, device='cuda:0')\n",
      "mean:  tensor(0.0139, device='cuda:0')\n",
      "std:  tensor(0.1493, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "denormed_pts = denormalize_point_clouds(sampled_pts_norm, shift, scale)\n",
    "print(\"max: \", denormed_pts.max())\n",
    "print(\"min: \", denormed_pts.min())\n",
    "print(\"mean: \", denormed_pts.mean())\n",
    "print(\"std: \", denormed_pts.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "64cc184d-7726-4567-af7d-7c9beaa887c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.txt  aerodynamics_workshop_2k.hdr  bsdf_256_256.bin\n"
     ]
    }
   ],
   "source": [
    "!ls /root/dataset_sj/DMTet/data/irrmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "092ff687-3881-4678-aaf7-51bfaba9824d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /root/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module renderutils_plugin, skipping build step...\n",
      "Loading extension module renderutils_plugin...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0.1502225399017334 (s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3616120/3621232852.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sample_points = torch.tensor(kaolin.ops.mesh.sample_points(ref_mesh.v_pos.unsqueeze(0), ref_mesh.t_pos_idx, 50000)[0][0])\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import nvdiffrast.torch as dr\n",
    "import kaolin\n",
    "\n",
    "from utils.render import light, util, render, mesh\n",
    "\n",
    "\n",
    "glctx = dr.RasterizeCudaContext()\n",
    "train_res = [1000, 1000]\n",
    "fovy = np.deg2rad(45)\n",
    "cam_near_far = [0.1, 1000.0]\n",
    "cam_radius = 2.0\n",
    "env_scale = 1.0\n",
    "envlight = light.load_env(\"/root/dataset_sj/DMTet/data/irrmaps/aerodynamics_workshop_2k.hdr\", env_scale)\n",
    "layers = 1\n",
    "flat_shading = False\n",
    "\n",
    "def _random_scene(train_res, fovy, cam_near_far, cam_radius):\n",
    "    # ==============================================================================================\n",
    "    #  Setup projection matrix\n",
    "    # ==============================================================================================\n",
    "    proj_mtx = util.perspective(fovy, train_res[1] / train_res[0], cam_near_far[0], cam_near_far[1])\n",
    "\n",
    "    # ==============================================================================================\n",
    "    #  Random camera & light position\n",
    "    # ==============================================================================================\n",
    "\n",
    "    # Random rotation/translation matrix for optimization.\n",
    "    mv     = util.translate(0, 0, -cam_radius) @ util.random_rotation_translation(0.2)\n",
    "    mvp    = proj_mtx @ mv\n",
    "    campos = torch.linalg.inv(mv)[:3, 3]\n",
    "\n",
    "    return mv[None, ...].cuda(), mvp[None, ...].cuda(), campos[None, ...].cuda(), train_res # Add batch dimension\n",
    "\n",
    "\n",
    "def get_item(glctx, ref_mesh, envlight, train_res, fovy, cam_near_far, cam_radius, layers=1, flat_shading=False, random_light=True, camera_light=False):\n",
    "    mv, mvp, campos, train_res = _random_scene(train_res, fovy, cam_near_far, cam_radius)\n",
    "    if random_light: # True\n",
    "        rnd_rot = util.random_rotation()\n",
    "        camera_mv = rnd_rot.unsqueeze(0).clone()\n",
    "    elif camera_light: # False\n",
    "        camera_mv = mv.clone()\n",
    "    else:\n",
    "        camera_mv = None\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        render_out = render.render_mesh(glctx, ref_mesh, mvp, campos, envlight, train_res, spp=1, \n",
    "                            num_layers=layers, msaa=True, background=None, xfm_lgt=camera_mv, flat_shading=flat_shading)\n",
    "        img = render_out['shaded']\n",
    "        img_second = render_out['shaded_second']\n",
    "        normal = render_out['normal']\n",
    "        depth = render_out['depth']\n",
    "        geo_normal = render_out['geo_normal']\n",
    "        pos = render_out['pos']\n",
    "    \n",
    "        sample_points = torch.tensor(kaolin.ops.mesh.sample_points(ref_mesh.v_pos.unsqueeze(0), ref_mesh.t_pos_idx, 50000)[0][0])\n",
    "        vertex_points = ref_mesh.v_pos\n",
    "    \n",
    "    return_dict = {\n",
    "        'mv' : mv,\n",
    "        'mvp' : mvp,\n",
    "        'campos' : campos,\n",
    "        'resolution' : train_res,\n",
    "        'spp' : 1, # from FLAGS\n",
    "        'img' : img,\n",
    "        'img_second' : img_second,\n",
    "        'spts': sample_points,\n",
    "        'vpts': vertex_points,\n",
    "        'faces': ref_mesh.t_pos_idx,\n",
    "        'depth': depth,\n",
    "        'normal': normal,\n",
    "        'geo_normal': geo_normal,\n",
    "        'geo_viewdir': render_out['geo_viewdir'],\n",
    "        'pos': pos,\n",
    "        'envlight_transform': camera_mv,\n",
    "        'mask': render_out['mask'],\n",
    "        'mask_cont': render_out['mask_cont'],\n",
    "        'rast_triangle_id': render_out['rast_triangle_id']\n",
    "    }\n",
    "\n",
    "start = time.time()\n",
    "data = get_item(glctx, ref_mesh, envlight, train_res, fovy, cam_near_far, cam_radius)\n",
    "end = time.time()\n",
    "print(f\"Time: {end - start} (s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4dc1c4b2-f27e-4b0b-85f3-1abb746595ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from utils.render import mesh, texture, render, light, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2785d13a-3a2b-44c3-934c-e63a767a3119",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDataset(Dataset):\n",
    "    \"\"\"Basic dataset interface\"\"\"\n",
    "    def __init__(self): \n",
    "        super().__init__()\n",
    "\n",
    "    def __len__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __getitem__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def collate(self, batch):\n",
    "        iter_res, iter_spp = batch[0]['resolution'], batch[0]['spp']\n",
    "        res_dict = {\n",
    "                'mv' : torch.cat(list([item['mv'] for item in batch]), dim=0),\n",
    "                'mvp' : torch.cat(list([item['mvp'] for item in batch]), dim=0),\n",
    "                'campos' : torch.cat(list([item['campos'] for item in batch]), dim=0),\n",
    "                'resolution' : iter_res,\n",
    "                'spp' : iter_spp,\n",
    "                'img' : torch.cat(list([item['img'] for item in batch]), dim=0)\n",
    "            }\n",
    "\n",
    "        if 'sampled_pts' in batch[0]:\n",
    "            res_dict['sampled_pts'] = batch[0]['sampled_pts']\n",
    "        if 'vertex_pts' in batch[0]:\n",
    "            res_dict['vertex_pts'] = batch[0]['vertex_pts']\n",
    "        if 'faces' in batch[0]:\n",
    "            res_dict['faces'] = batch[0]['faces']\n",
    "        if 'rast_triangle_id' in batch[0]:\n",
    "            res_dict['rast_triangle_id'] = batch[0]['rast_triangle_id']\n",
    "        \n",
    "        if 'depth' in batch[0]:\n",
    "            res_dict['depth'] = torch.cat(list([item['depth'] for item in batch]), dim=0)\n",
    "        if 'normal' in batch[0]:\n",
    "            res_dict['normal'] = torch.cat(list([item['normal'] for item in batch]), dim=0)\n",
    "        if 'geo_normal' in batch[0]:\n",
    "            res_dict['geo_normal'] = torch.cat(list([item['geo_normal'] for item in batch]), dim=0)\n",
    "        if 'geo_viewdir' in batch[0]:\n",
    "            res_dict['geo_viewdir'] = torch.cat(list([item['geo_viewdir'] for item in batch]), dim=0)\n",
    "        if 'pos' in batch[0]:\n",
    "            res_dict['pos'] = torch.cat(list([item['pos'] for item in batch]), dim=0)\n",
    "        if 'mask' in batch[0]:\n",
    "            res_dict['mask'] = torch.cat(list([item['mask'] for item in batch]), dim=0)\n",
    "        if 'mask_cont' in batch[0]:\n",
    "            res_dict['mask_cont'] = torch.cat(list([item['mask_cont'] for item in batch]), dim=0)\n",
    "        if 'envlight_transform' in batch[0]:\n",
    "            if batch[0]['envlight_transform'] is not None:\n",
    "                res_dict['envlight_transform'] = torch.cat(list([item['envlight_transform'] for item in batch]), dim=0)\n",
    "            else:\n",
    "                res_dict['envlight_transform'] = None\n",
    "\n",
    "        try:\n",
    "            res_dict['depth_second'] = torch.cat(list([item['depth_second'] for item in batch]), dim=0)\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            res_dict['normal_second'] = torch.cat(list([item['normal_second'] for item in batch]), dim=0)\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            res_dict['img_second'] = torch.cat(list([item['img_second'] for item in batch]), dim=0)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "        return res_dict\n",
    "\n",
    "\n",
    "class ShapeNetDataset(BaseDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        glctx,\n",
    "        data_root,\n",
    "        mesh_data_root,\n",
    "        num_pts,\n",
    "        rendering_kwargs,\n",
    "        categories=[\"chair\", \"table\"],\n",
    "        add_camera_cond=False,\n",
    "        split=\"train\", # [\"train\", \"val\", \"test\"]\n",
    "        split_type=\"text_cond\" # [\"text_cond\", \"uncond\"]\n",
    "    ):\n",
    "        assert split in [\"train\", \"val\", \"test\"] and split_type in [\"text_cond\", \"uncond\"]\n",
    "        \n",
    "        if categories == \"all\":\n",
    "            categories = [\"airplane\", \"car\", \"chair\", \"table\"] if split_type == \"uncond\" else [\"chair\", \"table\"]\n",
    "        \n",
    "        self.data_root = data_root\n",
    "        self.mesh_data_root = mesh_data_root\n",
    "        self.rendering_kwargs = rendering_kwargs\n",
    "        self.num_pts = num_pts\n",
    "        \n",
    "        split_root = os.path.join(data_root, \"splits\")\n",
    "        self.split = split\n",
    "        self.split_type = split_type\n",
    "        \n",
    "        self.glctx = glctx\n",
    "        self.fovy = np.deg2rad(rendering_kwargs.fovy)\n",
    "        self.add_camera_cond = add_camera_cond\n",
    "        self.envlight = light.load_env(os.path.join(data_root, \"irrmaps\", \"aerodynamics_workshop_2k.hdr\"), scale=rendering_kwargs.env_scale)\n",
    "        if rendering_kwargs.mtl_type == \"default\":\n",
    "            self.mtl = {\n",
    "                'name' : '_default_mat',\n",
    "                'bsdf': 'diffuse',\n",
    "                'uniform': True,\n",
    "                'kd'   : texture.Texture2D(torch.tensor([0.75, 0.3, 0.6], dtype=torch.float32, device='cuda'), trainable=False),\n",
    "                'ks'   : texture.Texture2D(torch.tensor([0.0, 0.0, 0.0], dtype=torch.float32, device='cuda'), trainable=False)\n",
    "            }\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # Get text info\n",
    "        self.text_data = None\n",
    "        if split_type == \"text_cond\":\n",
    "            text_path = os.path.join(data_root, \"text\", \"captions.tablechair.csv\")\n",
    "            self.text_data = {}\n",
    "            with open(text_path, \"r\") as file:\n",
    "                text_data = csv.DictReader(file)\n",
    "                for col in text_data:\n",
    "                    self.text_data[col[\"modelId\"]] = col[\"description\"]\n",
    "\n",
    "        # Get meshes info according to the split\n",
    "        split_model_ids = []\n",
    "        self.mesh_paths, self.camera_paths, self.img_paths = [], [], []\n",
    "        self.cat_ids, self.model_ids = [], []\n",
    "        for cat, cat_id in self.get_category_dict().items():\n",
    "            if cat in categories:\n",
    "                with open(os.path.join(data_root, f\"shapenet_v1_{cat_id}.json\"), \"r\") as file:\n",
    "                    mesh_paths_per_cat = json.load(file)\n",
    "                \n",
    "                if split_type == \"text_cond\":\n",
    "                    with open(os.path.join(split_root, split_type, f\"{cat_id}_{split}.lst\"), \"r\") as file:\n",
    "                        split_model_ids = [line.strip() for line in file]\n",
    "                elif split_type == \"uncond\":\n",
    "                    with open(os.path.join(split_root, split_type, f\"shapenet_{cat}\", f\"{split}.txt\"), \"r\") as file:\n",
    "                        split_model_ids = [line.strip() for line in file]\n",
    "\n",
    "                for path in mesh_paths_per_cat:\n",
    "                    model_id = path.split(\"/\")[-2]\n",
    "                    if split_type == \"text_cond\":\n",
    "                        if model_id in split_model_ids and model_id in self.text_data:\n",
    "                            self.mesh_paths.append(os.path.join(mesh_data_root, path))\n",
    "                            # self.camera_paths.append(os.path.join(data_root, \"rendered_shapenet_v1\", \"camera\", cat_id, model_id)) # TODO: check\n",
    "                            # self.img_paths.append(os.path.join(data_root, \"rendered_shapenet_v1\", \"img\", cat_id, model_id)) # TODO: check\n",
    "                            self.cat_ids.append(cat_id)\n",
    "                            self.model_ids.append(model_id)\n",
    "                            \n",
    "                    elif split_type == \"uncond\":\n",
    "                        if model_id in split_model_ids:\n",
    "                            self.mesh_paths.append(os.path.join(mesh_data_root, path))\n",
    "                            # self.camera_paths.append(os.path.join(data_root, \"rendered_shapenet_v1\", \"camera\", cat_id, model_id)) # TODO: check\n",
    "                            # self.img_paths.append(os.path.join(data_root, \"rendered_shapenet_v1\", \"img\", cat_id, model_id)) # TODO: check\n",
    "                            self.cat_ids.append(cat_id)\n",
    "                            self.model_ids.append(model_id)\n",
    "\n",
    "\n",
    "    def get_category_dict(self):\n",
    "        # should be updated if additional categories used\n",
    "        return {\n",
    "            \"chair\": \"03001627\", # 0\n",
    "            \"table\": \"04379243\", # 1\n",
    "            \"airplane\": \"02691156\", # 3\n",
    "            \"car\": \"02958343\" # 4\n",
    "        }\n",
    "\n",
    "\n",
    "    def normalize_point_clouds(self, pcs: torch.Tensor, return_shift_scale=True, padding=0.0):\n",
    "        # refactored version for batched processing\n",
    "        pcs = pcs.clone()\n",
    "        pc_max = pcs[..., :3].amax(dim=1, keepdim=True)  # (B, 1, 3)\n",
    "        pc_min = pcs[..., :3].amin(dim=1, keepdim=True)\n",
    "        shift = (pc_min + pc_max) / 2\n",
    "        scale = 2 / (pc_max - pc_min).amax(dim=-1, keepdim=True) * (1 - padding)\n",
    "        pcs[..., :3] = (pcs[..., :3] - shift) * scale\n",
    "\n",
    "        if return_shift_scale:\n",
    "            return pcs, shift, scale\n",
    "        else:\n",
    "            return pcs\n",
    "    \n",
    "    \n",
    "    def denormalize_point_clouds(pcs: torch.Tensor, shift: torch.Tensor, scale: torch.Tensor):\n",
    "        pcs = pcs.clone()\n",
    "        pcs[..., :3] = pcs[..., :3] / scale + shift\n",
    "        return pcs\n",
    "    \n",
    "    \n",
    "    def _rotate_scene(self, idx):\n",
    "        proj_mtx = util.perspective(self.fovy, self.rendering_kwargs.render_res[1] / self.rendering_kwargs.render_res[0],\n",
    "                                    self.rendering_kwargs.cam_near_far[0], self.rendering_kwargs.cam_near_far[1])\n",
    "\n",
    "        # Smooth rotation for display.\n",
    "        ang    = (idx / 50) * np.pi * 2\n",
    "        mv     = util.translate(0, 0, -self.rendering_kwargs.cam_radius) @ (util.rotate_x(-0.4) @ util.rotate_y(ang))\n",
    "        mvp    = proj_mtx @ mv\n",
    "        campos = torch.linalg.inv(mv)[:3, 3]\n",
    "\n",
    "        return mv[None, ...].cuda(), mvp[None, ...].cuda(), campos[None, ...].cuda()\n",
    "\n",
    "    def _random_scene(self):\n",
    "        # Setup projection matrix\n",
    "        proj_mtx = util.perspective(self.fovy, self.rendering_kwargs.render_res[1] / self.rendering_kwargs.render_res[0],\n",
    "                                    self.rendering_kwargs.cam_near_far[0], self.rendering_kwargs.cam_near_far[1])\n",
    "\n",
    "        # Random camera & light position\n",
    "        # Random rotation/translation matrix for optimization.\n",
    "        mv     = util.translate(0, 0, -self.rendering_kwargs.cam_radius) @ util.random_rotation_translation(0.2)\n",
    "        mvp    = proj_mtx @ mv\n",
    "        campos = torch.linalg.inv(mv)[:3, 3]\n",
    "\n",
    "        return mv[None, ...].cuda(), mvp[None, ...].cuda(), campos[None, ...].cuda() # Add batch dimension\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mesh_paths)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        self.ref_mesh = mesh.load_mesh(self.mesh_paths[idx], None, self.mtl,\n",
    "                                       use_default=self.rendering_kwargs.normal_only, no_additional=True)\n",
    "        self.ref_mesh = mesh.center_by_reference(self.ref_mesh, mesh.aabb_clean(self.ref_mesh), 1.)\n",
    "        self.ref_mesh = mesh.auto_normals(self.ref_mesh)\n",
    "        \n",
    "        resolution = self.rendering_kwargs.render_res\n",
    "        spp = self.rendering_kwargs.spp\n",
    "        if self.split == \"val\" or \"test\":\n",
    "            mv, mvp, campos = self._rotate_scene(idx)\n",
    "            camera_mv = None\n",
    "        else:\n",
    "            mv, mvp, campos = self._random_scene()\n",
    "            if self.rendering_kwargs.light_type == \"random\":\n",
    "                rnd_rot = util.random_rotation()\n",
    "                camera_mv = rnd_rot.unsqueeze(0).clone()\n",
    "            elif self.rendering_kwargs.light_type == \"camera\":\n",
    "                camera_mv = mv.clone()\n",
    "            else:\n",
    "                raise NotImplementedError() # camera_mv = None\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            render_out = render.render_mesh(self.glctx, self.ref_mesh, mvp, campos, self.envlight, resolution, spp,\n",
    "                                            num_layers=self.rendering_kwargs.layers, msaa=True, background=None,\n",
    "                                            xfm_lgt=camera_mv, flat_shading=self.rendering_kwargs.flat_shading)\n",
    "            \n",
    "            sampled_pts = kaolin.ops.mesh.sample_points(self.ref_mesh.v_pos.unsqueeze(0), self.ref_mesh.t_pos_idx, self.num_pts)[0][0]\n",
    "        \n",
    "        # TODO: remove unused variables\n",
    "        return_dict = {\n",
    "            \"mv\": mv,\n",
    "            \"mvp\": mvp,\n",
    "            \"campos\": campos,\n",
    "            \"resolution\": resolution,\n",
    "            \"spp\": spp,\n",
    "            \"envlight_transform\": camera_mv,\n",
    "            \"img\": render_out[\"shaded\"],\n",
    "            \"img_second\": render_out[\"shaded_second\"],\n",
    "            \"depth\": render_out[\"depth\"],\n",
    "            \"normal\": render_out[\"normal\"],\n",
    "            \"geo_normal\": render_out[\"geo_normal\"],\n",
    "            \"geo_viewdir\": render_out[\"geo_viewdir\"],\n",
    "            \"pos\": render_out[\"pos\"],\n",
    "            \"mask\": render_out[\"mask\"],\n",
    "            \"mask_cont\": render_out[\"mask_cont\"],\n",
    "            \"rast_triangle_id\": render_out[\"rast_triangle_id\"],\n",
    "            \"sampled_pts\": sampled_pts,\n",
    "            \"vertex_pts\": self.ref_mesh.v_pos,\n",
    "            \"faces\": self.ref_mesh.t_pos_idx,\n",
    "            \"cat_id\": self.cat_ids[idx],\n",
    "            \"model_id\": self.model_ids[idx]\n",
    "        }\n",
    "        \n",
    "        if render_out[\"depth_second\"] is not None:\n",
    "            return_dict[\"depth_second\"] = render_out[\"depth_second\"]\n",
    "        \n",
    "        if render_out[\"normal_second\"] is not None:\n",
    "            return_dict[\"normal_second\"] = render_out[\"normal_second\"]\n",
    "        \n",
    "        return return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2e9fe691-5ee8-4d8c-b8f5-b46d5f69afc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02691156  02958343  03001627  04090263\t04379243\n"
     ]
    }
   ],
   "source": [
    "!ls /root/dev/DMTet_Models/MeshDiffusion/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "766501be-5669-418b-abba-2a1fbe5555ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '/root/dev/DMTet_Models/Meshdiffusion/data/03001627/': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!ls /root/dev/DMTet_Models/Meshdiffusion/data/03001627/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2a7c39ae-8ca1-4dea-aeba-e2518a940fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "irrmaps\t\t\t   shapenet_v1_03790512.json  tets\n",
      "shapenet_v1_02691156.json  shapenet_v1_04090263.json  text\n",
      "shapenet_v1_02958343.json  shapenet_v1_04379243.json\n",
      "shapenet_v1_03001627.json  splits\n"
     ]
    }
   ],
   "source": [
    "!ls /root/dev/triplora/datasets/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bb9193cd-6c85-4066-bf7d-c5d2fdabc593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6577\n"
     ]
    }
   ],
   "source": [
    "from easydict import EasyDict\n",
    "\n",
    "data_root = \"/root/dev/triplora/datasets/data\"\n",
    "mesh_data_root = \"/root/dev/DMTet_Models/MeshDiffusion/data\"\n",
    "num_pts = 20480\n",
    "rendering_kwargs = EasyDict({\n",
    "    \"env_scale\": 1.0,\n",
    "    \"fovy\": 45,\n",
    "    \"cam_radius\": 2.0,\n",
    "    \"mtl_type\": \"default\",\n",
    "    \"spp\": 1,\n",
    "    \"render_res\": [1000, 1000],\n",
    "    \"cam_near_far\": [0.1, 1000.0],\n",
    "    \"normal_only\": True,\n",
    "    \"flat_shading\": False,\n",
    "    \"light_type\": \"random\",\n",
    "    \"layers\": 1\n",
    "})\n",
    "\n",
    "ds_train = ShapeNetDataset(glctx, data_root, mesh_data_root, num_pts, rendering_kwargs,\n",
    "                           categories=[\"chair\"], split=\"train\", split_type=\"uncond\")\n",
    "ds_val = ShapeNetDataset(glctx, data_root, mesh_data_root, num_pts, rendering_kwargs,\n",
    "                        categories=[\"chair\"], split=\"val\", split_type=\"uncond\")\n",
    "ds_test = ShapeNetDataset(glctx, data_root, mesh_data_root, num_pts, rendering_kwargs,\n",
    "                          categories=[\"chair\"], split=\"val\", split_type=\"uncond\")\n",
    "\n",
    "ds_train = ShapeNetDataset(glctx, data_root, mesh_data_root, num_pts, rendering_kwargs,\n",
    "                           categories=[\"chair\"], split=\"train\", split_type=\"text_cond\")\n",
    "ds_test = ShapeNetDataset(glctx, data_root, mesh_data_root, num_pts, rendering_kwargs,\n",
    "                          categories=[\"chair\"], split=\"test\", split_type=\"text_cond\")\n",
    "\n",
    "print(len(ds_train) + len(ds_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3c203f9d-225e-4f87-b316-09484ddb5406",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence, Union\n",
    "\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "\n",
    "def build_dataloaders(\n",
    "    batch_size: int,\n",
    "    num_workers: int,\n",
    "    ds: Dataset,\n",
    "    ddp=None,\n",
    "    shuffle=False,\n",
    "    pin_memory=None,\n",
    "    persistent_workers=None,\n",
    "    **kwargs,\n",
    ") -> Union[DataLoader, Sequence[DataLoader]]:\n",
    "    if pin_memory is None:\n",
    "        pin_memory = torch.cuda.is_available()\n",
    "    if persistent_workers is None:\n",
    "        persistent_workers = num_workers > 0\n",
    "    dl_kwargs = dict(\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        persistent_workers=persistent_workers,\n",
    "    )\n",
    "    dl_kwargs.update(kwargs)\n",
    "\n",
    "    if ddp is None:\n",
    "        ddp = dist.is_initialized() and dist.get_world_size() > 1\n",
    "\n",
    "    if ddp:\n",
    "        sampler = DistributedSampler(ds, shuffle=shuffle)\n",
    "        dl = DataLoader(ds, sampler=sampler, **dl_kwargs)\n",
    "    else:\n",
    "        dl = DataLoader(ds, shuffle=shuffle, **dl_kwargs)\n",
    "\n",
    "    return dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f85f593b-f49c-4e25-b646-d8f6c17eb67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = DataLoader(ds_train, batch_size=8, num_workers=0, collate_fn=ds_train.collate, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "968a3d58-7936-4f10-a804-406a832b623d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(dl_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "dc6c2c87-40cc-48da-8e19-537275a696c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['mv', 'mvp', 'campos', 'resolution', 'spp', 'img', 'sampled_pts', 'vertex_pts', 'faces', 'rast_triangle_id', 'depth', 'normal', 'geo_normal', 'geo_viewdir', 'pos', 'mask', 'mask_cont', 'envlight_transform', 'depth_second', 'normal_second', 'img_second'])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e1a44e3a-7ebd-413a-ba35-fe1c099272be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1000, 1000, 4])\n",
      "torch.Size([8, 1000, 1000, 4])\n",
      "torch.Size([8, 1000, 1000, 2])\n",
      "torch.Size([8, 1000, 1000, 2])\n",
      "torch.Size([20480, 3])\n",
      "torch.Size([324, 3])\n",
      "torch.Size([1258, 3])\n"
     ]
    }
   ],
   "source": [
    "print(data[\"img\"].shape)\n",
    "print(data[\"img_second\"].shape)\n",
    "print(data[\"depth\"].shape)\n",
    "print(data[\"depth_second\"].shape)\n",
    "print(data[\"sampled_pts\"].shape)\n",
    "print(data[\"vertex_pts\"].shape)\n",
    "print(data[\"faces\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67169ea-60d0-4086-aa7d-371958d48f23",
   "metadata": {},
   "source": [
    "## Data loading of MeshDiffusion and GET3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bdfb1f-b28a-449c-b318-e3db6596a23f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbca866-54d8-491f-acbf-ab27f2c2bbe1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "336b1ecc-6707-4770-8de7-15ab11bb60ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import trimesh\n",
    "\n",
    "# scene = trimesh.Scene()\n",
    "# scene.add_geometry(trimesh.load_mesh(mesh_path))\n",
    "# scene.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debc4d67-6e1e-4ab3-ae31-c281180329c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MeshDiffusion.nvdiffrec.lib.render import render\n",
    "\n",
    "with torch.no_grad():\n",
    "    render_out = render.render_mesh(glctx, ref_mesh, )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99365fdc-8092-4187-93b8-23c6c3caeeea",
   "metadata": {},
   "source": [
    "# DMTet related classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1b285a74-97fd-44de-bd10-4f1eae85ec08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dmtet_utils.py!!\n",
    "# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n",
    "#\n",
    "# NVIDIA CORPORATION & AFFILIATES and its licensors retain all intellectual property\n",
    "# and proprietary rights in and to this software, related documentation\n",
    "# and any modifications thereto.  Any use, reproduction, disclosure or\n",
    "# distribution of this software and related documentation without an express\n",
    "# license agreement from NVIDIA CORPORATION & AFFILIATES is strictly prohibited.\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def get_center_boundary_index(verts):\n",
    "    length_ = torch.sum(verts ** 2, dim=-1)\n",
    "    center_idx = torch.argmin(length_)\n",
    "    boundary_neg = verts == verts.max()\n",
    "    boundary_pos = verts == verts.min()\n",
    "    boundary = torch.bitwise_or(boundary_pos, boundary_neg)\n",
    "    boundary = torch.sum(boundary.float(), dim=-1)\n",
    "    boundary_idx = torch.nonzero(boundary)\n",
    "    return center_idx, boundary_idx.squeeze(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cff13d15-5d20-40cf-b2af-10225b78d380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128_tets_cropped.npz  README.md     generate_tets.py\n",
      "64_tets_cropped.npz   crop_tets.py\n"
     ]
    }
   ],
   "source": [
    "!ls /root/dev/DMTet_Models/MeshDiffusion/nvdiffrec/data_/tets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad6d23f7-8832-4e02-911b-2c6a69cfcd0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100_compress.npz  70_compress.npz  90_compress.npz\n",
      "64_compress.npz   80_compress.npz\n"
     ]
    }
   ],
   "source": [
    "!ls /root/dev/DMTet_Models/GET3D/tets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf2eb575-d8de-4501-81c9-58ebb1158511",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e08d4a3-b84b-4777-b4e5-d8e871fa8848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30512, 3)\n",
      "(159330, 4)\n",
      "\n",
      "(36562, 3)\n",
      "(192492, 4)\n"
     ]
    }
   ],
   "source": [
    "tets_meshdiffusion = np.load(\"./MeshDiffusion/nvdiffrec/data_/tets/64_tets_cropped.npz\")\n",
    "tets_get3d = np.load(\"./GET3D/tets/64_compress.npz\")\n",
    "\n",
    "print(tets_meshdiffusion[\"vertices\"].shape)\n",
    "print(tets_meshdiffusion[\"indices\"].shape)\n",
    "print()\n",
    "print(tets_get3d[\"vertices\"].shape)\n",
    "print(tets_get3d[\"tets\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "92f383c6-a75d-4aaa-ad61-b14be880e8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class DMTet:\n",
    "    def __init__(self, device=\"cuda\"):\n",
    "        self.triangle_table = torch.tensor([\n",
    "                [-1, -1, -1, -1, -1, -1],\n",
    "                [ 1,  0,  2, -1, -1, -1],\n",
    "                [ 4,  0,  3, -1, -1, -1],\n",
    "                [ 1,  4,  2,  1,  3,  4],\n",
    "                [ 3,  1,  5, -1, -1, -1],\n",
    "                [ 2,  3,  0,  2,  5,  3],\n",
    "                [ 1,  4,  0,  1,  5,  4],\n",
    "                [ 4,  2,  5, -1, -1, -1],\n",
    "                [ 4,  5,  2, -1, -1, -1],\n",
    "                [ 4,  1,  0,  4,  5,  1],\n",
    "                [ 3,  2,  0,  3,  5,  2],\n",
    "                [ 1,  3,  5, -1, -1, -1],\n",
    "                [ 4,  1,  2,  4,  3,  1],\n",
    "                [ 3,  0,  4, -1, -1, -1],\n",
    "                [ 2,  0,  1, -1, -1, -1],\n",
    "                [-1, -1, -1, -1, -1, -1]\n",
    "                ], dtype=torch.long, device=device)\n",
    "\n",
    "        self.num_triangles_table = torch.tensor([0,1,1,2,1,2,2,1,1,2,2,1,2,1,1,0], dtype=torch.long, device=device)\n",
    "        self.base_tet_edges = torch.tensor([0,1,0,2,0,3,1,2,1,3,2,3], dtype=torch.long, device=device)\n",
    "        self.v_id = torch.pow(2, torch.arange(4, dtype=torch.long, device=device))\n",
    "\n",
    "        self.tet_table = torch.tensor(\n",
    "            [[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "             [0, 4, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "             [1, 4, 7, 8, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "             [1, 0, 8, 7, 0, 5, 8, 7, 0, 5, 6, 8],\n",
    "             [2, 5, 7, 9, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "             [2, 0, 9, 7, 0, 4, 9, 7, 0, 4, 6, 9],\n",
    "             [2, 1, 9, 5, 1, 4, 9, 5, 1, 4, 8, 9],\n",
    "             [6, 0, 1, 2, 6, 1, 2, 8, 6, 8, 2, 9],\n",
    "             [3, 6, 8, 9, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "             [3, 0, 9, 8, 0, 4, 9, 8, 0, 4, 5, 9],\n",
    "             [3, 1, 9, 6, 1, 4, 9, 6, 1, 4, 7, 9],\n",
    "             [5, 0, 1, 3, 5, 1, 3, 7, 5, 7, 3, 9],\n",
    "             [3, 2, 8, 6, 2, 5, 8, 6, 2, 5, 7, 8],\n",
    "             [4, 0, 2, 3, 4, 2, 3, 7, 4, 7, 3, 8],\n",
    "             [4, 1, 2, 3, 4, 2, 3, 5, 4, 5, 3, 6],\n",
    "             [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]], dtype=torch.long, device=device)\n",
    "        self.num_tets_table = torch.tensor([0, 1, 1, 3, 1, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 0], dtype=torch.long, device=device)\n",
    "\n",
    "    ###############################################################################\n",
    "    # Utility functions # NOTE → needed?\n",
    "    ###############################################################################\n",
    "\n",
    "    def sort_edges(self, edges_ex2):\n",
    "        with torch.no_grad():\n",
    "            order = (edges_ex2[:,0] > edges_ex2[:,1]).long()\n",
    "            order = order.unsqueeze(dim=1)\n",
    "\n",
    "            a = torch.gather(input=edges_ex2, index=order, dim=1)      \n",
    "            b = torch.gather(input=edges_ex2, index=1-order, dim=1)  \n",
    "\n",
    "        return torch.stack([a, b],-1)\n",
    "\n",
    "    def map_uv(self, faces, face_gidx, max_idx):\n",
    "        N = int(np.ceil(np.sqrt((max_idx+1)//2)))\n",
    "        tex_y, tex_x = torch.meshgrid(\n",
    "            torch.linspace(0, 1 - (1 / N), N, dtype=torch.float32, device=self.device),\n",
    "            torch.linspace(0, 1 - (1 / N), N, dtype=torch.float32, device=self.device),\n",
    "            indexing='ij'\n",
    "        )\n",
    "\n",
    "        pad = 0.9 / N\n",
    "\n",
    "        uvs = torch.stack([\n",
    "            tex_x      , tex_y,\n",
    "            tex_x + pad, tex_y,\n",
    "            tex_x + pad, tex_y + pad,\n",
    "            tex_x      , tex_y + pad\n",
    "        ], dim=-1).view(-1, 2)\n",
    "\n",
    "        def _idx(tet_idx, N):\n",
    "            x = tet_idx % N\n",
    "            y = torch.div(tet_idx, N, rounding_mode='trunc')\n",
    "            return y * N + x\n",
    "\n",
    "        tet_idx = _idx(torch.div(face_gidx, 2, rounding_mode='trunc'), N)\n",
    "        tri_idx = face_gidx % 2\n",
    "\n",
    "        uv_idx = torch.stack((\n",
    "            tet_idx * 4, tet_idx * 4 + tri_idx + 1, tet_idx * 4 + tri_idx + 2\n",
    "        ), dim = -1). view(-1, 3)\n",
    "\n",
    "        return uvs, uv_idx\n",
    "\n",
    "    ###############################################################################\n",
    "    # Marching tets implementation\n",
    "    ###############################################################################\n",
    "\n",
    "    def __call__(self, pos_nx3, sdf_n, tet_fx4, return_tet_mesh=False, ori_v=None):\n",
    "        with torch.no_grad():\n",
    "            occ_n = sdf_n > 0\n",
    "            occ_fx4 = occ_n[tet_fx4.reshape(-1)].reshape(-1, 4)\n",
    "            occ_sum = torch.sum(occ_fx4, -1)\n",
    "            valid_tets = (occ_sum > 0) & (occ_sum < 4)\n",
    "            occ_sum = occ_sum[valid_tets]\n",
    "\n",
    "            # find all vertices\n",
    "            all_edges = tet_fx4[valid_tets][:, self.base_tet_edges].reshape(-1, 2)\n",
    "            all_edges = self.sort_edges(all_edges)\n",
    "            unique_edges, idx_map = torch.unique(all_edges, dim=0, return_inverse=True)  \n",
    "            \n",
    "            unique_edges = unique_edges.long()\n",
    "            mask_edges = occ_n[unique_edges.reshape(-1)].reshape(-1, 2).sum(-1) == 1\n",
    "            mapping = torch.ones((unique_edges.shape[0]), dtype=torch.long, device=self.device) * -1\n",
    "            mapping[mask_edges] = torch.arange(mask_edges.sum(), dtype=torch.long, device=self.device)\n",
    "            idx_map = mapping[idx_map] # map edges to verts\n",
    "\n",
    "            interp_v = unique_edges[mask_edges]\n",
    "        edges_to_interp = pos_nx3[interp_v.reshape(-1)].reshape(-1, 2, 3)\n",
    "        edges_to_interp_sdf = sdf_n[interp_v.reshape(-1)].reshape(-1, 2, 1)\n",
    "        edges_to_interp_sdf[:, -1] *= -1\n",
    "\n",
    "        denominator = edges_to_interp_sdf.sum(1, keepdim=True)\n",
    "\n",
    "        edges_to_interp_sdf = torch.flip(edges_to_interp_sdf, [1]) / denominator\n",
    "        verts = (edges_to_interp * edges_to_interp_sdf).sum(1)\n",
    "\n",
    "        idx_map = idx_map.reshape(-1, 6)\n",
    "\n",
    "        tetindex = (occ_fx4[valid_tets] * self.v_id.unsqueeze(0)).sum(-1)\n",
    "        num_triangles = self.num_triangles_table[tetindex]\n",
    "\n",
    "        # Generate triangle indices\n",
    "        faces = torch.cat((\n",
    "            torch.gather(input=idx_map[num_triangles==1], dim=1, index=self.triangle_table[tetindex[num_triangles==1]][:, :3]).reshape(-1, 3),\n",
    "            torch.gather(input=idx_map[num_triangles==2], dim=1, index=self.triangle_table[tetindex[num_triangles==2]][:, :6]).reshape(-1, 3),\n",
    "        ), dim=0)\n",
    "\n",
    "        # # Get global face index (static, does not depend on topology)\n",
    "        # num_tets = tet_fx4.shape[0]\n",
    "        # tet_gidx = torch.arange(num_tets, dtype=torch.long, device=self.device)[valid_tets]\n",
    "        # face_gidx = torch.cat((\n",
    "        #     tet_gidx[num_triangles == 1]*2,\n",
    "        #     torch.stack((tet_gidx[num_triangles == 2]*2, tet_gidx[num_triangles == 2]*2 + 1), dim=-1).view(-1)\n",
    "        # ), dim=0)\n",
    "\n",
    "        # uvs, uv_idx = self.map_uv(faces, face_gidx, num_tets*2)\n",
    "\n",
    "        # face_to_valid_tet = torch.cat((\n",
    "        #     tet_gidx[num_triangles == 1],\n",
    "        #     torch.stack((tet_gidx[num_triangles == 2], tet_gidx[num_triangles == 2]), dim=-1).view(-1)\n",
    "        # ), dim=0)\n",
    "\n",
    "        # valid_vert_idx = tet_fx4[tet_gidx[num_triangles > 0]].long().unique()\n",
    "        \n",
    "        if not return_tet_mesh:\n",
    "            return verts, faces # , uvs, uv_idx, face_to_valid_tet.long(), valid_vert_idx\n",
    "\n",
    "        occupied_verts = ori_v[occ_n]\n",
    "        mapping = torch.ones(pos_nx3.shape[0], dtype=torch.long, device=self.device) * -1\n",
    "        mapping[occ_n] = torch.arange(occupied_verts.shape[0], device=self.device)\n",
    "        tet_fx4 = mapping[tet_fx4.reshape(-1)].reshape(-1, 4)\n",
    "\n",
    "        idx_map = torch.cat([tet_fx4[valid_tets] + verts.shape[0], idx_map], -1) # t x 10\n",
    "        tet_verts = torch.cat([verts, occupied_verts], 0)\n",
    "        num_tets = self.num_tets_table[tetindex]\n",
    "\n",
    "        tets = torch.cat((\n",
    "            torch.gather(input=idx_map[num_tets==1], dim=1, index=self.tet_table[tetindex[num_tets==1]][:, :4]).reshape(-1, 4),\n",
    "            torch.gather(input=idx_map[num_tets==3], dim=1, index=self.tet_table[tetindex[num_tets==3]][:, :12]).reshape(-1, 4)\n",
    "        ), dim=0)\n",
    "\n",
    "        # Add fully occupied tets\n",
    "        fully_occupied = occ_fx4.sum(-1) == 4\n",
    "        tet_fully_occupied = tet_fx4[fully_occupied] + verts.shape[0]\n",
    "        tets = torch.cat([tets, tet_fully_occupied])\n",
    "\n",
    "        return verts, faces, tet_verts, tets\n",
    "\n",
    "\n",
    "class DMTetGeometry(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        grid_res,\n",
    "        scale,\n",
    "        data_root = \"./\",\n",
    "        renderer = None,\n",
    "        render_type = \"neural_render\",\n",
    "        device = \"cuda\",\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.grid_res = grid_res\n",
    "        self.dmtet = DMTet()\n",
    "\n",
    "        init_tets = np.load(os.path.join(data_root, f\"tets/{grid_res}_tets_cropped.npz\"))\n",
    "        self.verts = torch.tensor(init_tets[\"vertices\"], dtype=torch.float32, device=device)\n",
    "        length = self.verts.max(dim=0)[0] - self.verts.min(dim=0)[0]\n",
    "        \n",
    "        if isinstance(scale, list):\n",
    "            # TODO: check (GET3D: scale[1])\n",
    "            self.verts[:, 0] = self.verts[:, 0] * scale[0]\n",
    "            self.verts[:, 1] = self.verts[:, 1] * scale[1]\n",
    "            self.verts[:, 2] = self.verts[:, 2] * scale[2]\n",
    "        else:\n",
    "            self.verts = self.verts * scale\n",
    "        self.indices = torch.tensor(init_tets[\"indices\"], dtype=torch.long, device=device)\n",
    "\n",
    "        # Generate edges\n",
    "        edges = torch.tensor([0, 1, 0, 2, 0, 3, 1, 2, 1, 3, 2, 3], dtype=torch.long, device=device)\n",
    "        all_edges = self.indices[:, edges].reshape(-1, 2)\n",
    "        all_edges_sorted = torch.sort(all_edges, dim=1)[0]\n",
    "        self.all_edges = torch.unique(all_edges_sorted, dim=0)\n",
    "        \n",
    "        # Random init\n",
    "        sdf = torch.rand_like(self.verts[:, 0]).clamp(-1., 1.) - 0.1\n",
    "        self.sdf = torch.nn.Parameter(sdf.clone().detach(), requires_grad=True)\n",
    "        self.register_parameter(\"sdf\", self.sdf)\n",
    "\n",
    "        # Parameters used for fix boundary sdf\n",
    "        self.center_indices, self.boundary_indices = get_center_boundary_index(self.verts)\n",
    "        self.renderer = renderer\n",
    "        self.render_type = render_type\n",
    "\n",
    "\n",
    "    def getAABB(self):\n",
    "        return torch.min(self.verts, dim=0).values, torch.max(self.verts, dim=0).values\n",
    "\n",
    "\n",
    "    def get_mesh(self, v_deformed, sdf, with_uv=False, indices=None):\n",
    "        if indices is None:\n",
    "            indices = self.indices\n",
    "\n",
    "        verts, faces = self.dmtet(v_deformed, sdf, indices)\n",
    "        faces = torch.cat([\n",
    "            faces[:, 0:1], faces[:, 2:3], faces[:, 1:2]\n",
    "        ], dim=-1)\n",
    "        return verts, faces\n",
    "\n",
    "    def get_tet_mesh(self, v_deformed, sdf, with_uv=False, indices=None):\n",
    "        if indices is None:\n",
    "            indices = self.indices\n",
    "\n",
    "        verts, faces, tet_verts, tets = self.dmtet(v_deformed, sdf, indices, return_tet_mesh=True, ori_v=v_deformed)\n",
    "        faces = torch.cat([\n",
    "            faces[:, 0:1], faces[:, 2:3], faces[:, 1:2]\n",
    "        ], dim=-1)\n",
    "        return verts, faces, tet_verts, tets\n",
    "\n",
    "\n",
    "    def render_mesh(self, mesh_verts, mesh_faces, camera_mv, resolution=256, hierarchical_mask=False):\n",
    "        \"\"\" Add description \"\"\"\n",
    "        out_dict = dict()\n",
    "\n",
    "        if self.render_type == \"neural_render\":\n",
    "            tex_pos, mask, hard_mask, rast, v_pos_clip, mask_pyramid, depth = self.renderer.render_mesh(\n",
    "                mesh_verts.unsqueeze(0),\n",
    "                mesh_faces.int(),\n",
    "                camera_mv,\n",
    "                mesh_verts.unsqueeze(0),\n",
    "                resolution=resolution,\n",
    "                device=self.device,\n",
    "                hierarchical_mask=hierarchical_mask\n",
    "            )\n",
    "\n",
    "            out_dict[\"tex_pos\"] = tex_pos\n",
    "            out_dict[\"mask\"] = mask\n",
    "            out_dict[\"hard_mask\"] = hard_mask\n",
    "            out_dict[\"rast\"] = rast\n",
    "            out_dict[\"v_pos_clip\"] = v_pos_clip\n",
    "            out_dict[\"mask_pyramid\"] = mask_pyramid\n",
    "            out_dict[\"depth\"] = depth\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "        return out_dict\n",
    "\n",
    "    def render(self, v_deformed=None, sdf=None, camera_mv=None, resolution=256):\n",
    "        \"\"\" Add description \n",
    "        Here I assume a batch of meshes (can be different mesh and geometry), for the other shapes, the batch is 1. \"\"\"\n",
    "        verts_list, faces_list, all_render_output, batch_size = [], [], [], v_deformed.shape[0]\n",
    "        for i in range(batch_size):\n",
    "            verts, faces = self.get_mesh(v_deformed[i], sdf[i]) # (N,3), (F,3)\n",
    "            verts_list.append(verts)\n",
    "            faces_list.append(faces)\n",
    "            render_output = self.render_mesh(verts, faces, camera_mv[i], resolution)\n",
    "            all_render_output.append(render_output)\n",
    "\n",
    "        # Concat all render outputs\n",
    "        out_keys = all_render_output[0].keys()\n",
    "        out_dict = dict()\n",
    "        for k in out_keys:\n",
    "            value = [v[k] for v in all_render_output]\n",
    "            out_dict[k] = value\n",
    "            # We can do concatenation outside of the render\n",
    "        return out_dict\n",
    "\n",
    "# TODO: add useful functions from meshdiffusion\n",
    "# NOTE: Consider deform_scale!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f95783-824d-45b8-9d8a-6998247c3b33",
   "metadata": {},
   "source": [
    "# Applying LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b7656e3-5fa5-4f83-a433-e2d8a68836fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !export HF_HOME=~/.cache/huggingface "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69bf2553-6b08-458b-a473-ddbb658bf613",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import peft\n",
    "import transformers\n",
    "import diffusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10a09d39-01e0-446d-9e6d-0be11afca456",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def model_params(model):\n",
    "    model_size = 0\n",
    "    for param in model.parameters():\n",
    "        model_size += param.data.nelement()\n",
    "    return model_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dca6aac-6b2b-427f-92e0-50341e7b4048",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from diffusers import UNet2DConditionModel\n",
    "\n",
    "# unet = UNet2DConditionModel.from_pretrained(\n",
    "#     \"stabilityai/stable-diffusion-xl-base-1.0\", subfolder=\"unet\"\n",
    "# )\n",
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-2-base\",\n",
    "    subfolder=\"unet\"\n",
    "    # cache_dir=\"/root/dataset_sj/.cache/huggingface/models/cuda/stable-diffusion-2-base/unet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c2f4d4d-de6e-4764-a6c7-200cbdf18a30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86.5910724"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_params(unet) / 10e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8935220-b8fa-4730-a05c-bf80eb4296c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from diffusers.models import AutoencoderKL\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-2-base\",\n",
    "    subfolder=\"vae\"\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9eaa16bd-7400-42f2-96f8-6b7fa030a639",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.3653863"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_params(vae) / 10e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec0e9bb1-0082-4651-93c4-d23739c56424",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['training', '_parameters', '_buffers', '_non_persistent_buffers_set', '_backward_pre_hooks', '_backward_hooks', '_is_full_backward_hook', '_forward_hooks', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_state_dict_hooks', '_state_dict_pre_hooks', '_load_state_dict_pre_hooks', '_load_state_dict_post_hooks', '_modules', 'layers_per_block', 'gradient_checkpointing'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(vae.encoder).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f746535d-3bb0-4c29-8b25-7eb4d3af6d43",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8, 32, 32])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae.encoder(torch.rand((8, 3, 256, 256)).to(device)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c1a0ddeb-257b-4ed6-8056-e65ce902d175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3, 512, 512])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae.decoder(torch.rand((8, 4, 64, 64))).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dc960e-30b6-4af0-b546-1736ceeb7b03",
   "metadata": {},
   "source": [
    "## AutoencoderKL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7c54a5-c816-45cc-9541-407af12ce569",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.TripLoRA.triplane_ae import AutoencoderTriplane\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8aa941-7fd1-47cd-beae-b0903753542b",
   "metadata": {},
   "source": [
    "## UnetTriplaneModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d057ace8-9c6c-4f2f-90e1-b58f6b6be064",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from models.TripLoRA.triplane.triplane_unet import UNetTriplaneModel\n",
    "\n",
    "model = UNetTriplaneModel(\n",
    "    triplane_res=256,\n",
    "    in_channels=32,\n",
    "    out_channels=2*4,\n",
    "    down_block_types=(\"DownBlockTriplane\", \"ResnetDownsampleBlockTriplane\"),\n",
    "    up_block_types=(\"UpBlockTriplane\", \"ResnetUpsampleBlockTriplane\"),\n",
    "    block_out_channels=(32, 64),\n",
    "    layers_per_block=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4afa1864-8907-43c5-810e-59e941c1540f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5117784"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_params(model) / 10e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "637ce597-024f-4b97-be66-8ea9231b4a1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252\n"
     ]
    }
   ],
   "source": [
    "model_state_dict = model.state_dict()\n",
    "expected_keys = list(model_state_dict.keys())\n",
    "print(len(expected_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2e386981-a0e5-4fd2-861b-ced18e808e37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "686\n"
     ]
    }
   ],
   "source": [
    "pretrained_state_dict = unet.state_dict()\n",
    "loaded_keys = list(pretrained_state_dict.keys())\n",
    "print(len(loaded_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8d86e7e5-8cf8-4765-b1f1-5e3facd55bcf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['conv_in.conv_xy.weight',\n",
       " 'conv_in.conv_xy.bias',\n",
       " 'conv_in.conv_xz.weight',\n",
       " 'conv_in.conv_xz.bias',\n",
       " 'conv_in.conv_yz.weight',\n",
       " 'conv_in.conv_yz.bias',\n",
       " 'down_blocks.0.resnets.0.norm1.norm_xy.weight',\n",
       " 'down_blocks.0.resnets.0.norm1.norm_xy.bias',\n",
       " 'down_blocks.0.resnets.0.norm1.norm_xz.weight',\n",
       " 'down_blocks.0.resnets.0.norm1.norm_xz.bias',\n",
       " 'down_blocks.0.resnets.0.norm1.norm_yz.weight',\n",
       " 'down_blocks.0.resnets.0.norm1.norm_yz.bias',\n",
       " 'down_blocks.0.resnets.0.conv1.conv_xy.weight',\n",
       " 'down_blocks.0.resnets.0.conv1.conv_xy.bias',\n",
       " 'down_blocks.0.resnets.0.conv1.conv_xz.weight',\n",
       " 'down_blocks.0.resnets.0.conv1.conv_xz.bias',\n",
       " 'down_blocks.0.resnets.0.conv1.conv_yz.weight',\n",
       " 'down_blocks.0.resnets.0.conv1.conv_yz.bias',\n",
       " 'down_blocks.0.resnets.0.norm2.norm_xy.weight',\n",
       " 'down_blocks.0.resnets.0.norm2.norm_xy.bias']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_keys[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "432fea55-8c5e-4e9a-a6ed-c1b4ec4b73a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['down_blocks.0.attentions.1.proj_out.bias',\n",
       " 'down_blocks.0.resnets.0.norm1.weight',\n",
       " 'down_blocks.0.resnets.0.norm1.bias',\n",
       " 'down_blocks.0.resnets.0.conv1.weight',\n",
       " 'down_blocks.0.resnets.0.conv1.bias',\n",
       " 'down_blocks.0.resnets.0.time_emb_proj.weight',\n",
       " 'down_blocks.0.resnets.0.time_emb_proj.bias',\n",
       " 'down_blocks.0.resnets.0.norm2.weight',\n",
       " 'down_blocks.0.resnets.0.norm2.bias',\n",
       " 'down_blocks.0.resnets.0.conv2.weight',\n",
       " 'down_blocks.0.resnets.0.conv2.bias',\n",
       " 'down_blocks.0.resnets.1.norm1.weight',\n",
       " 'down_blocks.0.resnets.1.norm1.bias',\n",
       " 'down_blocks.0.resnets.1.conv1.weight',\n",
       " 'down_blocks.0.resnets.1.conv1.bias',\n",
       " 'down_blocks.0.resnets.1.time_emb_proj.weight',\n",
       " 'down_blocks.0.resnets.1.time_emb_proj.bias',\n",
       " 'down_blocks.0.resnets.1.norm2.weight',\n",
       " 'down_blocks.0.resnets.1.norm2.bias',\n",
       " 'down_blocks.0.resnets.1.conv2.weight',\n",
       " 'down_blocks.0.resnets.1.conv2.bias',\n",
       " 'down_blocks.0.downsamplers.0.conv.weight',\n",
       " 'down_blocks.0.downsamplers.0.conv.bias']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_keys[57:80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d72c9576-f9d2-4e17-9d9a-c3ae284ffea2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded pretrained weights:  conv_in.conv_xy.weight\n",
      "loaded pretrained weights:  conv_in.conv_xy.bias\n",
      "loaded pretrained weights:  conv_in.conv_xz.weight\n",
      "loaded pretrained weights:  conv_in.conv_xz.bias\n",
      "loaded pretrained weights:  conv_in.conv_yz.weight\n",
      "loaded pretrained weights:  conv_in.conv_yz.bias\n",
      "loaded pretrained weights:  down_blocks.0.resnets.0.norm1.norm_xy.weight\n",
      "loaded pretrained weights:  down_blocks.0.resnets.0.norm1.norm_xy.bias\n",
      "loaded pretrained weights:  down_blocks.0.resnets.0.norm1.norm_xz.weight\n",
      "loaded pretrained weights:  down_blocks.0.resnets.0.norm1.norm_xz.bias\n",
      "loaded pretrained weights:  down_blocks.0.resnets.0.norm1.norm_yz.weight\n",
      "loaded pretrained weights:  down_blocks.0.resnets.0.norm1.norm_yz.bias\n",
      "loaded pretrained weights:  down_blocks.0.resnets.0.conv1.conv_xy.weight\n",
      "loaded pretrained weights:  down_blocks.0.resnets.0.conv1.conv_xy.bias\n",
      "loaded pretrained weights:  down_blocks.0.resnets.0.conv1.conv_xz.weight\n",
      "loaded pretrained weights:  down_blocks.0.resnets.0.conv1.conv_xz.bias\n",
      "loaded pretrained weights:  down_blocks.0.resnets.0.conv1.conv_yz.weight\n",
      "loaded pretrained weights:  down_blocks.0.resnets.0.conv1.conv_yz.bias\n",
      "loaded pretrained weights:  down_blocks.0.resnets.0.norm2.norm_xy.weight\n",
      "loaded pretrained weights:  down_blocks.0.resnets.0.norm2.norm_xy.bias\n",
      "loaded pretrained weights:  down_blocks.0.resnets.0.norm2.norm_xz.weight\n",
      "loaded pretrained weights:  down_blocks.0.resnets.0.norm2.norm_xz.bias\n",
      "loaded pretrained weights:  down_blocks.0.resnets.0.norm2.norm_yz.weight\n",
      "loaded pretrained weights:  down_blocks.0.resnets.0.norm2.norm_yz.bias\n",
      "loaded pretrained weights:  down_blocks.0.resnets.0.conv2.conv_xy.weight\n",
      "loaded pretrained weights:  down_blocks.0.resnets.0.conv2.conv_xy.bias\n",
      "loaded pretrained weights:  down_blocks.0.resnets.0.conv2.conv_xz.weight\n",
      "loaded pretrained weights:  down_blocks.0.resnets.0.conv2.conv_xz.bias\n",
      "loaded pretrained weights:  down_blocks.0.resnets.0.conv2.conv_yz.weight\n",
      "loaded pretrained weights:  down_blocks.0.resnets.0.conv2.conv_yz.bias\n",
      "loaded pretrained weights:  down_blocks.0.downsamplers.0.conv.conv_xy.weight\n",
      "loaded pretrained weights:  down_blocks.0.downsamplers.0.conv.conv_xy.bias\n",
      "loaded pretrained weights:  down_blocks.0.downsamplers.0.conv.conv_xz.weight\n",
      "loaded pretrained weights:  down_blocks.0.downsamplers.0.conv.conv_xz.bias\n",
      "loaded pretrained weights:  down_blocks.0.downsamplers.0.conv.conv_yz.weight\n",
      "loaded pretrained weights:  down_blocks.0.downsamplers.0.conv.conv_yz.bias\n",
      "loaded pretrained weights:  down_blocks.1.resnets.0.norm1.norm_xy.weight\n",
      "loaded pretrained weights:  down_blocks.1.resnets.0.norm1.norm_xy.bias\n",
      "loaded pretrained weights:  down_blocks.1.resnets.0.norm1.norm_xz.weight\n",
      "loaded pretrained weights:  down_blocks.1.resnets.0.norm1.norm_xz.bias\n",
      "loaded pretrained weights:  down_blocks.1.resnets.0.norm1.norm_yz.weight\n",
      "loaded pretrained weights:  down_blocks.1.resnets.0.norm1.norm_yz.bias\n",
      "loaded pretrained weights:  down_blocks.1.resnets.0.conv1.conv_xy.weight\n",
      "loaded pretrained weights:  down_blocks.1.resnets.0.conv1.conv_xy.bias\n",
      "loaded pretrained weights:  down_blocks.1.resnets.0.conv1.conv_xz.weight\n",
      "loaded pretrained weights:  down_blocks.1.resnets.0.conv1.conv_xz.bias\n",
      "loaded pretrained weights:  down_blocks.1.resnets.0.conv1.conv_yz.weight\n",
      "loaded pretrained weights:  down_blocks.1.resnets.0.conv1.conv_yz.bias\n",
      "loaded pretrained weights:  down_blocks.1.resnets.0.norm2.norm_xy.weight\n",
      "loaded pretrained weights:  down_blocks.1.resnets.0.norm2.norm_xy.bias\n",
      "loaded pretrained weights:  down_blocks.1.resnets.0.norm2.norm_xz.weight\n",
      "loaded pretrained weights:  down_blocks.1.resnets.0.norm2.norm_xz.bias\n",
      "loaded pretrained weights:  down_blocks.1.resnets.0.norm2.norm_yz.weight\n",
      "loaded pretrained weights:  down_blocks.1.resnets.0.norm2.norm_yz.bias\n",
      "loaded pretrained weights:  down_blocks.1.resnets.0.conv2.conv_xy.weight\n",
      "loaded pretrained weights:  down_blocks.1.resnets.0.conv2.conv_xy.bias\n",
      "loaded pretrained weights:  down_blocks.1.resnets.0.conv2.conv_xz.weight\n",
      "loaded pretrained weights:  down_blocks.1.resnets.0.conv2.conv_xz.bias\n",
      "loaded pretrained weights:  down_blocks.1.resnets.0.conv2.conv_yz.weight\n",
      "loaded pretrained weights:  down_blocks.1.resnets.0.conv2.conv_yz.bias\n",
      "loaded pretrained weights:  down_blocks.1.resnets.0.conv_shortcut.conv_xy.weight\n",
      "loaded pretrained weights:  down_blocks.1.resnets.0.conv_shortcut.conv_xy.bias\n",
      "loaded pretrained weights:  down_blocks.1.resnets.0.conv_shortcut.conv_xz.weight\n",
      "loaded pretrained weights:  down_blocks.1.resnets.0.conv_shortcut.conv_xz.bias\n",
      "loaded pretrained weights:  down_blocks.1.resnets.0.conv_shortcut.conv_yz.weight\n",
      "loaded pretrained weights:  down_blocks.1.resnets.0.conv_shortcut.conv_yz.bias\n",
      "loaded pretrained weights:  up_blocks.0.resnets.0.norm1.norm_xy.weight\n",
      "loaded pretrained weights:  up_blocks.0.resnets.0.norm1.norm_xy.bias\n",
      "loaded pretrained weights:  up_blocks.0.resnets.0.norm1.norm_xz.weight\n",
      "loaded pretrained weights:  up_blocks.0.resnets.0.norm1.norm_xz.bias\n",
      "loaded pretrained weights:  up_blocks.0.resnets.0.norm1.norm_yz.weight\n",
      "loaded pretrained weights:  up_blocks.0.resnets.0.norm1.norm_yz.bias\n",
      "loaded pretrained weights:  up_blocks.0.resnets.0.conv1.conv_xy.weight\n",
      "loaded pretrained weights:  up_blocks.0.resnets.0.conv1.conv_xy.bias\n",
      "loaded pretrained weights:  up_blocks.0.resnets.0.conv1.conv_xz.weight\n",
      "loaded pretrained weights:  up_blocks.0.resnets.0.conv1.conv_xz.bias\n",
      "loaded pretrained weights:  up_blocks.0.resnets.0.conv1.conv_yz.weight\n",
      "loaded pretrained weights:  up_blocks.0.resnets.0.conv1.conv_yz.bias\n",
      "loaded pretrained weights:  up_blocks.0.resnets.0.norm2.norm_xy.weight\n",
      "loaded pretrained weights:  up_blocks.0.resnets.0.norm2.norm_xy.bias\n",
      "loaded pretrained weights:  up_blocks.0.resnets.0.norm2.norm_xz.weight\n",
      "loaded pretrained weights:  up_blocks.0.resnets.0.norm2.norm_xz.bias\n",
      "loaded pretrained weights:  up_blocks.0.resnets.0.norm2.norm_yz.weight\n",
      "loaded pretrained weights:  up_blocks.0.resnets.0.norm2.norm_yz.bias\n",
      "loaded pretrained weights:  up_blocks.0.resnets.0.conv2.conv_xy.weight\n",
      "loaded pretrained weights:  up_blocks.0.resnets.0.conv2.conv_xy.bias\n",
      "loaded pretrained weights:  up_blocks.0.resnets.0.conv2.conv_xz.weight\n",
      "loaded pretrained weights:  up_blocks.0.resnets.0.conv2.conv_xz.bias\n",
      "loaded pretrained weights:  up_blocks.0.resnets.0.conv2.conv_yz.weight\n",
      "loaded pretrained weights:  up_blocks.0.resnets.0.conv2.conv_yz.bias\n",
      "loaded pretrained weights:  up_blocks.0.resnets.0.conv_shortcut.conv_xy.weight\n",
      "loaded pretrained weights:  up_blocks.0.resnets.0.conv_shortcut.conv_xy.bias\n",
      "loaded pretrained weights:  up_blocks.0.resnets.0.conv_shortcut.conv_xz.weight\n",
      "loaded pretrained weights:  up_blocks.0.resnets.0.conv_shortcut.conv_xz.bias\n",
      "loaded pretrained weights:  up_blocks.0.resnets.0.conv_shortcut.conv_yz.weight\n",
      "loaded pretrained weights:  up_blocks.0.resnets.0.conv_shortcut.conv_yz.bias\n",
      "loaded pretrained weights:  up_blocks.0.resnets.1.norm1.norm_xy.weight\n",
      "loaded pretrained weights:  up_blocks.0.resnets.1.norm1.norm_xy.bias\n",
      "loaded pretrained weights:  up_blocks.0.resnets.1.norm1.norm_xz.weight\n",
      "loaded pretrained weights:  up_blocks.0.resnets.1.norm1.norm_xz.bias\n",
      "loaded pretrained weights:  up_blocks.0.resnets.1.norm1.norm_yz.weight\n",
      "loaded pretrained weights:  up_blocks.0.resnets.1.norm1.norm_yz.bias\n",
      "loaded pretrained weights:  up_blocks.0.resnets.1.conv1.conv_xy.weight\n",
      "loaded pretrained weights:  up_blocks.0.resnets.1.conv1.conv_xy.bias\n",
      "loaded pretrained weights:  up_blocks.0.resnets.1.conv1.conv_xz.weight\n",
      "loaded pretrained weights:  up_blocks.0.resnets.1.conv1.conv_xz.bias\n",
      "loaded pretrained weights:  up_blocks.0.resnets.1.conv1.conv_yz.weight\n",
      "loaded pretrained weights:  up_blocks.0.resnets.1.conv1.conv_yz.bias\n",
      "loaded pretrained weights:  up_blocks.0.resnets.1.norm2.norm_xy.weight\n",
      "loaded pretrained weights:  up_blocks.0.resnets.1.norm2.norm_xy.bias\n",
      "loaded pretrained weights:  up_blocks.0.resnets.1.norm2.norm_xz.weight\n",
      "loaded pretrained weights:  up_blocks.0.resnets.1.norm2.norm_xz.bias\n",
      "loaded pretrained weights:  up_blocks.0.resnets.1.norm2.norm_yz.weight\n",
      "loaded pretrained weights:  up_blocks.0.resnets.1.norm2.norm_yz.bias\n",
      "loaded pretrained weights:  up_blocks.0.resnets.1.conv2.conv_xy.weight\n",
      "loaded pretrained weights:  up_blocks.0.resnets.1.conv2.conv_xy.bias\n",
      "loaded pretrained weights:  up_blocks.0.resnets.1.conv2.conv_xz.weight\n",
      "loaded pretrained weights:  up_blocks.0.resnets.1.conv2.conv_xz.bias\n",
      "loaded pretrained weights:  up_blocks.0.resnets.1.conv2.conv_yz.weight\n",
      "loaded pretrained weights:  up_blocks.0.resnets.1.conv2.conv_yz.bias\n",
      "loaded pretrained weights:  up_blocks.0.resnets.1.conv_shortcut.conv_xy.weight\n",
      "loaded pretrained weights:  up_blocks.0.resnets.1.conv_shortcut.conv_xy.bias\n",
      "loaded pretrained weights:  up_blocks.0.resnets.1.conv_shortcut.conv_xz.weight\n",
      "loaded pretrained weights:  up_blocks.0.resnets.1.conv_shortcut.conv_xz.bias\n",
      "loaded pretrained weights:  up_blocks.0.resnets.1.conv_shortcut.conv_yz.weight\n",
      "loaded pretrained weights:  up_blocks.0.resnets.1.conv_shortcut.conv_yz.bias\n",
      "loaded pretrained weights:  up_blocks.0.upsamplers.0.conv.conv_xy.weight\n",
      "loaded pretrained weights:  up_blocks.0.upsamplers.0.conv.conv_xy.bias\n",
      "loaded pretrained weights:  up_blocks.0.upsamplers.0.conv.conv_xz.weight\n",
      "loaded pretrained weights:  up_blocks.0.upsamplers.0.conv.conv_xz.bias\n",
      "loaded pretrained weights:  up_blocks.0.upsamplers.0.conv.conv_yz.weight\n",
      "loaded pretrained weights:  up_blocks.0.upsamplers.0.conv.conv_yz.bias\n",
      "loaded pretrained weights:  up_blocks.1.resnets.0.norm1.norm_xy.weight\n",
      "loaded pretrained weights:  up_blocks.1.resnets.0.norm1.norm_xy.bias\n",
      "loaded pretrained weights:  up_blocks.1.resnets.0.norm1.norm_xz.weight\n",
      "loaded pretrained weights:  up_blocks.1.resnets.0.norm1.norm_xz.bias\n",
      "loaded pretrained weights:  up_blocks.1.resnets.0.norm1.norm_yz.weight\n",
      "loaded pretrained weights:  up_blocks.1.resnets.0.norm1.norm_yz.bias\n",
      "loaded pretrained weights:  up_blocks.1.resnets.0.conv1.conv_xy.weight\n",
      "loaded pretrained weights:  up_blocks.1.resnets.0.conv1.conv_xy.bias\n",
      "loaded pretrained weights:  up_blocks.1.resnets.0.conv1.conv_xz.weight\n",
      "loaded pretrained weights:  up_blocks.1.resnets.0.conv1.conv_xz.bias\n",
      "loaded pretrained weights:  up_blocks.1.resnets.0.conv1.conv_yz.weight\n",
      "loaded pretrained weights:  up_blocks.1.resnets.0.conv1.conv_yz.bias\n",
      "loaded pretrained weights:  up_blocks.1.resnets.0.norm2.norm_xy.weight\n",
      "loaded pretrained weights:  up_blocks.1.resnets.0.norm2.norm_xy.bias\n",
      "loaded pretrained weights:  up_blocks.1.resnets.0.norm2.norm_xz.weight\n",
      "loaded pretrained weights:  up_blocks.1.resnets.0.norm2.norm_xz.bias\n",
      "loaded pretrained weights:  up_blocks.1.resnets.0.norm2.norm_yz.weight\n",
      "loaded pretrained weights:  up_blocks.1.resnets.0.norm2.norm_yz.bias\n",
      "loaded pretrained weights:  up_blocks.1.resnets.0.conv2.conv_xy.weight\n",
      "loaded pretrained weights:  up_blocks.1.resnets.0.conv2.conv_xy.bias\n",
      "loaded pretrained weights:  up_blocks.1.resnets.0.conv2.conv_xz.weight\n",
      "loaded pretrained weights:  up_blocks.1.resnets.0.conv2.conv_xz.bias\n",
      "loaded pretrained weights:  up_blocks.1.resnets.0.conv2.conv_yz.weight\n",
      "loaded pretrained weights:  up_blocks.1.resnets.0.conv2.conv_yz.bias\n",
      "loaded pretrained weights:  up_blocks.1.resnets.0.conv_shortcut.conv_xy.weight\n",
      "loaded pretrained weights:  up_blocks.1.resnets.0.conv_shortcut.conv_xy.bias\n",
      "loaded pretrained weights:  up_blocks.1.resnets.0.conv_shortcut.conv_xz.weight\n",
      "loaded pretrained weights:  up_blocks.1.resnets.0.conv_shortcut.conv_xz.bias\n",
      "loaded pretrained weights:  up_blocks.1.resnets.0.conv_shortcut.conv_yz.weight\n",
      "loaded pretrained weights:  up_blocks.1.resnets.0.conv_shortcut.conv_yz.bias\n",
      "loaded pretrained weights:  up_blocks.1.resnets.1.norm1.norm_xy.weight\n",
      "loaded pretrained weights:  up_blocks.1.resnets.1.norm1.norm_xy.bias\n",
      "loaded pretrained weights:  up_blocks.1.resnets.1.norm1.norm_xz.weight\n",
      "loaded pretrained weights:  up_blocks.1.resnets.1.norm1.norm_xz.bias\n",
      "loaded pretrained weights:  up_blocks.1.resnets.1.norm1.norm_yz.weight\n",
      "loaded pretrained weights:  up_blocks.1.resnets.1.norm1.norm_yz.bias\n",
      "loaded pretrained weights:  up_blocks.1.resnets.1.conv1.conv_xy.weight\n",
      "loaded pretrained weights:  up_blocks.1.resnets.1.conv1.conv_xy.bias\n",
      "loaded pretrained weights:  up_blocks.1.resnets.1.conv1.conv_xz.weight\n",
      "loaded pretrained weights:  up_blocks.1.resnets.1.conv1.conv_xz.bias\n",
      "loaded pretrained weights:  up_blocks.1.resnets.1.conv1.conv_yz.weight\n",
      "loaded pretrained weights:  up_blocks.1.resnets.1.conv1.conv_yz.bias\n",
      "loaded pretrained weights:  up_blocks.1.resnets.1.norm2.norm_xy.weight\n",
      "loaded pretrained weights:  up_blocks.1.resnets.1.norm2.norm_xy.bias\n",
      "loaded pretrained weights:  up_blocks.1.resnets.1.norm2.norm_xz.weight\n",
      "loaded pretrained weights:  up_blocks.1.resnets.1.norm2.norm_xz.bias\n",
      "loaded pretrained weights:  up_blocks.1.resnets.1.norm2.norm_yz.weight\n",
      "loaded pretrained weights:  up_blocks.1.resnets.1.norm2.norm_yz.bias\n",
      "loaded pretrained weights:  up_blocks.1.resnets.1.conv2.conv_xy.weight\n",
      "loaded pretrained weights:  up_blocks.1.resnets.1.conv2.conv_xy.bias\n",
      "loaded pretrained weights:  up_blocks.1.resnets.1.conv2.conv_xz.weight\n",
      "loaded pretrained weights:  up_blocks.1.resnets.1.conv2.conv_xz.bias\n",
      "loaded pretrained weights:  up_blocks.1.resnets.1.conv2.conv_yz.weight\n",
      "loaded pretrained weights:  up_blocks.1.resnets.1.conv2.conv_yz.bias\n",
      "loaded pretrained weights:  up_blocks.1.resnets.1.conv_shortcut.conv_xy.weight\n",
      "loaded pretrained weights:  up_blocks.1.resnets.1.conv_shortcut.conv_xy.bias\n",
      "loaded pretrained weights:  up_blocks.1.resnets.1.conv_shortcut.conv_xz.weight\n",
      "loaded pretrained weights:  up_blocks.1.resnets.1.conv_shortcut.conv_xz.bias\n",
      "loaded pretrained weights:  up_blocks.1.resnets.1.conv_shortcut.conv_yz.weight\n",
      "loaded pretrained weights:  up_blocks.1.resnets.1.conv_shortcut.conv_yz.bias\n",
      "loaded pretrained weights:  mid_block.resnets.0.norm1.norm_xy.weight\n",
      "loaded pretrained weights:  mid_block.resnets.0.norm1.norm_xy.bias\n",
      "loaded pretrained weights:  mid_block.resnets.0.norm1.norm_xz.weight\n",
      "loaded pretrained weights:  mid_block.resnets.0.norm1.norm_xz.bias\n",
      "loaded pretrained weights:  mid_block.resnets.0.norm1.norm_yz.weight\n",
      "loaded pretrained weights:  mid_block.resnets.0.norm1.norm_yz.bias\n",
      "loaded pretrained weights:  mid_block.resnets.0.conv1.conv_xy.weight\n",
      "loaded pretrained weights:  mid_block.resnets.0.conv1.conv_xy.bias\n",
      "loaded pretrained weights:  mid_block.resnets.0.conv1.conv_xz.weight\n",
      "loaded pretrained weights:  mid_block.resnets.0.conv1.conv_xz.bias\n",
      "loaded pretrained weights:  mid_block.resnets.0.conv1.conv_yz.weight\n",
      "loaded pretrained weights:  mid_block.resnets.0.conv1.conv_yz.bias\n",
      "loaded pretrained weights:  mid_block.resnets.0.norm2.norm_xy.weight\n",
      "loaded pretrained weights:  mid_block.resnets.0.norm2.norm_xy.bias\n",
      "loaded pretrained weights:  mid_block.resnets.0.norm2.norm_xz.weight\n",
      "loaded pretrained weights:  mid_block.resnets.0.norm2.norm_xz.bias\n",
      "loaded pretrained weights:  mid_block.resnets.0.norm2.norm_yz.weight\n",
      "loaded pretrained weights:  mid_block.resnets.0.norm2.norm_yz.bias\n",
      "loaded pretrained weights:  mid_block.resnets.0.conv2.conv_xy.weight\n",
      "loaded pretrained weights:  mid_block.resnets.0.conv2.conv_xy.bias\n",
      "loaded pretrained weights:  mid_block.resnets.0.conv2.conv_xz.weight\n",
      "loaded pretrained weights:  mid_block.resnets.0.conv2.conv_xz.bias\n",
      "loaded pretrained weights:  mid_block.resnets.0.conv2.conv_yz.weight\n",
      "loaded pretrained weights:  mid_block.resnets.0.conv2.conv_yz.bias\n",
      "loaded pretrained weights:  mid_block.resnets.1.norm1.norm_xy.weight\n",
      "loaded pretrained weights:  mid_block.resnets.1.norm1.norm_xy.bias\n",
      "loaded pretrained weights:  mid_block.resnets.1.norm1.norm_xz.weight\n",
      "loaded pretrained weights:  mid_block.resnets.1.norm1.norm_xz.bias\n",
      "loaded pretrained weights:  mid_block.resnets.1.norm1.norm_yz.weight\n",
      "loaded pretrained weights:  mid_block.resnets.1.norm1.norm_yz.bias\n",
      "loaded pretrained weights:  mid_block.resnets.1.conv1.conv_xy.weight\n",
      "loaded pretrained weights:  mid_block.resnets.1.conv1.conv_xy.bias\n",
      "loaded pretrained weights:  mid_block.resnets.1.conv1.conv_xz.weight\n",
      "loaded pretrained weights:  mid_block.resnets.1.conv1.conv_xz.bias\n",
      "loaded pretrained weights:  mid_block.resnets.1.conv1.conv_yz.weight\n",
      "loaded pretrained weights:  mid_block.resnets.1.conv1.conv_yz.bias\n",
      "loaded pretrained weights:  mid_block.resnets.1.norm2.norm_xy.weight\n",
      "loaded pretrained weights:  mid_block.resnets.1.norm2.norm_xy.bias\n",
      "loaded pretrained weights:  mid_block.resnets.1.norm2.norm_xz.weight\n",
      "loaded pretrained weights:  mid_block.resnets.1.norm2.norm_xz.bias\n",
      "loaded pretrained weights:  mid_block.resnets.1.norm2.norm_yz.weight\n",
      "loaded pretrained weights:  mid_block.resnets.1.norm2.norm_yz.bias\n",
      "loaded pretrained weights:  mid_block.resnets.1.conv2.conv_xy.weight\n",
      "loaded pretrained weights:  mid_block.resnets.1.conv2.conv_xy.bias\n",
      "loaded pretrained weights:  mid_block.resnets.1.conv2.conv_xz.weight\n",
      "loaded pretrained weights:  mid_block.resnets.1.conv2.conv_xz.bias\n",
      "loaded pretrained weights:  mid_block.resnets.1.conv2.conv_yz.weight\n",
      "loaded pretrained weights:  mid_block.resnets.1.conv2.conv_yz.bias\n",
      "loaded pretrained weights:  conv_norm_out.norm_xy.weight\n",
      "loaded pretrained weights:  conv_norm_out.norm_xy.bias\n",
      "loaded pretrained weights:  conv_norm_out.norm_xz.weight\n",
      "loaded pretrained weights:  conv_norm_out.norm_xz.bias\n",
      "loaded pretrained weights:  conv_norm_out.norm_yz.weight\n",
      "loaded pretrained weights:  conv_norm_out.norm_yz.bias\n",
      "loaded pretrained weights:  conv_out.conv_xy.weight\n",
      "loaded pretrained weights:  conv_out.conv_xy.bias\n",
      "loaded pretrained weights:  conv_out.conv_xz.weight\n",
      "loaded pretrained weights:  conv_out.conv_xz.bias\n",
      "loaded pretrained weights:  conv_out.conv_yz.weight\n",
      "loaded pretrained weights:  conv_out.conv_yz.bias\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "new_state_dict = {}\n",
    "for key in model_state_dict.keys():\n",
    "    # Transform the key to match the pre-trained model format\n",
    "    # ex. down_blocks.0.resnets.0.norm1.norm_xy.weight -> down_blocks.0.resnets.0.norm1.weight\n",
    "    mapped_key = key.replace(\".norm_xy.\", \".\").replace(\".norm_xz.\", \".\").replace(\".norm_yz.\", \".\")\n",
    "    mapped_key = mapped_key.replace(\".conv_xy.\", \".\").replace(\".conv_xz.\", \".\").replace(\".conv_yz.\", \".\")\n",
    "    \n",
    "    # Check if this transformed key exists in the pre-trained model's keys\n",
    "    if mapped_key in pretrained_state_dict:\n",
    "        # Map the pre-trained weights to the new model key\n",
    "        new_state_dict[key] = pretrained_state_dict[mapped_key]\n",
    "        print(\"loaded pretrained weights: \", key)\n",
    "        cnt += 1\n",
    "    else:\n",
    "        new_state_dict[key] = model_state_dict[key]\n",
    "        print(\"not loaded: \", key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4e40932c-fab2-4f36-b564-e5ed2ea3ccb3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "252"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d619b5-31b2-403e-94cc-c9c8eb5ac287",
   "metadata": {},
   "source": [
    "- all params is loaded from the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bf56f7d5-601e-452b-aa93-3d16e96afa3c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 5,117,784 || all params: 5,117,784 || trainable%: 100.0\n"
     ]
    }
   ],
   "source": [
    "trainable_params = 0\n",
    "all_params = 0\n",
    "for _, param in model.named_parameters():\n",
    "    num_params = param.numel()\n",
    "    all_params += num_params\n",
    "    if param.requires_grad:\n",
    "        trainable_params += num_params\n",
    "print(f\"trainable params: {trainable_params:,d} || all params: {all_params:,d} || trainable%: {100 * trainable_params / all_params}\")\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "be1fc81d-c226-4115-b557-ad4630904542",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "model = model.to(device)\n",
    "model.requires_grad_(False)\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=4,\n",
    "    lora_alpha=4,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"conv_xy\", \"conv_yz\", \"conv_xz\"],\n",
    "    init_lora_weights=\"gaussian\",\n",
    "    bias=\"none\"\n",
    ") # scale = alpha / r\n",
    "\n",
    "model.add_adapter(config)\n",
    "# model_lora = get_peft_model(model, config)\n",
    "lora_layers = filter(lambda p: p.requires_grad, model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "917cb78d-9fd3-4717-8f77-a9e54d4cd449",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 402,912 || all params: 5,520,696 || trainable%: 7.298210225667199\n"
     ]
    }
   ],
   "source": [
    "trainable_params = 0\n",
    "all_params = 0\n",
    "for _, param in model.named_parameters():\n",
    "    num_params = param.numel()\n",
    "    all_params += num_params\n",
    "    if param.requires_grad:\n",
    "        trainable_params += num_params\n",
    "print(f\"trainable params: {trainable_params:,d} || all params: {all_params:,d} || trainable%: {100 * trainable_params / all_params}\")\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "66314528-3d7d-4bc5-902e-e48a935203ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 0 || all params: 5,520,696 || trainable%: 0.0\n"
     ]
    }
   ],
   "source": [
    "model.disable_adapters()\n",
    "trainable_params = 0\n",
    "all_params = 0\n",
    "for _, param in model.named_parameters():\n",
    "    num_params = param.numel()\n",
    "    all_params += num_params\n",
    "    if param.requires_grad:\n",
    "        trainable_params += num_params\n",
    "print(f\"trainable params: {trainable_params:,d} || all params: {all_params:,d} || trainable%: {100 * trainable_params / all_params}\")\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "081ed1e2-4212-4f35-b608-1103dd2c9169",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 402,912 || all params: 5,520,696 || trainable%: 7.298210225667199\n"
     ]
    }
   ],
   "source": [
    "model.enable_adapters()\n",
    "trainable_params = 0\n",
    "all_params = 0\n",
    "for _, param in model.named_parameters():\n",
    "    num_params = param.numel()\n",
    "    all_params += num_params\n",
    "    if param.requires_grad:\n",
    "        trainable_params += num_params\n",
    "print(f\"trainable params: {trainable_params:,d} || all params: {all_params:,d} || trainable%: {100 * trainable_params / all_params}\")\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763020b7-a78d-46a8-b983-474ba04cc01a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "triplora",
   "language": "python",
   "name": "triplora"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
